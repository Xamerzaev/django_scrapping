[
    {
        "url": "https://habr.com/ru/company/skillfactory/blog/589293/",
        "title": "Нейроны мозга человека сильно отличаются от нейронов других млекопитающих",
        "tag": "skillfactoryнаучно-популярноемозгбиологиячитальный залнейроныионыфизиологияфизиология дизайнабелки",
        "body": "Импульсы нейронов вырабатываются белками, которые контролируют поток ионов, благодаря чему эти белки называют ионными каналами. Нейробиологи MIT показали, что количество ионных каналов в нейронах человека гораздо меньше, чем у других млекопитающих. За подробностями приглашаем под кат, пока у нас начинается флагманский курс Data Science.Вот что рассказывает о своём открытии Марк Харнетт, доцент кафедры мозга и когнитивных наук Института исследования мозга Макговерна при Массачусетском технологическом институте и старший автор исследования:«Если мозг может сэкономить энергию за счёт снижения плотности ионных каналов, то потратить её он может на другие процессы в нейронах или в цепях».  Учёные выявили «строительный план», который наблюдается у всех видов, кроме человека. Они обнаружили, что с увеличением размера нейронов увеличивается и плотность нейронных каналов в них. Но нейроны человека оказались ярким исключением. «Предыдущие сравнительные исследования показали, что человеческий мозг устроен так же, как мозг других млекопитающих, поэтому мы удивились, обнаружив убедительные доказательства, что человеческие нейроны особенные», — рассказывает бывший аспирант MIT Лу Болье-Ларош, — ведущий автор опубликованного в Nature исследования. План строения нейроновНейроны в мозге млекопитающих получают электрические сигналы от тысяч других клеток, и от этого зависит, будут ли они подавать электрический импульс, который также называют потенциалом действия. В 2018 году Харнетт и Болье-Ларош обнаружили, что нейроны человека и крысы различаются электрическими свойствами, прежде всего в дендритах. Дендриты — древовидные антенны, которые обрабатывают данные от других клеток. Ранее считалось, что плотность ионных каналов нейронов постоянна у всех видов, но выяснилось, что у человека она ниже, чем у других млекопитающих.В новом исследовании Харнетт и Болье-Ларош сравнили нейроны различных видов млекопитающих, чтобы понять, можно ли найти определяющие экспрессию ионных каналов закономерности. Они изучали два типа калиевых каналов, связанных с напряжением, и HCN-канал, который в пятом слое пирамидальных нейронов проводит и калий, и натрий.Для исследования учёные взяли ткани мозга 10 видов млекопитающих: этрусских землероек, песчанок, мышей, крыс, морских свинок, хорьков, кроликов, мармозеток, макак, а также ткани страдающих эпилепсией людей, которые остались после операции. Так Харнетт и Болье-Ларош охватили диапазон толщины коры и размеров нейронов царства млекопитающих, а исследование стало самым масштабным в своём роде. Почти у всех перечисленных видов млекопитающих с увеличением размера нейронов увеличивалась и плотность ионных каналов. Исключением оказались нейроны человека: плотность ионных каналов в них намного меньше ожидаемой. По словам Харнетта, увеличение плотности каналов у разных видов удивляет, поскольку чем больше каналов, тем больше энергии требуется для перекачки ионов в клетку и из клетки. Ситуация прояснилась, когда исследователи обратили внимание на количество каналов в общем объёме коры головного мозга. В крошечном мозге этрусской землеройки, который заполнен маленькими нейронами, мозговых клеток больше, чем в том же объёме мозга кролика. При этом у кролика гораздо больше нейронов в целом, а плотность ионных каналов его мозга выше. Проще говоря, плотность каналов в одинаковом объёме ткани одинакова у обоих видов. Это верно для всех перечисленных видов, кроме человека.«Похоже, что кора головного мозга пытается сохранить количество ионных каналов на единицу объёма одинаковым у всех видов. Иными словами, для данного объёма коры энергетические затраты одинаковы, по крайней мере в случае ионных каналов».Возможная причина — энергияУчёные полагают, что столь низкая плотность ионных каналов могла возникнуть как способ экономить энергию на перекачке ионов. Сохранённая энергия позволяет, к примеру, создавать сложные синаптические связи между нейронами или быстрее генерировать потенциалы действия. «Мы думаем, что люди в процессе эволюции преодолели это ограничение, которое влияло на размер коры, и стали тратить меньше АТФ по сравнению с другими видами», — рассказывает Харнетт. Харнетт надеется понять, куда может уходить сэкономленная энергия, существуют ли мутации, которые помогают коре мозга человека достигать высокой эффективности и снижается ли плотность ионных каналов у приматов. А пока учёные разбираются с нейронами мозга, обратите внимание на краткие программы наших курсов, чтобы с помощью искусственных нейросетей научиться решать проблемы бизнеса.Узнать подробности акцииПрофессия Data Scientist (24 месяца)Курс «Machine Learning и Deep Learning» (6 месяцев)Другие профессии и курсыData Science и Machine LearningПрофессия Data ScientistПрофессия Data AnalystКурс «Математика для Data Science»Курс «Математика и Machine Learning для Data Science»Курс по Data EngineeringКурс «Machine Learning и Deep Learning»Курс по Machine LearningPython, веб-разработкаПрофессия Fullstack-разработчик на PythonКурс «Python для веб-разработки»Профессия Frontend-разработчикПрофессия Веб-разработчикМобильная разработкаПрофессия iOS-разработчикПрофессия Android-разработчикJava и C#Профессия Java-разработчикПрофессия QA-инженер на JAVAПрофессия C#-разработчикПрофессия Разработчик игр на UnityОт основ — в глубинуКурс «Алгоритмы и структуры данных»Профессия C++ разработчикПрофессия Этичный хакерА также:Курс по DevOpsВсе курсы",
        "user": "\n      Mojsha\n    ",
        "time": "сегодня в 23:07",
        "ratings": " 106.76 \n    Рейтинг\n  ",
        "hub": "Блог компании SkillFactory Научно-популярное Мозг Биология Читальный зал ",
        "suit": "\n      www.skillfactory.ru\n    ",
        "date": "1  ноября  2012"
    },
    {
        "url": "https://habr.com/ru/post/589375/",
        "title": "Основные варианты использования CSS переменных (Custom Properties)",
        "tag": "csscustom propertiesvar",
        "body": "CSS Variables или CSS Custom Properties уже давно используются в разработке и поддерживаются большинством популярных браузеров. Если у вас нет обязательного требования разрабатывать под IE, то вполне вероятно, вы уже успели оценить их преимущества.По этой теме написано множество статей, но я сфокусируюсь на том, чтобы показать распространенные кейсы по использованию, которые сам применял на практике. Будет мало теории, но много кода.Варианты использования1. Определение переменныхНачнём с базовых вещей, а именно: как определять переменные. Стоит помнить, что их можно определить как в корневом элементе :root (переменные будут доступны по всему приложению), так и внутри любого селектора (будут доступны только для выбранного элемента и его дочерних элементов)::root {\n    /* Определение переменной внутри корневого элемента */\n    --color: green;\n}\nbody {\n    /* Определение переменной внутри селектора */\n    --font: 15px;\n    background-color: var(--color);\n    font-size: var(--font);\n}2. Переопределение переменныхВ примере показан кейс, когда при наведении на элемент, должен меняться цвет. Для этого мы переопределяем переменную --color по событию :hover:body {\n    --color: green;\n}\nbutton {\n    color: var(--color);\n}\nbutton:hover {\n    --color: red;\n}Второй пример показывает переопределение переменной в @media выражении. К сожалению, мы не можем вынести 1024px в переменную и использовать ее в @media - это является одним из ограничений CSS переменных.:root {\n    --color: green;\n}\nbody {\n    background-color: var(--color);\n}\n@media (max-width: 1024px) {\n    :root {\n        --color: red;\n    }\n}3. Локальный fallbackПредставьте, что вы вызываете переменную --color-green, а ее не существует. Страшного не случится, но заданное CCS-свойство не отработает. Для подстраховки можно задать резервное значение вторым аргументом. Это значение также может быть CSS-переменной::root {\n    --color-green: green;\n    --color-blue: blue;\n    --color-red: red;\n}\nbody {\n    color: var(--color-green, blue);\n    color: var(--color-green, var(--color-blue));\n    color: var(--color-green, var(--color-blue, var(--color-red, red)));\n}4. Привязка переменныхПри объявлении переменных могут использоваться другие переменные::root {\n    --color1: var(--color2);\n    --color2: var(--color3);\n    --color3: red;\n}\nbody {\n    background-color: var(--color1);\n}5. Переменные в calc()В calc() можно перемножать числа со значениями в единицах измерения, н-р: px.На выходе получим результирующее значение в той единице измерения, на которую умножили.В примере показано, как перемножается 2 на 10px и в итоге получается 20px. Не важно, используем мы для этого обычные значения или CSS переменные - результат будет одинаковый::root {\n    --increment: 2;\n    --size: 10px;\n}\nbody {\n    /* Будет 20px; */\n    font-size: calc(var(--increment) * var(--size));\n}К примеру, у нас есть переменная --font: 20. Но без указания единицы измерения мы не сможем ее корректно использовать в font-size.Это можно решить с помощью calc(). Для преобразования числа в px, к примеру, достаточно умножить число на 1px в calc()::root {\n    --font: 20;\n}\ndiv {\n    /* Будет 20px; */\n    font-size: calc(var(--font) * 1px);\n}6. Прозрачность в цветовых функцияхЗадания цвета - самый распространенный кейс использования переменных.Вот стандартный пример по использованию HEX-цвета для определения значения переменной::root {\n    /* HEX формат */\n    --color-blue: #42c8f5;\n}\nbody {\n    color: var(--color-blue);\n}Часто бывает, что для цвета нужно задавать различную прозрачность. В CSS для этого есть:rgba()HSLAHWBRRBBBBAA HEX Color NotationИспользование rgba()При использовании переменных, удобнее это делать функцией rgba(), которая принимает 4 числовых значения для:Красного цветаЗеленого цветаСинего цветаАльфа-канала для задания прозрачностиНа самом деле, внутри CSS допускается использовать практически всё (за небольшим исключением), даже код на JavaScript!Поэтому, задание в переменной значения цветов для Red, Green, Blue - вполне допустимо.:root {\n    /* Указание значений цветов: Red Green Blue */\n    --color-blue--rgb: 66, 200, 245;\n}Вызовем функцию rgba() с переменной --color-blue--rgb. Для rgba() не хватает четвертого аргумента задающего прозрачность - добавим его через запятую:body {\n    background-color: rgba(var(--color-blue--rgb), 0.7);\n}На выходе собираются аргументы для rgba(): значения из переменной и альфа-канала.По итогу получаем цвет:body {\n    background-color: rgba(66, 200, 245, 0.7);\n}Использование hsla()Кроме rgba() можно использовать и hsla(). Это не так удобно, но как вариант, можно попробовать.Идея следующая:Определяем переменные с базовыми значениями для основных параметров hsla(): --hue, --saturation, --lightness, --opacity.При использовании, указываем все базовые параметры в селекторе.Меняем один / несколько переменных в селекторе (изменения коснутся только данного селектора и его дочерних элементов).:root {\n    --hue: 285;\n    --saturation: 100%;\n    --lightness: 60%;\n    --opacity: 0.7;\n}\nbody {\n    /* Переопределяем значение для --hue */\n    --hue: 400;\n\n    background-color: hsla(\n        var(--hue),\n        var(--saturation),\n        var(--lightness),\n        var(--opacity)\n    );\n}На выходе получаем цвет:body {\n    background-color: hsla(400, 100%, 60%, 0.7);\n}7. Переменные в SVGС помощь переменных мы также можем изменять цвет внутри SVG: fill или stroke. Сложность заключается в том, что изображение SVG должно быть инлайново встроено на страницу, но при использовании сборщиков - это не проблема.Итак, имеем SVG-элемент на странице, у которого в fill указана переменная --color:<svg class=\"icon\">\n    <circle cx=\"50\" cy=\"50\" r=\"50\" fill=\"var(--color)\" />\n</svg>И саму переменную --color в CSS::root {\n    --color: green;\n}Значение переменной можно переопределять при необходимых условиях: например, при событии hover на SVG:.icon:hover {\n    --color: pink;\n}Использование с JavaScript. API CSS Style DeclarationCSS переменные работают в runtime, в отличие переменных препроцессоров. Это значит, что мы можем получить к ним доступ в процессе работы приложения через JavaScript.Рассмотрим, как можно работать с CSS переменными через JavaScript:В CSS у нас есть 2 переменные::root {\n    --color: green;\n}А вот код на JavaScript:// Получаем корневой элемент в DOM\nconst root = document.querySelector(':root');\n\n// Получаем \"Вычисленные свойства\"\nconst rootStyles = getComputedStyle(root);\n\n// Получаем значение переменной по ее названию\nconst color = rootStyles.getPropertyValue('--color'); // => 'green'\n\n// Изменяем значение переменной\nroot.style.setProperty('--color', '#88d8b0');\n\n// Удаляем CSS переменную\nroot.style.removeProperty('--color');Для примера, я показал все возможные действия с переменными: чтение, изменение, удаление. Использовать операции можно по необходимости.Вычисленные свойстваВ коде выше я затронул тему \"Вычисленных свойств\". Рассмотрим подробнее: для этого создадим небольшой пример:HTML-код:<h1>Hello</h1>CSS-код:h1 {\n    font-size: 5rem;\n}Единицы измерения можно подразделить на абсолютные и относительные:Абсолютные - это пиксели (px), которые привязываются к разрешению экрана.Относительные (н-р: rem) формируются относительно заданного параметра.Для отображения, относительные единицы измерения должны быть преобразованы в абсолютные. Если мы откроем инспектор объектов в Google Chrome (или другом браузере) на вкладке \"Elements\", то сможем увидеть это преобразование:В секции Styles - значения в том виде, в котором мы их указали в CSS (относительные).В секции Computed - значения, приведенные к абсолютным единицам измерения. Функцией getComputedStyle в JavaScript мы как раз таки и получаем эти вычисленные значения.Так для чего же может понадобиться работать с CSS переменными через JavaScript? Самый распространенный кейс - это задание цветовой схемы: при переключении схемы мы обращаемся к CSS переменной и просто изменяем ее значение. Все значения, где используется эта переменная, будут обновлены.Небольшую демку по изменению цветовой схемы можно посмотреть здесь.Проверка поддержки переменныхИз CSS:@supports ( (--a: 0) ) {\n    /* Стили с поддержкой переменных */\n}\n@supports ( not (--a: 0) ) {\n    /* Стили без поддержки переменных */\n}Из JavaScript:if (window.CSS && window.CSS.supports && window.CSS.supports('--a', 0)) {\n    // Сценарии с поддержкой переменных\n} else {\n    // Сценарии без поддержки переменных\n}Достоинства и ограниченияДостоинства:Работают в runtime, в отличие от переменных препроцессоровМожно обратиться из JavaScriptНельзя использовать:В именах обычных свойств CSS: var(--side): 10pxВ значениях media-запросов: @media screen and (min-width: var(--desktop))В подстановке URL: url(var(--image))Ограничения:Нельзя сбрасывать значения всех переменных --: initialЗаключениеИспользование CSS-переменных еще один шаг к отказу от препроцессоров. Ждем реализации миксинов @apply на CSS и CSS Nesting Module!",
        "user": "\n      anton-sergeenkov\n    ",
        "time": "сегодня в 22:44",
        "ratings": " 106.76 \n    Рейтинг\n  ",
        "hub": "CSS *HTML *JavaScript *",
        "suit": "\n      www.skillfactory.ru\n    ",
        "date": "1  ноября  2012"
    },
    {
        "url": "https://habr.com/ru/post/589359/",
        "title": "Засекреченные нюансы роботов телеприсутствия",
        "tag": "робот телеприсутствия",
        "body": "Если не использовать роботов телеприсутствия и не жить с ними, а только читать о них в обзорах, то вы никогда не узнаете самого интересного. Потому что интересное скрывается разработчиками от ехидных глаз покупателей. Покупателям некоторые вещи лучше и не знать, так что я здесь пишу не для них, а для любознательных хабровчан, которые все равно роботов не покупают, а делают сами :-).Первый \"секрет\" состоит в том, что несмотря на то, что в интернете описано более 30 моделей роботов, на самом деле подавляющее большинство из них так и остались экспериментальными образцами, некоторые фирмы обанкротились, другие продают роботов по ценам, несоизмеримо высоким по сравнению с их функиональными возможностями. В итоге реально на рынке остались фактически три типа роботов: Double 3, Ohmni и BotEyes:   Три лучших серийных робота телеприсутствияРассмотрим теперь нюансы основных характеристик роботовКачество видеоНекоторые производители указывают в характеристиках \"требуемую полосу пропускания интернета\" и каждый хочет показать как можно меньшую величину. Это чистая липа и вот почему. Подавляющее большинство роботов использует одну и ту же технологию WebRTC, разработанную Google и стандартизованную консорциумом W3C. В эту технологию уже встроены алгоритмы сжатия видео кодеками VP8 и H.264, методы доступа к камере, алгоритмы подстройки разрешения видео под пропускную способность канала связи, а также алгоритмы получения IP адресов робота и клиента. Качество видео определяется пропускной способностью канала от видеокамеры робота до экрана монитора удаленного пользователя. Уменьшить разрешающую способность видео возможно программно, но увеличить - нет. Пропускная способность всего канала передачи видео определяется самым медленным звеном. Таких звеньев три: интернет, процессор в роботе и процессор в гаджете пользователя, который управляет роботом. С практической точки зрения самым узким звеном в канале связи является интернет, особенно если робот находится на выставке, где к одной и той же точке доступа WiFi подключено множество потребителей. В условиях, когда на стороне клиента используется мощный десктопный компьютер и интернет имеет большую пропускную способность, качество видео определяется производительностью процессора в роботе. Если же клиент использует старенький маломощный смартфон, то качество видео зависит от производительности его процессора.Поскольку производители роботов стараются использовать наиболее мощные процессоры, существующие на рынке на момент создания робота, то качество видео у всех роботов получается примерно одинаковым, если они сделаны в одно и то же время и используют сопоставимые по производительности процессоры. Исключение составляют случаи, когда в роботе используются не универсальные процессоры, а специализированные, для обработки видео.По мере появления новых процессоров наиболее быстро они внедряются в смартфоны и планшеты по причине огромного размера этого рынка. Рынок же специализированных процессорных модулей (печатных плат) более инертен по естественным причинам, связанным с малым тиражом таких изделий. Поэтому роботы, имеющие возможность замены планшета, установленного в голове робота, позволяют получить более качественное видео в любой момент, путем простой замены устаревшей модели планшета на более современную.Встроенный или сменный планшет?Многие роботы телеприсутствия выпускаются с уже встроенными сенсорными экранами и со встроенными процессорными платами (например, Double 3 и Ohmni), другие используют готовые планшеты, вставляемые в голову робота (например, Double 2, BotEyes, Padbot U1). Встроенный сенсорный экран со встроенным процессорным модулем, в отличие от готового планшета, имеет ряд достоинств: он позволяет подключить любую веб камеру, например, поворотную камеру с увеличением (pan-tilt-zoom) или несколько камер, нестандартные микрофоны (микрофонный массив), датчики столкновений. Теоретически их можно подключить и к стандартному планшету через USB-OTG кабель, но эта технология поддерживается не всеми планшетами и не всеми ОС. Недостатком роботов с интегрированными в них процессорными модулями и сенсорными экранами является то, что используемая в них элементная база очень быстро устаревает. В то же время в роботах со сменным планшетом его всегда можно заменить на более мощный, новейшей модификации, да еще и с обновляемой ОС.Преимущество сменного планшетаРобот со сменным планшетом имеет еще одно, неочевидное с первого взгляда, преимущество: его можно расположить как горизонтально, так и вертикально. Что это дает? Дело в том, что матрица видео сенсора в камере имеет определенное соотношение сторон, например, 16:9. Если монитор вашего компьютера, который управляет роботом, имеет такое же соотношение сторон, а положение камеры и положение монитора совпадают (оба расположены, например, горизонтально), то изображение с камеры целиком отображается на экране. Однако, если планшет в роботе (или, что то же самое, камера) расположены вертикально, а монитор - горизонтально, то справа и слева от видео вы увидите большие черные полосы, занимающие большую часть экрана, и это будет выглядеть примерно вот так (https://www.youtube.com/watch?v=eXro2yj9JgI ):Так выглядит изображение из робота при несогласованности соотношения сторон камеры и монитораЕсли же мы попытаемся растянуть изображение по ширине, то часть изображения будет обрезана по вертикали. Единственный корректный выход - согласовать положение монитора и положение камеры в роботе. А именно, если вы используете настольный компьютер, у которого экран расположен горизонтально, то и в роботе планшет должен быть расположен горизонтально. Если же вы используете смартфон, который удобно держать вертикально, то и в роботе планшет надо расположить вертикально. Возможность смены положения планшета имеется только в роботе BotEyes.Дополнительным преимуществом применения сменного планшета является возможность установить на него любую привычную программу типа Skype или Zoom вместо приложения по умолчанию.Возможность смены планшета также сильно экономит деньги покупателя, поскольку в зависимости от его требований в робот можно поставить недорогой планшет стоимостью 300 долл. или дорогой за 1500 долл.Размеры планшетаРазмер планшета определяет, насколько детально вы можете рассмотреть удаленную сцену. Обычно в роботах используют экраны размером 10 дюймов. Необходимый размер зависит от того, с какой целью вы используете робота. Достоинством роботов со сменным планшетом является возможность использования любых планшетов, например, от 8 до 12 дюймов (см. рис. ниже).Сменный планшета в универсальном держателеВарианты управления движениемБольшинство роботов для управления движением используют четыре стрелки: «вперед», «назад», «влево», «вправо». Скорость либо не регулируется вообще, либо для ее изменения нужно остановить робота, изменить скорость и затем возобновить движение. Такой подход удобен, когда нужно перевести робота на большое расстояние в просторном помещении. Однако, если вы находитесь в комнате, где много людей или стульев, такое управление требует множества лишних манипуляций, робот теряет маневренность. В некоторых роботах используется так называемый «круг управления», который позволяет изменять скорость и направление движения плавно, без остановки робота:Круг управление движением роботаЕсли указатель мыши (или палец на сенсорном экране) расположен близко к центру круга, скорость минимальная или нулевая, чем дальше от центра, тем больше скорость. Стрелки указывают направление движения робота в момент, когда указатель мыши находится в секторе со стрелкой. Круг делается прозрачным, чтобы основное изображение было видно сквозь него, линии - полупрозрачными.Такой орган управления делает робота высоко маневренным, позволяет двигаться, не задевая даже очень близко расположенные предметы. Это особенно полезно, когда робот едет по производственному цеху, где на полу может лежать много как крупных, так и мелких предметов.   Click-To-Drive + Self-DrivingЭтот уникальный способ управления движением предложен фирмой Double Robotics. Суть его в том, что пользователь указывает точку на полу, в которую должен переехать робот, и он перемещается в нее автоматически, избегая по пути препятствия с помощью системы сенсоров. Это свойство выглядит очень интересным, но имеет ряд ограничений.Во-первых, хотя технология RealSense, использованная для отображения 3D сцены, должна работать на дистанции до 10 м, на самом деле робот распознает сцену на гораздо меньшем расстоянии, после прохождения которого он останавливается и движение нужно начинать заново. Т.е. вместо того, чтобы один раз нажать стрелку «вперед» и ехать сколько нужно, в случае Click-To-Drive может потребоваться многократно указывать роботу новое место назначения. На скриншоте, который мы сняли во время нашего удаленного тура по Британской картинной галерее (см. рисунок ниже), видна площадь светлых точек на полу - это область, в которой работает технология click-to-drive. Длина области составляет примерно два-три метра.Размер области, в пределах которой работала технология click-to-drive во время нашего экспериментаПри хорошем освещении и просторных помещениях этот способ достаточно удобен и почти незаменим в случае, когда робот не способен наклонить голову, чтобы увидеть препятствие рядом с колесами.Вторым недостатком является невозможность езды в тесном помещении с множеством препятствий, поскольку ультразвуковых сенсоров расстояния недостаточно для обнаружения, например, мелких или тонких высоких предметов, а также точного определения границ крупных предметов. Для ориентации в таких условиях необходимо использовать сканирующий лидар, который, однако, очень дорого стоит. Следует также отметить, что проблема управления движением существует только при первоначальном тестировании робота людьми без опыта. Если же человек использует робота регулярно, то проблемы управления движением полностью исчезают, поскольку это слишком просто.Устойчивость к падениям и высота роботаИдеальный робот телеприсутствия должен быть легким и высоким, чтобы человеку не нужно было наклоняться для просмотра изображения на экране. Но удержать планшет на высоком и подвижном шесте можно только с помощью автоматической балансировки. Поэтому очевиден огромный соблазн сделать робота по принципу сегвея. Однако это желание порождает большое количество технических проблем, описанных ниже. Самая главная проблема в том, что двухколесные роботы падают, и это приводит к порче планшета, который стоит часто дороже самого робота. Это происходит в случае, когда робот задевает одним колесом, например, диван, а второе продолжает ехать, или при переезде дверного порожка. Вторая проблема состоит в поперечной устойчивости. Когда человек едет на сегвее, он движением своего тела обеспечивает поперечную устойчивость. В случае робота необходимо делать вторую систему автобалансировки - поперечную, наряду с продольной. По этой причине первые двухколесные роботы, не имеющие поперечной балансировки, не могли без падения переехать даже провод, лежащий на полу. Последние версии таких роботов имеют систему поперечной балансировки и стали намного устойчивее первых версий, однако, они все равно падают (из моего личного опыта) при переезде даже невысоких порожков, если колеса переезжают порожек не одновременно. Третья проблема состоит в том, что в таких роботах сложно реализовать наклон планшета, поскольку датчик наклона, обеспечивающий балансировку, находится в самом планшете. Кроме того, усложняется сам алгоритм балансировки. Для компенсации невозможности изменять направление обзора с помощь наклона планшета используется камера типа «рыбий глаз», которая позволяет видеть, что находится внизу робота. К сожалению, такая камера сильно искажает пространство и порождает дискомфорт оператора. Четвертая проблема состоит в том, что вследствие инерционности всей системы и ограниченного быстродействия двигателей робот во время начала движения и после остановки наклоняется назад и вперед, что также вызывает психологический дискомфорт.Все эти недостатки и проблемы компенсируются тем, что двухколесные роботы являются самыми легкими на рынке, что очень удобно при их транспортировке. Противоположные свойства и проблемы характерны для трех- и четырехколесных роботов. Проблему устойчивости они решают гораздо проще: с помощью увеличенной площади основания и увеличенного веса шасси. Такой подход сразу позволяет решить ряд проблем двухколесных роботов: наклон планшета на любой угол, легкий осмотр пространства вокруг колес, отсутствие необходимости линзы «рыбий глаз» для камеры, наличие поперечной и продольной устойчивости одновременно, возможность преодоления довольно высоких порогов. Платой за это является больший вес, большие расходы на транспортировку, большие габариты основания.Наклон головыДля того, чтобы объезжать препятствия, оператор должен их видеть. Роботы телеприсутствия используют для этого два способа: вторую камеру, которая показывает пространство вокруг колес, и наклон камеры или планшета.Недостатком варианта со второй (навигационной) камерой является уменьшение полезной площади экрана. На фото ниже показано, как примерно выглядит экран оператора такого робота:Внизу видно изображение от навигационной камеры Есть варианты, когда вид на колеса накладывается на вид от основной камеры. При этом площадь основного изображения становится больше, но все равно его часть занимает вид от навигационной камеры. В случае, если для осмотра пола наклоняется планшет, такого эффекта нет и полезное изображение занимает всю площадь экрана:При использовании наклоняемой камеры основной вид занимает всю площадь экранаИспользование наклоняемой камеры одновременно для навигации и основной цели оправдано, поскольку объезд препятствий занимает незначительную долю общего времени использования камеры. Второе преимущество состоит в том, что задняя камера планшета оказывается свободной и ее можно использовать для осмотра сцены позади робота путем быстрого переключения камер. Кроме того, задняя камера у большинства планшетов имеет гораздо большее разрешение и это можно использовать, например, для чтения текста на школьной доске. Есть еще полезный психологический эффект от возможности наклонять планшет: наклон можно сделать таким, что у собеседников возникает ощущение, что они смотрят друг другу в глаза. Это очень важно.Тип аккумулятораТип аккумулятора влияет на три характеристики робота: безопасность, время работы без подзарядки и устойчивость.Литиевые аккумуляторы, используемые в подавляющем большинстве роботов телеприсутствия, опасны по воспламенению. Несмотря на энергичные усилия, предпринятые в последние годы при их конструировании и производстве, несмотря на наличие защиты от перезарядки, короткого замыкания, балансировку, до сих пор бывают случаи их взрыва и возгорания. Список инцидентов можно посмотреть на сайте Департамента транспорта США и они продолжаются по сей день. Поэтому применение непроливаемых свинцово-кислотных аккумуляторов в роботах телеприсутствия с точки зрения безопасности является более предпочтительным.Из документов о безопасности следует также, что чем выше емкость литиевого аккумулятора, тем большая энергия выделяется при его возгорании и тем тяжелее последствия. Поэтому к перевозке воздушным транспортом допущены только аккумуляторы мощностью менее 100 Вт-час. Мы думаем, что именно это обстоятельство ограничивает продолжительность работы многих роботов телеприсутствия пятью часами. В отличие от этого, роботы со свинцово-кислотными аккумуляторами являются безопасными, поэтому на их емкость не накладывается ограничений при транспортировке и в робот можно установить аккумулятор любой емкости. Большой вес свинцово-кислотных аккумуляторов, расположенных в нижней части шасси, обеспечивает также хорошую устойчивость робота.Громкость звукаВ современных планшетах установлены мощные динамики и их мощности достаточно для общения с отдельным человеком в нешумном окружении. Однако в многолюдном шумном помещении (например, на выставке) необходим мощный звук. Его можно получить с помощью дополнительного внешнего Bluetooth громкоговорителя. У некоторых роботов такие громкоговорители уже установлены в самом роботе. Однако и их мощности может быть недостаточно. Наши эксперименты с пятью типами покупных внешних громкоговорителей с Bluetooth показали, что они существенно усиливают звук по сравнению с динамиками смартфонов, однако практически не увеличивают громкость звучания планшетов. Поэтому в тех случаях, когда для выступающего на митинге нужна мощная акустика, такой же мощности аппаратура нужна и роботу. Выбор средства видео коммуникации Современные роботы телеприсутствия, практически все без исключения, используют технологию Google WebRTC. Однако существует также такие общепринятые для видео коммуникации средства, как Skype, Zoom, Google Hangout и др. Особенностью их применения является то, что в их пользовательский интерфейс невозможно встроить органы управления движением робота. В таком случае делают второе приложение, которое управляет движением независимо от приложения для видео связи. При использовании десктопного компьютера два приложения легко открыть на одном экране, однако в случае мобильных устройств это не всегда возможно и доставляет некоторые неудобства. Из известных нам роботов только BotEyes позволяет использовать сторонние приложения для видео связи помимо приложения на базе WebRTC.При использовании Skype удаленную сцену можно просматривать на экране современного телевизора. Управление движением робота в этом случае осуществляется из любого гаджета, например, смартфона.Автоматическая установка в докВсе рассматриваемые роботы имеют функцию автоматической установки в док с расстояния около метра от док-станции. Делают они это путем распознавания изображения док-станции, что требует хорошего ее освещения.Следует отметить, что эта функция имеет скорее рекламную цель, нежели практическую, поскольку установка в док вручную не создает неудобств для пользователя по сравнению с автоматической, которая к тому же не имеет 100%-ной надежности. Сторожевой таймерРобот едет, когда вы нажали на кнопку «вперед» и останавливается, когда кнопка отжата. Представьте теперь, что интернет-связь разорвалась до того, как вы отжали кнопку. Робот будет ехать без остановки. То же самое произойдет при зависании программы в компьютере, в браузере или приложении, из которого вы управляете роботом. Такие проблемы давно известны в области систем автоматического управления и решаются с помощью сторожевого таймера. Работает сторожевой таймер следующим образом: контроллер, управляемый движением, вырабатывает короткие импульсы (порядка 1 сек), в течение которых двигатели работают. Если импульсы пропадают, двигатели останавливаются. Для непрерывного движения из браузера пользователя должна поступать серия импульсов, запускающих импульсы в таймере контроллера, если все работает исправно. При выходе из строя любого звена на пути передачи запускающих импульсов двигатели останавливаются через 1 сек. Аналогичная система может иметь несколько уровней иерархии и называется многоуровневым сторожевым таймером. Совместимость с различными гаджетами При выборе робота телеприсутствия следует обращать внимание, с какими ОС и марками гаджетов он может работать. Например, некоторые роботы работают только с iPad и не работают с планшетами на базе Андроид, некоторые не могут управляться из десктопных компьютеров. Увеличение видеоУвеличение видео позволяет рассмотреть мелкие детали, например, текст, написанный на доске. Увеличение может быть цифровым и оптическим. Наиболее эффективно и надежно оптическое увеличение, которое реализуется с помощью системы линз и микромоторов. Функцию увеличения имеют все рассматриваемые роботы, однако наилучшим образом она работает у робота Double 3.Переключение камерПереключение с передней камеры на заднюю и обратно не только удобно для улучшения обзора окружающего пространства, но и дает возможность воспользоваться задней камерой планшета, которая имеет намного большее разрешение, чем передняя. Такую возможность имеет только робот BotEyes. Запись и сохранение видео и изображенияЗапись видео и фото - одна из редких функций робота телеприсутствия и ее часто можно заменить сторонними программами, которые записывают видео с экрана и делают его снимки. Записывать полезно, например, урок в школе или доклад на конференции, чтобы просмотреть его повторно. Демонстрация экранаДемонстрация экрана позволяет открыть любой файл (фото, видео, чертежи, текст) на экране пользователя и показать его на экране робота. Это редкая функция и она реализована не во всех роботах. ЗаключениеРоботы телеприсутствия существенно различаются своими характеристиками и ценой, поэтому перед приобретением робота необходимо четко понять, для чего он вам нужен и в зависимости от этого выбрать робота с необходимым набором технических характеристик. Нужно также помнить, что производители роботов умышленно указывают не все характеристики или указывают их неправильным образом. Поэтому желательно понимать, что и как должно работать и как оно работает в выбраном вами роботе.",
        "user": "\n      viktordenisenko\n    ",
        "time": "сегодня в 21:01",
        "ratings": " 106.76 \n    Рейтинг\n  ",
        "hub": "Робототехника Искусственный интеллект Интернет вещей Видеоконференцсвязь ",
        "suit": "\n      www.skillfactory.ru\n    ",
        "date": "1  ноября  2012"
    },
    {
        "url": "https://habr.com/ru/company/eaton/blog/589353/",
        "title": "Как защитить ЦОД от аварий и форс-мажоров?",
        "tag": "цодыцоды.рфцодостроениебезопасностьавария",
        "body": "Почтовый сервис Mail.Ru, платёжная система Qiwi, социальная сеть «ВКонтакте», крупнейший хостинг-провайдер Европы OVH — все они столкнулись с серьёзными сбоями в работе центров обработки данных. Компании не только потеряли деньги из-за выхода оборудования из строя, но и понесли репутационные потери. В этом посте мы расскажем о том, как защитить ЦОД от подобных угроз.Причиной поломки или аварии в центре обработки данных может стать что угодно — от высоких нагрузок на серверы до несоблюдения техники безопасности. Так, в ЦОДе DataLine в Москве в 2019 году причиной пожара стало короткое замыкание в системе кондиционирования, а прошлогоднее отключение ряда функций «ВКонтакте» произошло вследствие перегрева серверного оборудования. Центры обработки данных компании OVH пострадали из-за неполадок в системе бесперебойного питания. Столь серьёзные форс-мажоры случаются не каждый день — обычно поломки бывают менее критичными. Тем не менее, эта проблема очень распространена: по данным проведённого ресурсом «Цоды.рф» опроса, почти 80% компаний сталкивались с прерыванием работы сервисов из-за сбоев в работе ЦОДа. Решение, которое помогает предотвратить сбои или минимизировать их последствия — постоянный мониторинг инженерной инфраструктуры.Каким бывает мониторинг ЦОД?Чаще всего центры обработки данных применяют полуавтоматический или же полностью автоматический подход к мониторингу. При полуавтоматическом мониторинге ответственный специалист или группа постоянно следят за показателями всех датчиков, расположенных в ЦОДе, — от датчиков температуры и влажности до сенсора пролива охлаждающей жидкости на пол — и оперативно реагируют, когда эти показатели выходят за пределы нормы. Недостатки такого подхода — высокооплачиваемый ручной труд, необходимость постоянного присутствия специалистов в ЦОДе, а также отсутствие инструментов для хранения исторических данных и их анализа. Сотрудники лишь реагируют на проблемы, но не имеют возможности выявлять закономерности их появления. Автоматический мониторинг проводится удалённо, а сбором и обработкой данных с датчиков занимается облачная платформа, она же показывает их оператору в удобном формате. Чтобы получить доступ к данным, достаточно подключиться к платформе с любого компьютера или мобильного устройства с доступом к сети. При подобном подходе сокращается количество специалистов, необходимых для обслуживания системы. Кроме того, операторы могут работать удалённо — это стало особенно актуально сейчас, когда компании вынуждены функционировать в условиях карантинных ограничений. Преимущества удалённого мониторинга ЦОДов Удалённый контроль состояния оборудования в центре обработки данных позволяет сделать обслуживание более эффективным: оператор получает информацию о состоянии инфраструктуры до того, как на место отправится механик. Это поможет существенно сэкономить, если ЦОД находится в другом городе. Важное преимущество облачной платформы — длительное хранение данных. Изменения того или иного показателя можно отслеживать в течение определённого периода времени чтобы выявлять неполадки. Пример из практики: Eaton поставил крупному заказчику, ЦОДу в Финляндии, несколько источников бесперебойного питания, соединённых с системой удалённого мониторинга. Со временем система стала фиксировать постоянное превышение температуры в ЦОДе. Оказалось, перестала работать система кондиционирования, а её датчики не сработали. Выявить проблему до того, как она стала критической, позволил всесторонний удалённый мониторинг. В дополнение к информированию о состоянии оборудования здесь и сейчас систему можно научить формировать прогнозы относительно срока службы оборудования и его обслуживания. Для этого нужно интегрировать платформу с инструментами машинного обучения и искусственного интеллекта. Наконец, с удалённым мониторингом работа центра обработки данных становится максимально прозрачной: владельцы бизнеса и топ-менеджеры могут самостоятельно в любой момент и из любой точки мира получить информацию о состоянии оборудования. Как работает система удалённого мониторинга? Своевременно и точно оповещать об инцидентах системе позволяет «красная зона» — набор показателей, которые свидетельствуют о чрезвычайных событиях. Как только показатели «краснеют», система автоматически рассылает оповещения об этом ответственным сотрудникам. Важно настроить оповещения так, чтобы их было не слишком много. Если система будет рапортовать о даже самых незначительных отклонениях, такие сообщения станут обыденностью и специалисты пропустят действительно важный сигнал. Для того, чтобы картина состояния оборудования в ЦОД была полной, рекомендуется отслеживать три основные группы параметров:1)     параметры окружающей среды — температура, относительная влажность, состав воздуха — позволяют отслеживать корректность работы систем кондиционирования и охлаждения;2)     параметры источников бесперебойного питания — напряжение каждой ячейки батареи, общее напряжение батареи, потребляемый ток, потребляемая мощность, состояние ИБП — дают возможность спрогнозировать необходимость обслуживания или замены ИБП;3)     параметры работы серверов — загрузка, сетевой трафик — их помощью можно понять, как более эффективно использовать вычислительные мощности центра и не допустить их перегрузки. Частота сбора показателей зависит от параметра. Если электропитание следует измерять не реже раза в секунду, то температуру и влажность можно контролировать каждые 10‑15 минут. Системы удалённого мониторинга позволяют настроить частоту сбора данных вручную. Потенциальные недостатки систем удалённого мониторинга Для того, чтобы система удалённого мониторинга работала эффективно, следует изучить её уязвимости: это поможет предотвратить сбои в работе. Во-первых, в системе могут быть ошибки, появившиеся в результате действия человеческого фактора, — ошибки разработчиков. От ошибок, конечно, никто не застрахован, но в системах, разработанных надёжными и опытными компаниями, шанс этого ниже. Во-вторых, есть вероятность вторжения в информационную инфраструктуру ЦОД с целью похищения данных или нарушения работы критически важной инфраструктуры. Чтобы эту вероятность минимизировать, необходимы технические меры защиты — например, двухфакторная аутентификация при входе, своевременное обновление ПО, использование ПО для обеспечения кибербезопасности и в целом применение наиболее эффективных методов защиты ИТ- и ОТ-инфраструктуры. В-третьих, через систему удалённого мониторинга не получится управлять оборудованием и вычислительной инфраструктурой ЦОД, поскольку данные передаются только в одном направлении: от оборудования в облако. Самый большой риск — подмена данных и в связи с этим отсутствие надлежащей реакции на инциденты. Система удалённого мониторинга — оптимальный инструмент для отслеживания состояния инженерной инфраструктуры ЦОД. Она позволяет управлять оборудованием и вычислительными мощностями без капитальных затрат и снижает репутационные и финансовые риски в результате инцидентов. При этом риск возникновения ошибок невысок и его можно минимизировать.",
        "user": "\n      eaton_ru\n    ",
        "time": "сегодня в 20:27",
        "ratings": " 24.98 \n    Рейтинг\n  ",
        "hub": "Блог компании Eaton Habr Восстановление данных *Будущее здесь Инженерные системы ",
        "suit": "\n      www.eaton.com\n    ",
        "date": "1  января  1911"
    },
    {
        "url": "https://habr.com/ru/post/589347/",
        "title": "Как я реализовал git-flow для SQL",
        "tag": "sql servergitc#gitlabteamcity",
        "body": "Если у Вас в команде используется MSSQL Server и у Вас есть хранимые процедуры, функции или представления которые используют Ваши приложения либо они используются для интеграции данных с другими системами то возможно эта статья для Вас. Все началось с маленькой ошибки при интеграции, очень важных данных, с связанного сервера MSSQL. Нужно было внести правки в хранимую процедуру по интеграции данных с другого сервера. Открыв код процедуры мы обнаружили что код  был изменен. В обычном процессе все пакеты по изменению объектов базы данных скидываются на администраторов и они после проверки их  применяют на тестовую среду, после тестирование процедуры применяются на продуктивную среду. В компании бывают моменты, когда происходит какой то сбой ночью либо в выходной, специалисты поддержки 24/7  вызванивают разработчика и администратора, администраторы выдают временные права и разработчик решает проблему. Но когда и кем были внесены изменения не ясно. Были ли они внесены намеренно или случайно(например в следствии правки другой процедуры открыл случайно и начал менять эту) тоже непонятно. Для поиска исходной процедуры первым делом пробовали поднять бэкапы базы данных для выяснения когда была внесена правка, этот процесс затянулся на столько долго что проще было переписать процедуру с нуля. В итоге нашли корректный вариант и вернули, дальше выяснять не стали что бы не терять времени. Это была не первая причина для поиска решений позволяющих использовать git для SQL, но данная проблема побудила нас к действию. После чтения кучи статей и развертывания разных систем управления исходным кодом для SQL Server, приложений которые позволяют сливать коды в гит и встраиваются в Management Studio в песочнице, мы так и не смогли найти подходящее нам решение: какие - то не поддерживали нашу версию MSSQL Server, некоторые пробные версии вообще не ставились и выдавали ошибку.Проанализировав ситуацию мы решили это своими руками. Нужен был инструмент для быстрой работы с git и SQL что бы не пришлось держать открытыми несколько приложений либо Management Studio и консоль.Гит должен был отображать корректный SQL  и мы могли его подвязать на Code Review и CI/CD.Должно быть все просто без особых танцев с бубном что бы разработчиков не нагружать лишней работойШаг первый. Для работы с SQL и GIT был выбран инструмент Azure Data Studio. По началу он показался не особо удобным, но это дело привычки сейчас проблем нет. Особенно для тех кто использует в своей разработке VSCode  привыкнуть будет легко.Поскольку основная структура таблиц генерируется из системы(CodeFirst), код создания таблиц мы выгружать не стали, выгрузили в папку только все хранимые процедуры, функции и представление в итоге получилась вот такая структура:Так же добавили папку для разных SQL плюшек упрощающих жизнь.  Для упрощение будущего кода на CI был введен стандарт именования: scheme.Name.Type.sql например хранимые процедуры в схеме dbo: dbo.CalculateAcidNetto.Procedure.sql, представления: dbo.CalculateAcidNetto.View.sql и тд.Любой файл можно открыть и выполнить для проверки в любой среде, прописав строку подключение к тестовому серверу.Шаг второй. залить данную папку на GIT. Поскольку в компании есть строгий Code style для SQL нужно было что бы GitLab корректно отображал файлы и можно было проводить Code review и писать замечания. Немного потанцевав с бубном возле кодировки файлов (MSSQL выгружает файлы не в UTF-8 и GitLab не отображал их корректно) мы смогли добиться желаемых результатов достаточно легко.Мы ведем разработку по принципу непрерывной интеграции, при слиянии каждого Merge Request  данные публикуются на тестовый сервер и доступны для тестирования по окончанию спринта все протестированные фитчи идут в срез на продуктивный сервер. Шаг третий. Нужно связать данный репозиторий и CI/CD в нашем случае это TeamCity.  Идея такая после слияние в ветку для теста все скрипты должны обновляться в тестовой среде(на тестовом сервере). И по такому же принципу обновлять продуктивную среду. В этом случае даже если кто то залезет и поправит код на прямую(сейчас это запрещено) то можно будет вернуть все на рабочее, согласованное и протестированное состояние. Вариантов решения было не много написать скрипт на PowerShell который берет папку с билда, и пробегая по всем SQL файлам проверяет есть там данный объект, если есть то обновляет его, если нет то создает либо написать консольное приложение на C#.Выбрали второй вариант просто потому что проще, подвязан в GIT и может автоматически публиковаться в нужную папку на сервере с CI. CI запускает данную утилиту, путь к папке с билдом и имя строки подключения передает параметрами.И вот все заработало. Что бы не быть голословным приведу скрипты из утилитки. Program.cs выглядит достаточно просто:IFileExecuter fileExecuter = new SqlFileToDBExecuter(conectionString);\n\nvar updater = new UpdateScriptService(fileExecuter, \".git\", \"UsefulSQLQueries\");\n\nupdater.UpdateFromTheDirectory(path, isRelease);\n\nConsole.WriteLine(\"Процедуры только для PROD:\\n\" + string.Join(\", \\n\", updater.ProductionOnly));\n            Console.WriteLine(\"\\n\");\n            Console.WriteLine(\"Процедуры требующие внимания:\\n\" + string.Join(\", \\n\", updater.Warnings));\n            Console.WriteLine(\"\\n\");\n            Console.WriteLine(\"Оставшиеся ошибки (файл - ошибка):\\n\" + string.Join(\", \\n\", updater.Errors.Select(x => x.Key + \" - \" + x.Value)));Где :conectionString -  это строка подключения которую мы получили(либо получили название и подтянули её из кофига),path -  это путь до билда с файлами.IFileExecuter - интерфейс, на случает если мы решим сменить БД можно будет варьировать реализацию.    public interface IFileExecuter\n    {\n        void Execute(ExecutingModel model);\n    }Код SqlFileToDBExecuter:using System.Collections.Generic;\nusing System.Data.SqlClient;\nusing System.Text.RegularExpressions;\n\nnamespace Util.UpdateDatabaseScripts\n{\n    public class SqlFileToDBExecuter : IFileExecuter\n    {\n        private readonly string _conectionString;\n\n        private const string ALTER = \"ALTER \";\n        private const string CREATE = \"CREATE \";\n\n        public SqlFileToDBExecuter(string conectionString)\n        {\n            _conectionString = conectionString;\n        }\n\n        public List<string> ProductOnlyCollection { get; set; }\n\n        private T SQLQuery<T>(SqlConnection connection, string sql)\n        {\n            using (SqlCommand command = new SqlCommand(sql, connection))\n            {\n                using (SqlDataReader reader = command.ExecuteReader())\n                {\n                    reader.Read();\n                    return reader.GetFieldValue<T>(0);\n                }\n            }\n        }\n\n        private void SQLExec(SqlConnection connection, string sql)\n        {\n            using (SqlCommand command = new SqlCommand(sql, connection))\n            {\n                command.ExecuteNonQuery();\n            }\n        }\n\n        public void Execute(ExecutingModel model)\n        {\n            using (SqlConnection connection = new SqlConnection(_conectionString))\n            {\n                connection.Open();\n\n                UpdateScript(model.Name, model.Content, model.Folder, model.Type, connection);\n\n                connection.Close();\n            }\n        }\n\n        private string UpdateScript(string name, string text, string folder, string type, SqlConnection connection)\n        {\n            string checkedIfExist = \"\";\n\n            if (type.Trim().ToUpper() != \"FUNCTION\")\n            {\n                checkedIfExist = $\"select count(*) FROM sys.{folder} where name = '{name}'\";\n            }\n            else\n            {\n                checkedIfExist = @$\"SELECT COUNT(*)\n                  FROM sys.sql_modules m \n                  INNER JOIN sys.objects o ON m.object_id=o.object_id\n                  WHERE type_desc like '%function%' and name='{name}'\";\n            }\n\n            var count = SQLQuery<int>(connection, checkedIfExist);\n\n            var pattern = @\"(CREATE|ALTER) *\" + type;\n\n            Regex regex = new Regex(pattern, RegexOptions.IgnoreCase);\n\n            var replaceTo = \"\";\n\n            if (count == 0)\n            {\n                replaceTo = CREATE + type.ToUpper();\n            }\n            else\n            {\n                replaceTo = ALTER + type.ToUpper();\n            }\n\n            text = regex.Replace(text, replaceTo);\n\n            SQLExec(connection, text);\n\n            return text;\n        }\n    }\n}\nДанный файл создает/обновляет объекты базы данных. Реализован по патерну команда.В данном листинге кода видно что представления и хранимые процедуры находятся в одних системных таблицах, а функции в других, название папки очень помогло что бы не заводить данные объекты константами(исключение только фукнции).UpdateScriptService:using System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\n\nnamespace Util.UpdateDatabaseScripts\n{\n    public class UpdateScriptService\n    {\n        private readonly IFileExecuter _fileExecuter;\n        private string[] _excludes;\n\n        /// <summary>\n        /// Количество проходов при создании объектов БД        \n        /// </summary>\n        private const int COUNT_REPEATS = 3;\n        public const string WARNING = \"--WARNING\";\n        public const string PRODUCTION_ONLY = \"--PRODUCTION_ONLY\";\n\n        public UpdateScriptService(IFileExecuter fileExecuter, params string[] excludes)\n        {\n            _fileExecuter = fileExecuter;\n            _excludes = excludes;\n\n            ProductionOnly = new List<string>();\n            Warnings = new List<string>();\n            Errors = new List<KeyValuePair<string, string>>();\n        }\n\n        public List<string> ProductionOnly { get; set; }\n\n        public List<string> Warnings { get; set; }\n\n        public List<KeyValuePair<string, string>> Errors { get; set; }\n\n        public void UpdateFromTheDirectory(string rootDirectory, bool isRelease)\n        {\n            var absolutePath = rootDirectory;\n            var folderEntries = Directory.GetDirectories(absolutePath);\n            var typeFolder = \"\";\n\n            foreach (string folderName in folderEntries)\n            {\n                if (_excludes.Any(x => folderName.Contains(x)))\n                {\n                    continue;\n                }\n\n                typeFolder = Path.GetFileName(folderName);\n                string[] fileEntries = Directory.GetFiles(folderName);\n\n                var errorsFile = new List<KeyValuePair<string, string>>();\n\n                var forDoing = fileEntries.ToList();\n\n                for (int i = 0; i < COUNT_REPEATS; i++)\n                {\n                    foreach (string fileName in forDoing)\n                    {\n                        try\n                        {\n                            var name = Path.GetFileName(fileName).Split(\".\")[1];\n                            var type = Path.GetFileName(fileName).Split(\".\")[2];\n\n                            string text = System.IO.File.ReadAllText(fileName);\n\n                            var model = new ExecutingModel\n                            {\n                                Folder = typeFolder,\n                                Name = name,\n                                Type = type,\n                                Content = text\n                            };\n\n                            if (text.Contains(WARNING))\n                            {\n                                Warnings.Add(fileName);\n                            }\n\n                            if (text.Contains(PRODUCTION_ONLY))\n                            {\n                                ProductionOnly.Add(fileName);\n                            }\n\n                            if (!isRelease)\n                            {\n                                if (text.Contains(PRODUCTION_ONLY)) continue;\n                            }\n\n                            _fileExecuter.Execute(model);\n                        }\n                        catch (Exception ex)\n                        {\n                            errorsFile.Add(new KeyValuePair<string, string>(fileName, ex.Message));\n                        }\n                    }\n\n                    if (errorsFile.Count() > 0)\n                    {\n                        forDoing = errorsFile.Select(x => x.Key).ToList();\n\n                        if (i == COUNT_REPEATS - 1) Errors.AddRange(errorsFile);\n\n                        errorsFile.Clear();\n                    }\n                    else\n                    {\n                        break;\n                    }\n                }\n            };\n        }\n    }\n}\nДанный файл перебирает все файлы в папке, находит наши пометки в SQL: --WARNIN - файл требует внимание - так мы помечаем скрипты которые либо не всегда работают, либо их нужно оптимизировать и пока мы наблюдаем за работой.--PRODUCTION_ONLY - данные скрипты работают только на продуктивной среде, бывают случаи когда имитировать интергацию либо линковать сервер в тестовой среде просто нзапрещено , данные скрипты на тестовые среды не накатываются.При выводе всех данных в консоль мы получает отчет в логах TeamCity:Или можно отправить его на почту.Для развитие данной системы управления исходным кодом SQL объектов мы планируем реализовать выборочное обнволение для этого нам нужно получить по коду MR с гита список файлов которые изменились и передать их утилите тогда утилита сможет обновлять только измененые файлы а не постоянно обновлять все объекты.Я надеюсь эта статья окажется полезной, спасибо за внимание.",
        "user": "\n      kapec_art\n    ",
        "time": "сегодня в 19:48",
        "ratings": " 24.98 \n    Рейтинг\n  ",
        "hub": ".NET *SQL *Git *",
        "suit": "\n      www.eaton.com\n    ",
        "date": "1  января  1911"
    },
    {
        "url": "https://habr.com/ru/post/589341/",
        "title": "Автоматизируем рутину в работе с HTML/CSS/JS",
        "tag": "автоматизацияавтоматизация для самых маленькихавтоматизация версткиhtmlhtml5html-версткаjavascriptсреда разработкиfrontend",
        "body": "Строим простую среду разработкиNode.JS + NVMДля работы нам понадобится среда выполнения Node.js и менеджер версий nwp (опционально)Node или Node.js — программная платформа, основанная на движке V8, превращающая JavaScript из узкоспециализированного языка в язык общего назначения.Node Version Manager, чаще называемый nvm, является наиболее популярным средством установки нескольких версий Node.jsNodeJS потребуется для установки пакетов, модулей и зависимостей проекта, а NVM — для установки, удаления и переключения версий NodeJS. Переключение потребуется вам при работе с проектами, которые написаны под более новую или старую версию NodeJS.Визуализация работы NVMОсновные команды NVMСписок установленных версийnvm lsСписок доступных версийnvm list availableУстановка последней версииnvm install latestУстановка версии из списка доступныхnvm install <version>Переключиться на другую версиюnvm use <version>По желанию вы можете попробовать другие менеджеры версий.nvs (Node Version Switcher) — это кроссплатформенный вариант nvm с возможностью интеграции с VS Code.Volta — это новый диспетчер версий, созданный командой LinkedIn. Заявлено, что он отличается увеличенной скоростью и межплатформенной поддержкой.Структура файлов и каталоговДля начала давайте ознакомимся с имеющееся структурой каталогов.Была разработана достаточно продуманная структура файлов и каталогов. Она позволяет удобно взаимодействовать нескольким программам.Псевдографическое дерево структуры├───portfolio\n└───projects\n    ├───in-process\n    │   ├───builds\n    │   │   └───aigen31.github.io\n    │   ├───sources   \n    │   └───tasks\n    └───readyОписание каждого из каталогов/portfolio (опционально) — сохранение превью для портфолио, далее они идут на публикацию на freelance-площадку./projects/in-proggress — проекты, находящиеся в процессе разработки или вероятнее всего будут дорабатываться./projects/ready — разработанные проекты./projects/builds — папка с собранными сборщиком проектами, которые синхронизируются с удалённым хостингом проектов (в моём случаю GitHub)./projects/sources — проекты с исходным кодом, над которыми происходит работа./projects/tasks — задачи автоматизации Powershell.Gulp для решения рутинных задачВизуализация работы Gulp.jsО моей сборкеДля моих проектов и этой статьи была подготовлена сборка Gulp, вдохновлённая сборкой другого разработчика WebDesign Master. Она была доработана под общие нужды и избавлена от некоторого лишнего кода, который уже был неактуальным с новыми версиями некоторых модулей.Её полный разбор будет в другой статье.Основные функции сборщикаМинифицирует CSS, JS и сжимает изображения.Конвертирует шрифты в разные форматы.Встраивает HTML код из одного HTML файла в другой.Запуск проектаЗагрузите проект из Git, распакуйте архив и пройдите до корневого каталога.В корневом каталоге зайдите в контекстное меню (через ПКМ) с зажатым Shift > \"Открыть с помощью Powershell\".Контекстное менюВведите команду для разрешения исполнения неподписанных скриптовset-executionpolicy remotesignedУстановите Gulp глобально (ключ -g)npm i gulp -g Инициализируете проект, вводя ваши данныеnpm initУстановите зависимости для проектаnpm iЗапустите проект, используя командуgulpПосле этого начнётся запуск задач по умолчанию, в том числе запуск локального сервера BrowserSync, который мониторит изменения HTML/CSS/JS файлов и шрифтов с изображениями.Сохранение актуальности GulpЧтобы данная сборка оставалась актуальной долгое время, пакеты с модулями нужно своевременно обновлять и при необходимости переписывать опции модулей в gulpfile.js.Для этого нам нужно глобально установить пакет npm-check-updates.npm-check-updates обновляет в package.json ваши зависимости, игнорируя указанные версииОсновные командыПроверка версий пакетовncuОбновление версий в packaje.jsonncu -uПосле данных команд выполняем установку пакетовnpm iРабота с GitGit — распределённая система управления версиями. Проект был создан Линусом Торвальдсом для управления разработкой ядра Linux, первая версия выпущена 7 апреля 2005 года. На сегодняшний день его поддерживает Джунио Хамано.Визуализация системы управления версиями. Источник:https://dev.to/lydiahallie/cs-visualized-useful-git-commands-37p1Установка Git рекомендуется всем разработчикам, т. к. приходится работать не только с вашими, начатыми с нуля проектами, но и чужими, где уже имеется готовый код.Для установки рекомендуются базовые знания английского или переводчик, так вы можете поменять настройки под себя и отключить лишние функции. К примеру я убираю пункт с установкой Git Bash.Git Bash — это командная строка, с помощью которой пользователи могут использовать функции Git. Он эмулирует среду bash в Windows и позволяет пользователю использовать большинство стандартных команд Unix.Загружаем и устанавливаем Git.Основные командыИнициализация проектаgit initДобавление файлов в репозиторийgit add <file_name> или точка \".\" (для добавления всех файлов)Создание коммита с комментарием (ключ -m)git commit -m \"comment\"Обновить удалённую ссылку со связанными объектами. Проще говоря — синхронизировать ваш локальный Git с удалённым (к примеру GitHub)git push -u origin master (может быть main, если вы поставили данную опцию в Git)Создание новой ветви и переход к ней (ключ -b)git checkout -bПереход к другой ветвиgit checkoutСлияние с веткой (вы должны находиться в ветке, в которую будут включаться изменения)git mergeЭто команды, которые помогут вам работать с простыми проектами и манипулировать ветками.Автоматизация с PowershellPowerShell — расширяемое средство автоматизации от Microsoft с открытым исходным кодом, состоящее из оболочки с интерфейсом командной строки и сопутствующего языка сценариев.Push в Git при мониторинге измененийБыл разработан один из базовых сценариев Powershell, который позволяет избавиться от постоянной монотонной синхронизации с удалённым хостингом Git. Он находится в папке task и мониторит папку /builds/aigen31.github.ioОн работает на основе объекта FileSystemWatcher, который мониторит изменения в указанном каталоге и вызывает по порядку командыgit status\ngit add .\ngit commit -m 'update'\ngit push -u origin masterСчитается, что для простых проектов не стоит описывать каждый коммит, т. к. вероятнее всего, над его вёрсткой занимаетесь только вы. В ином случае стоит отказаться от данной автоматизации и писать коммиты осмысленоВы можете дописывать сценарии автоматизации по своему желанию, их расположение идеально подходит для манипуляции с проектами. Можете написать в комментариях, что хотели бы предложитьРепозиторий в GitДля того, чтобы сохранять ваши работы удалённо, рекомендую использовать GitHub в качестве хоста.Если репозиторий является веб-страницей, то её можно будет подключить к GitHub Pages и использовать в качестве хостинга статичных сайтов.Бесплатная синхронизация с OneDriveСервис OneDrive от Microsoft позволяет синхронизировать рабочие папки (рабочий стол, документы, изображения) с облаком. Для сохранности своих данных рекомендуется синхронизировать в крайнем случае рабочий стол и документы, т. к. изображения на практике могут иметь большой вес, а в бесплатном тарифе OneDrive можно хранить лишь 5гб.Для увеличения объёма хранилища до 1тб. придётся покупать пакет Office 365 (2300 и более руб./год). Это составляет около 200 руб./месяцДругие облачные хранилищаЕсли сравнить Office 365 с Яндекс 360, то при оплате второго сервиса за месяц цена будет равной. Но при полной оплате за год стоимость будет сильно отличаться — 1399 руб./год В Яндекс.Диске так же имеется возможность синхронизации рабочих папок, но она отсутствует в бесплатном тарифе.Рекомендую для начала опробовать OneDrive для синхронизации, цена не очень разная.В облаке для безопасности я советую хранить закодированный файл-ключ менеджера паролей KeePassXC (без него мы не сможем войти в базу), о котором будет рассказано в следующем разделе.Локальный менеджер паролей KeePassXCKeePassXC - бесплатный менеджер паролей с открытым исходным кодом. Все началось с сообщества KeePassX. Он построен с использованием библиотек Qt5, что делает его мультиплатформенным приложением, которое можно запускать в Linux, Windows и macOSХранение паролей в KeePassXC и похожих менеджерах имеет ряд плюсов:Можно добавлять пароли из таблицы CSV и любых других мест (игровые аккаунты, различные браузеры и сервисы).Защита с помощью закодированного файла и пароля, который имеется только у вас. Его стоит хранить в нескольких местах, чтобы точно не потерять. Иначе вы утеряете доступ к базе данных.Сортировка по группам.Существуют онлайн менеджеры паролей и зачастую они платные. Этот менеджер хранит всё локально и он бесплатный. От подобных онлайн платформ смысл появляется тогда, когда нужно открыть /общий доступ к паролям, в нашем случае такое не требуется.Подключение по FTP и SSH посредством встроенных функций Windows 10FTPЕсли вам не нужны все возможности различных FTP-клиентов, то вы можете спокойно подключаться к серверу через проводник с помощью этих действий:Зайдите в контекстное менюВыберете пункт \"Добавить новый элемент в сетевое окружение\"Следуете инструкциямSSH через OpenSSH от MicrosoftOpenSSH — это средство подключения для удаленного входа, использующее протокол SSHПроверьте наличие OpenSSH через \"Параметры\" > \"Приложения\" > \"Приложения и возможности\" > \"Дополнительные компоненты\". Вероятнее всего, он уже будет установлен, как это было у меня, в ином случае установите клиент через пункт \"Добавить компонент\".OpenSSH в \"Параметрах\"Подключение происходит с помощью следующей команды в терминалеssh username@servernameЗаключениеМы поверхностно разобрались в среде разработки для работы с HTML/CSS/JS: установили NodeJS и менеджером версий, немного изучили автоматизацию работы с помощью Gulp и Powershell (более подробный разбор будет в другой статье), узнали про применение Git для гибкого изменения проектов и их хранения на удалённом хостинге. После этой статьи некоторые разделы будут разветвляться на другие, более углублённые.Так же не забыли про безопасность и сохранность данных, которая очень нужна, чтобы был обходной путь при потери или поломки рабочего устройства, либо же кражи или утечки информации.Надеюсь, это было полезно для вас, спасибо за чтение статьи!Мой Github с проектамиТерминологияЗависимость — объект, который может быть использован как сервис.Инициализация — создание, активация, подготовка к работе, определение параметров. Приведение программы или устройства в состояние готовности к использованию.Ключ — опция, которая немного изменяет действие командыКоммит — операция, которая отправляет последние изменения исходного кода в репозиторий, делая эти изменения частью основной ревизии репозиторияМодуль — это просто файл. Один скрипт – это один модульНативный (родной) код — это код, поставляемый разработчиками технологии.Пакет — файл или каталог, который описывается файлом.Репозиторий — место, где хранятся и поддерживаются какие-либо данные.Терминал — программа или устройство, используемая для взаимодействия пользователя с компьютером или компьютерной системой, локальной или удалённойСистема управления версиями — программное обеспечение для облегчения работы с изменяющейся информацией.Среда выполнения — вычислительное окружение, необходимое для выполнения компьютерной программы и доступное во время выполнения компьютерной программы.Источникиhttps://ru.wikipedia.org/wiki/Node.jshttps://docs.microsoft.com/ru-ru/windows/dev-environment/javascript/nodejs-on-windowshttps://github.com/aigen31/gulphttps://www.npmjs.com/package/npm-check-updateshttps://ru.wikipedia.org/wiki/Githttps://ru.wikipedia.org/wiki/PowerShellhttps://en.wikipedia.org/wiki/KeePassXChttps://docs.microsoft.com/ru-ru/windows-server/administration/openssh/openssh_install_firstuse",
        "user": "\n      aigen31\n    ",
        "time": "сегодня в 19:30",
        "ratings": " 24.98 \n    Рейтинг\n  ",
        "hub": "JavaScript *Системы управления версиями *HTML *Облачные сервисы ",
        "suit": "\n      www.eaton.com\n    ",
        "date": "1  января  1911"
    },
    {
        "url": "https://habr.com/ru/post/589337/",
        "title": "Виртуальная «Тройка» + Samsung Pay. Промежуточные итоги тестирования",
        "tag": "ТройкаМетроТранспортоплата проезда",
        "body": "В Москве тестируется новый способ оплаты проезда в общественном транспорте Москвы - виртуальная карта \"Тройка\" - удобный, но при доскональном следовании инструкциям работать не будет.Я состою в фокус-группе тестирования данного функционала. Поделюсь первыми впечатлениями.Телефон Samsung Galaxy S8. Платежное приложение Samsung Pay. Проверка на турникетах Московского Метрополитена (станция Бибирево, западный вход - левый и правый турникеты, станция Домодедовская, западный вход - правый турникет).Отмечу плюсы и минусы, выявленные мной по итогам 5 дней с начала тестирования.ВАЖНО: обновление приложения Samsung Pay необходимо делать до покупки абонемента! (пояснения далее).Сразу отмечу, что столкнулся с проблемами, которые не позволяли мне использовать сервис. Но в итоге я их смог устранить.Проблема и решение10.11.2021 мне пришло на почту письмо от Мосметро с инструкциями по подключению функционала:Вот, что вам нужно сделать далее:1. Следуйте подробной инструкции https://virtual-troika.mosmetro.ru/guide (начиная с раздела «Вступление в бета-тестирование») для установки тестовой версии приложения «Метро Москвы» и выпуска виртуальной «Тройки». Покупка билета производится за счет денежных средств пассажира.2. Для тестирования виртуальной «Тройки» используйте карту на турникетах и валидаторах, которые принимают бесконтактные банковские карты.Оплата проезда с Google Pay: Поднесите телефон с включенным экраном к терминалу, при этом разблокировать телефон не требуется, карта считается со смартфона автоматически.Оплата проезда с Samsung Pay:Убедитесь, что у Вас установлена специальная тестовая версия Samsung Pay по ссылке https://www.samsung.com/ru/apps/mobile/samsungpay/test/troika/ Не запускайте Samsung Pay для оплаты поездки: Тройка работает только в фоновом режиме. Смартфон можно прикладывать к терминалу даже с заблокированным и выключенным экраном.3. После прохождения тестирования заполните форму обратной связи https://docs.google.com/forms/d/e/1FAIpQLSf1WS54W2EsM4V94DuEX98l7h-b59tm4O3dy7srLWQSNecivw/viewform. При возникновении вопросов в ходе тестирования, пожалуйста, обратитесь к инструкции. Если ваш вопрос не решен, пишите на почту questions@mobile-mosmetro.ru или чат-боту в приложении «Метро Москвы».Спасибо за помощь!Начал настройку утром 11.11.2021. Выполняя действия последовательно, я сначала установил приложение \"Метро Москвы\" из пункта 1, оплатил абонемент на 30 дней, привязал виртуальную Тройку к Samsung Pay. К слову, она отобразилась в приложении как банковская карта. Затем приступил к пункту 2 данного письма - начал обновлять приложение Samsung Pay. Боясь потерять привязанные карты, в том числе и Тройку, я поставил тестовое приложение Samsung Pay поверх существующей версии, но оплата проезда не срабатывала - турникет выдавал сообщение \"Ошибка. Отказ чтения карты\". Экран телефона никак не реагировал на контакт с турникетом.Отправил запросы в техническую поддержку:Адресат запросаОтветРазработчик приложения \"Метро Москвы\"Нет ответаЧат в приложении \"Метро Москвы\"Оператор сказала, что они данные вопросы не решаютНомер департамента транспорта 3210Оператор сказала, что они данные вопросы не решаютПоддержка Samsung PayНет ответаФорма обратной связи из письмаНет ответаE-mail questions@mobile-mosmetro.ruРяд односложных ответов (пересказ инструкции), которые попадали в спам и появлялись спустя длительное время\"Общение\" с поддержкой продлилось 11-12.11.2021, в это время я пользовался способом оплаты по банковской карте на турникетах.В это же время я предположил, что установка тестовой версии Samsung Pay поверх боевой прошла с ошибкой. Поэтому я снес приложение и установил тестовую версию с нуля. Все карты, кроме Тройки появились в приложении после авторизации. В приложении Метро Москвы отсутствовали кнопки \"Добавить в Samsung Pay\" или \"Добавить в Google Pay\". И как добавить уже выпущенную виртуальную карту в кошелек, никакой информации нет. Поддержка тоже не давала ответа.13.11.2021 понял, что в выходные дни мой вопрос никто не решит, и попробовал функцию \"Перенос\" в приложении \"Метро Москвы\". Судя по инструкции, она позволяет перенести виртуальную карту на другой телефон. После нажатия на кнопку \"Перенести\" (точное название не помню, так как сейчас кнопка недоступна, а количество переносов в месяц ограничено) Тройка перешла в статус \"Временно заблокирована\", а через 1 час на почту пришло письмо, что карта готова к переносу.Зайдя в приложение Метро Москвы, я увидел кнопки добавления карты в кошельки. Добавил карту в Samsung Pay, и она появилась в новом разделе \"Транспортные карты\", который появился в тестовой версии.После этого проход по виртуальной Тройке в метро работает. Удобство использованияДля прохода достаточно достать телефон и поднести его к турникету без разблокировки экрана (для прохода доступны только турникеты, принимающие оплаты по банковским картам). Подтверждать транзакцию пальцем, лицом или пин-кодом не нужно. Считывание происходит примерно за 1-2 секунды. Для сравнения - оплата по банковской карте примерно в 2 раза дольше.В приложении Метро Москвы фиксируется вся история проходов (в какое время на какой станции осуществлялся проход). На наземном транспорте пока не тестировал.В приложении Samsung Pay история отображается не полностью - отсутствует время совершения поездки, только дата.ИТОГОПлюсы функционала: Сокращает время прохода (снижает количество операций с телефоном до нуля)Собирает в одном месте статистику использованияДает возможность удобной и быстрой покупки абонементовМинусы функционала:Замечания к отображению истории в Samsung PayМинусы пилота:Недостаточная инструкция для простого пользователя, которая может привести к нерабочему сервисуПоддержка не решает вопросы",
        "user": "\n      maxon87\n    ",
        "time": "сегодня в 19:15",
        "ratings": " 24.98 \n    Рейтинг\n  ",
        "hub": "Платежные системы *Гаджеты Смартфоны Будущее здесь ",
        "suit": "\n      www.eaton.com\n    ",
        "date": "1  января  1911"
    },
    {
        "url": "https://habr.com/ru/post/589335/",
        "title": "Мы хватаемся за программы, а надо за цель",
        "tag": "помогителичная эффективностьпродуктивностьджедайские техникикак делать делакак вести конспектыкак не пропускать сроки",
        "body": "Вечер понедельника. Я в очередной раз сижу и выбираю программу, с которой я буду эффективно делать дела. А перед этим выбирал ту, которая будет мне эффективно напоминать заниматься спортом. А перед этим — вести бюджет. А утром я рассматривал все свои учётки в облаках и миллионы автозагруженных фотографий и недоумевал, откуда столько. И твёрдо решил, что теперь-то точно всё скачаю на ноут и залью в одно место. И менеджер паролей сменю. Да, как же. Со мной такое происходит примерно раз в полгода. Где-то взгляд зацепился за обзор новой классной проги, которая чуть эффективнее помогает двигать строчки в списке — и всё, надо скачать, установить, потратить время. Взорвать всю систему и переделать с нуля. Всё, чтобы через два дня забросить. Нормально, у всех так было.Но я не знаю о вас, зато знаю о себе, поэтому давайте просто расскажу всё, чтобы никому советов не давать.ОблакаУ меня, если память не врёт, есть терабайт на Яндексе, 15 гигов у гугла на заброшенном аккаунте и 5 на айклауде. Где-то живут заброшенный пустой дропбокс и диск от микрософта, но серьёзно, дропбокс?Но у гугла лежат все мои фотки за 10 лет, а айклауд удобно присобачен к macOS. С другой стороны, за терабайт на Яндексе я не плачу, потому что там работал, и они в какой-то момент дали пожизненный диск. Клёво, но приложение жуть какое неудобное.Смотрю на фото, ловлю ощущение — хочу все фотки скачать в одно место и разложить по годам. Или лучше по событиям. А потом по годам. И удалить лишнее. И альбомы завести и лица распознать. И бросаюсь, значит, заказывать выгрузку у гугла, чтобы эти 150 гигов (загруженных, пока было бесплатно) оттуда «спасти».Пока жду письмо счастья со ссылками на скачивание, ловлю другое важное ощущение — а зачем тебе это делать, Женя? Что ты собираешься делать с этой разложенной коллекцией? Ну посмотришь, какие у тебя были смешные одноклассники. Ну зайдёшь в семейные фотки. Окей, а дальше что? И сейчас можно так сделать.Ответа нет. Ссылка на выгрузку из гугла протухает от старости уже в четвёртый раз. Как-нибудь потом. Я вспоминаю, что давно хотел сменить везде пароли.ПаролиГде-то точно есть хранилище у ластпасса с одним мастер-паролем. Но давно не обновлял. А ещё дашлейн с другим мастер-паролем. Тоже давно не обновлял, половина паролей явно протухла. А ещё где-то лежит текстовый файлик с паролями — сразу видно, хорошего инфобезопасника родной университет выпустил, ничего не скажешь.Теперь всё это добро надо сгрести в кучу и обновить, потому что часть паролей точно потерялась в заброшенном файрфоксе, новые где-то в связке ключей у эпла, а ещё часть вроде свежих — в хроме. Наверное.И это всё при том, что уже три месяца как придумал себе правило, по которому генерируются пароли в голове, и никакие менеджеры вообще не нужны. Но оттуда же надо всё достать. Стоп, а зачем?От всего самого важного пароли уже в голове, для них есть правило. Если правила нет — попробуй просто восстановить на почту и сгенерируй новый пароль по правилу. Что ты мучаешься, зачем время тратить?Справедливо. Время это важно, не надо его тратить на ерунду. Вот бы была прога, чтобы все дела записывать, да?Списки делУ главреда много забот — целый список, который нужно где-то вести. На работе есть ноушен, чтобы там писать планы и доки, а ещё кликап, в котором можно писать планы и задачи ставить, а ещё трелло, чтобы писать планы и задачи ставить. А ещё встроенные заметки на ноуте. А ещё где-то есть загашник с уанноутом и эверноутом и ещё чётотам-ноутом. И ещё наверняка что-то, но список всех программ потерялся в какой-то ещё программе. Надо бы всё перенести в одно место, чтобы каждый раз не проверять десять программ.А ещё же личные дела — как я вспомню, что пора купить землю для пересадки цветов, если не запишу? Буду как дурак бегать вокруг магазина и за голову хвататься.Надо завести одну программу и туда все дела записать. Да ведь? Кажется, что нет, и вы поняли принцип, по которому строится эта статья.Никакая, даже самая клёвая прога (или новый ноут, или тринадцатый айфон, или мотивирующая книжка об успехе), не поможет писать, планировать, работать или учиться, если не разобраться, зачем это делать. Нужна цель, другими словами.Хоть ты тресни — новая прога для ведения списков дел не поможет вести список дел, если в неё не заглядывать. Новый не тормозящий ноут с ретина-дисплеем не добавит очков продуктивности, если тупить в нём в ютубчик. Напоминалки от напоминалки станут надоедать и отключатся. Приложка для занятий спортом удалится, чтобы не бесить — я знаю, я сам так делаю постоянно. Вот это, знаете, придумать, как оптимизировать мессенджеры, чтобы не отвечать всем в разных местах — по работе в слаке, маме в вайбере, дальним родственникам в ватсапе, хотя тебе самому бы и телеграма хватило за глаза. Надо постоянно себя одёргивать и спрашивать — а зачем? Дела не делаются, когда не понимаешь, зачем их делать — или не можешь хотя бы сформулировать. Дела не делаются, если вместо дел выбирать прогу для делания дел. Учёба на любых курсах уедет на задний план, если переживать, что не получается эффективно ей заниматься. Выбирать там приложение для конспектов. Или заказать уже пятый блокнот с озона, хотя в четырёх предыдущих пустые страницы.Надо найти цель. Или стимул. Эту статью я написал не потому что у меня там удобный редактор букв и ноут с хорошей клавиатурой, и не потому что она была в списке дел запланирована на вечер 15 ноября. А потому что появилась цель — разобраться, что идёт не так, и поговорить об этом. Вдруг я не один такой?А дальше как пойдёт, можно и прогу для автоматического подсчёта денег на всех картах скачать, если время сегодня останется. Как раз классную нашёл.Другое нытьё:Пост про кучу хлама на рабочем столеПост про то, что я никак не могу выбрать рюкзакПост про то, как я 13 лет делал футбольный симулятор на вижуал бейсике",
        "user": "\n      evil_me\n    ",
        "time": "сегодня в 19:10",
        "ratings": " 24.98 \n    Рейтинг\n  ",
        "hub": "Информационная безопасность *Читальный зал Софт Ноутбуки Лайфхаки для гиков ",
        "suit": "\n      www.eaton.com\n    ",
        "date": "1  января  1911"
    },
    {
        "url": "https://habr.com/ru/company/otus/blog/588923/",
        "title": "Тестовая документация и анализ требований",
        "tag": "game qaтестирование игртестовая документацияанализ требованийgame qa engineerчек-листы",
        "body": "В преддверии старта курса \"Game QA Engineer\" публикуем текстовую расшифровку онлайн-интенсива по курсу, который провела Надежда Чертовских — руководитель отдела QA в компании BeresnevGames и преподаватель OTUS.  Цели интенсива:познакомиться с основными видами тестовой документации;проанализировать документ от game-дизайнера;попрактиковать составление чек-листа. Для начала давайте обсудим такой животрепещущий вопрос: почему сегодня на курсе «тестировщик игр» мы обсуждаем документацию?Как тестировщик, который целый день только в игры играет и сообщает разработчикам об обнаруженных ошибках, связан с документацией? Какая вообще работа у тестировщика игры? Какие у тестировщика могут быть документы? Существует распространённое заблуждение, что тестировщик игр целый день только и делает, что в игры играет. Но на самом деле это не так. Тестировщик в геймдеве точно  такой же тестировщик, как и в любой другой сфере, и работает точно по такому же принципу, но продукт у него не web-страничка, не application на операционной системе, а игра (мобильная, консольная, десктопная). Тестирование может быть автоматизированным и ручнымНа интенсиве мы поговорим в целом об артефактах тестирования, с которыми работает тестировщик:План тестирования (Test Plan) Тест-кейс (Test Case) Чек-лист (Check-List)Баг-репорт (Bug Report)Отчёт о тестировании (Test Report)Инструкция (Manual)Из этого мы можем сделать вывод, что тестировщик не только читает требования, которые подготовили к продукту, но и сам генерирует документы. В ходе интенсива мы более подробно поговорили о 6 типах документов, которые перечислили выше, обсудили, какие из них полезные, какие используются чаще, какие меньше и составили чек-лист по требованиям. План тестированияПлан тестирование (далее ПТ) или тест-план – это большой документ, который чаще всего описывает весь объем работ по тестированию проекта либо части проекта (например, релиза или предрелизного билда). ПТ описывает, что будет тестироваться, в какие сроки, какими инструментами, какая команда, обязанности и ответственности каждого члена команды. Также часто в ПТ включается стратегия тестирования, график релизов на  несколько ближайших спринтов. В зависимости от команды бывает разная степень детализации ПТ и его могут делать разные люди в команде. В каких-то компаниях ПТ делает менеджер, в каких-то middle-тестировщик, либо senior-тестировщик, либо тимлид отдела тестирования. Всё, что мы далее обсудим по документам, которые генерирует тестировщик, может отличаться от компании к компании, от команды к команде. В зависимости от команды и компании форм-фактор всех документов может быть либо уже обговорён и установлен, либо, если вы приходите первым QA специалистом на проект, то вы сами устанавливаете, как удобно вам.Форм-фактор у тест-плана может быть разный (схема, интеллектуальная карта и т.д.) и зависит от того, как команде будет удобнее взаимодействовать с документами. Чаще всего ПТ требуется именно для людей, которые принимают решения по проекту, чтобы они поняли, что в следующий момент мы делаем: релизим билд или нужно подвинуть сроки, добавить к тестированию, убавить к каким-то другим срокам. И лучше всего не делать ПТ огромным, чтобы человек, который будет его читать, смог осилить весь объём. Если посмотреть примеры тест-планов в интернете — часто это одностраничная схема, чтобы все в общем и целом понимали, какой объем тестирование предстоит. Обычно план тестирования делается до начала тестирования и до момента релиза. Таким образом План тестирования:описывает стратегию тестирования, цели, график, оценку, результаты, а также ресурсы необходимые для тестирования;имеет разную степень детализации;имеет разный форм-фактор;составляется не более, чем на 2-х страницах;составляется до начала тестирования.Оставляю здесь шаблон на тест-планПример тест-плана с сайта с сайта www.guru99.comТест-кейсТест-кейс можно сравнить с рецептом — это последовательность шагов, которые приводят к какому-то результату. Тест-кейс лучше не делать избыточным. Тестировщики чаще всего хорошо знают свой проект, поэтому досконально писать тест-кейс нет необходимости. Тест-кейс должен быть краткий и понятный, так чтобы другой тестировщик, либо другой специалист в команде смог быстро пройти по нему и проверить, что все происходит так, как нужно. Тест-кейсы можно формировать в последовательный сценарий, чтобы проверить, как игрок пройдет по этому функционалу от начала до конца. Тест-кейсы можно группировать в смысловые блоки.Например, если в игре запускается какой-то ивент, формируется набор тест-кейсов для проверки этого ивента. Тест-кейсы лучше писать по требованиям гейм-дизайнерского документа. Но, если функционал уже готов, а требований тест-кейсов по нему не написано, можно написать уже по факту. Лишним не будет.Составляющие тест-кейса: идентификатор (уникальный номер, по которому вы сможете найти этот тест-кейс и на него сослаться);название сценария (какое-то краткое, но ёмкое); ссылка на требования ГДД;предусловия (опционально, если они требуются для тест-кейса);шаги сценария;ожидаемый результат;фактический результат (опционально).Пример тест-кейсаЧек-листЧек-листы можно сравнить со списком покупок, который мы формируем на проверку. Например, чек-лист на Smoke-тест, чтобы проверить, что игра запускается и весь функционал, который должен в игре отрабатывать отрабатывает, иконка приложения соответствует иконке нашего приложения. Также чек-лист может быть составлен на регрессионное тестирование и даже на тестирование требований. Чек-листы чаще всего составляются без детализации и их можно скомпоновать в наборы и проверять тоже для любого функционала либо нового, либо регрессионного. Чек-листы лучше сразу писать по требованиям (геймдизайнерскому документу) перед стартом тестирования функционала или по итогу.Небольшой пример из игры нашей студии: есть поле для ввода имени питомца и есть несколько условий на этом поле: имя питомца должно состоять из более чем 2 символов и только в этом случае кнопка из серой неактивной станет зеленой активной и можно будет питомца наименовать. Мы начинаем формировать чек-лист к этому полю если количество символов больше 2 то кнопка принять становится активное, если меньше 2 не активно. Первые 2 пункта чек-листа, которые можно проверить. На скриншоте мы видим, как игрок из Китая захотел назвать питомца очень коротким ёмким именем и, к сожалению, не смог это сделать и обращался в тех. поддержку. В результате ему пришлось выдумывать более длинное имя.Ссылка на mindmap чек-лист для мобильной игры:[Mindmap] Mobile Game Testing Checklistmeu-solutions.comБаг-репортБаг-репорт оформляется, когда баг уже локализован и его можно повторить. Если баг плавающий, нужно пытаться его повторить или занести в систему, где фиксируются баги, как плавающий баг. Ключевой момент, что баг можно повторить и воспроизвести, только тогда его заносят в систему с багами, где хранятся баг-репорты. Если создать и оформить какой-то баг, и разработчик не сможет его воспроизвести, то тут появится множество вопросов. Поэтому лучше всего сразу проверить на нескольких устройствах, если это возможно и посмотреть на разных операционных системах, на разных разрешениях экрана, то есть максимально локализовать проблему. В баг-репорте обязательно должны быть:Подробное описание проблемы – что, где, когда случилось. Важность дефекта, который указывает тестировщик, а уже приоритет по исправлению этой ошибки указывает менеджер либо команда из разработчиков. Условия воспроизведения – версия игры, версия операционной системы и другие уникальные условия, которые могут помочь разработчику быстро найти баг, устранить и передать задачу на тестинг.Алгоритм воспроизведения – пошаговые предусловия предусловия, которые необходимы для воспроизведения бага. Доказательства – скрины, видео, логи с устройств.Скриншоты из разных систем, в которых баг-репорт можно вести. В разных компаниях в разных командах условия могут быть абсолютно разные, и где хранятся баг репорты — также зависит от компании.Отчет о тестированииОтчет о тестировании пишется, когда функционал уж проверен и релиз либо предрелиз показывает итог проделанной работы. Отчет о тестировании может быть представлен как текст, таблица, график или диаграмма, если это позволяет инструмент. Составляющая отчёта о тестировании:Кто тестировал (состав команды).Когда тестировал (даты проведения тестов).Как тестировал (процесс тестирования, описание применяемых методов и технологий).Какие возникли проблемы и как решились. ИнструкцияИнструкцию можно писать до, во время или после тестирования. Инструкцию никогда не поздно написать. Это помогает как новичкам, так и коллегам, которые работают в одной команде. С помощью инструкции можно быстро сориентироваться в проекте. Например, вышел новый функционал. Лучше написать инструкцию, как этот функционал проверить, как переключаться, если проверка нового функционала подразумевает переключение между версиями или предусматривает какой-то сложный алгоритм проверки. Это экономит время на объяснения, когда требуется делегировать задачу либо в команду пришел новый человек и нужно его обучить. Также инструкция помогает выгрузить старое и не потерять. Скорость выпуска релизов в геймдеве довольно высокая и часто есть необходимость вернуться к старому функционалу, который ранее уже  тестировался, но прошло какое-то время и тестируется новый функционал, а нужно вернуться к проверке того старого. Поэтому лучше всего, чтобы было прописано, как  тестировать, где тестировать, что и куда переключать. Лучше всего все в картинках, гифках или видео. Сегодня современные инструменты всё это позволяют сделать быстро и без проблем. Также ели есть возможность сохранять какие-то состояния проекта, состояния продукта, то лучше где-то всё это фиксировать и выкладывать в общем доступе. Инструкции лучше писать сразу. Это позволяет избежать множество проблем в дальнейшем.Где хранить:Google Docs, Google SheetsCucumber (Hip Test)Test ITZephyr, Test Management for JiraQaseNotionTestRailГеймдизайнерский документ (ГДД, диздок)В геймдизайнерском документе гейм-дизайнер пишет требования к продукту или к отдельному функционалу. Детализация у геймдизайнерского документа может быть разная. Форм-фактор также может быть разным. Существует несколько способов проверки требований к игре: по принципу Что? Где? Когда? или по принципу проверки на полноту, однозначность, непротиворечивость, тестируемость, необходимость, осуществимость.После того как геймдизайнерский документ готов лучше всего, если его прочитают и вместе обсудят специалист по тестированию, разработчик и сам гейм-дизайнер. Тестировщику в этом случае следует задавать следующие вопросы: не противоречит ли те требования, которые гейм-дизайнер написал, функционалу, который сейчас есть, не будут ли нововведения противоречить наративу, функциям и механикам, которые есть в игре сейчас. Требования геймдизайнерского документы должны пониматься всеми однозначно, что исключает какого-либо двоякого толкования. Важно смотреть на полноту содержания: все ли условия предусмотрены, все ли сценарии, которые могут возникнуть у игрока в связи с новым функционалом продуманы.  Разработчик смотрит на возможность реализации ГДД.Также необходимо  продумать, как новый функционал будет тестироваться, после того как разработчик его реализует. Практическая часть интенсива. Мы попробуем сформировать чек-лист и вопросов гейм-дизайнеру по новому продукту на основе ГДД.На картинке мы видим 3 скрина игры. скрин – изображение и кнопка Play, при нажатии на которую, мы попадаем в игровой modскрин – на старте есть какое-то количество жизней и шаговскрин – при проигрыше попадаем на экран проигрыша, где написано итоговое количество набранных за игру баллов и кнопка сыграть ещё. Итоги интенсива:Узнали, какие бывают виды документации у тестировщика игр.Обсудили зачем тот или иной документ нужен.Попрактиковались в создании чек-листа. Список материалов для самостоятельного изучения:Блог Ольги Назиной (Киселевой)Форум сайта Software testing Блог LernQA  Шаблоны документов для тестирования программного обеспечения StrongQAСтатьи на Habr, DTFКонференции (QualityConf, SQADays, …)Каналы и чаты в telegramПоиск в GoogleУзнать подробнее о курсе Game QA Engineer можно здесь.",
        "user": "\n      MaxRokatansky\n    ",
        "time": "сегодня в 19:02",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": "Блог компании OTUS Тестирование игр *",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    },
    {
        "url": "https://habr.com/ru/post/589325/",
        "title": "Специализированная СХД как средство оптимизации IoT-данных",
        "tag": "схдbig dataiotfast datasmart city5gv2xotanvmeмедицина",
        "body": "Расширение цифрового пространства началось далеко не вчера, однако, из-за пандемии скорость течения этого процесса вынуждено увеличилась до беспрецедентного уровня, причем как в количественном, так и в качественном отношении. Государственным и частным организациям пришлось развернуть собственные цифровые системы, что привело к экспоненциальному росту внедрения технологий и сетей передачи данных. От автоматизированных цепочек поставок и систем по поддержанию социального дистанцирования до более ресурсоэффективных и удобных «умных» городов и транспортных средств — пространство интернета вещей (IoT) росло огромными темпами, и параллельно с этим увеличивался объем создаваемых данных. По прогнозам, к 2025 г. IoT-устройствами будет сгенерировано 73,1 зеттабайта данных.Эти данные надо не просто получать, но и хранить, обеспечивать к ним доступ и анализировать для получения ценной аналитической информации. Процесс, позволяющий выполнить такие задачи, должен опираться на комплексную информационную архитектуру, способную на протяжении всего жизненного цикла данных обрабатывать запросы на их использование, поступающие от самых разных приложений.Как выглядит жизненный цикл IoT-данных?Львиная доля IoT-данных хранится в облаке на больших накопителях объемом до 20 Тбайт. Этот огромный массив информации используется для выполнения завязанных на большие данные (big data) и быстрые данные (fast data) рабочих задач, таких как исследование генома, аналитика пакетных данных, прогнозное моделирование и оптимизация цепочки поставок.В некоторых сценариях использования на следующем этапе данные отправляются на периферию сети, часто для кэширования на распределенные периферийные сервера (edge servers) и последующего использования в таких приложениях и устройствах, работающих в реальном времени, как автономные автомобили, облачные игры, промышленные роботы и потоковая передача видео в 4K/8K. После этого мы, наконец, добираемся до конечных точек (endpoints), в которых данные генерируются оборудованием, умными устройствами и носимыми гаджетами, подсоединенными к сети. В данном случае для вариантов использования, сопряженных с большими потоками данных, основной задачей становится сокращение времени отклика сети (сетевой задержки) и увеличение пропускной способности между слоями (как от конечных точек в облако, так и в обратном направлении). Одним из возможных решений для этой задачи может стать технология 5G, в которой «информационные сверхскоростные магистрали» под нужды инновационных решений, наиболее чувствительных к времени задержки и величине пропускного канала, строились бы на миллиметровых волнах (mmWave) длиной от 20 до 100 ГГц.В чем ценность ваших IoT-данных?В нашем цифровом мире инфраструктура данных играет ключевую роль, поскольку хранение и анализ данных должны выполняться быстро, эффективно и безопасно. Таким образом, в рамках «ценностного» подхода роль архитектуры выходит за пределы простых регистрации и хранения и включает в себя преобразование информации и создание ценности для бизнеса. Приведем несколько примеров:Автономные автомобили буквально напичканы разными датчиками, камерами, лидарами, радарами и другими устройствами, генерирующими огромное количество данных (ожидается, что их ежедневный объем для одного транспортного средства достигнет 2 Тбайт). Эти данные ложатся в основу принятия в реальном времени решений, связанных с управлением транспортным средством, с опорой на такие технологии, как трехмерные карты, продвинутые системы помощи водителю (ADAS), обновления по беспроводной сети (over-the-air или OTA) и связь автомобиля со всеми объектами (Vehicle-to-Everything или V2X). Кроме того, IoT-данные повышают ценность персонализированного контента информационно-развлекательных систем и бортовых служб, улучшая впечатления пассажиров от взаимодействия с автомобилем. Принятие решений в реальном времени является важнейшей составляющей обеспечения безопасности пассажиров. В этой связи критически важной характеристикой информационной архитектуры, поддерживающей такие решения, является низкая задержка и высокая пропускная способность для облегчения прогнозного технического обслуживания.Носимые устройства медицинского назначения — По некоторым прогнозам, в 2021 году конечные пользователи потратят на носимые девайсы в общей сложности 81,5 млн. долл. США. Такие устройства генерируют важные данные, используемые для контроля за фазами сна, измерения пройденного за сутки расстояния и определения уровня питательных веществ и кислорода в крови. По этим IoT-данным можно выявить дневную, месячную и годовую динамику, которая может лечь в основу рекомендаций по выработке полезных для здоровья привычек с опорой на фактические данные. Кроме того, такие данные помогут подобрать схему лечения и профилактики, наиболее подходящую конкретному человеку, в особенности учитывая позитивную динамику в области телемедицины и оказания услуг здравоохранения в дистанционном режиме, которая сохраниться и после пандемии. В таком случае приоритетной задачей архитектуры СХД будет долгосрочное хранение критически важных данных о пациенте.Помимо этого, упомянутые далее сценарии использования IoT подтверждают важный тезис о том, почему подход к архитектуре СХД следует адаптировать под каждый конкретный случай и как можно удовлетворить соответствующие каждому сценарию требования.Поисково-спасательные беспилотные летательные аппараты (БПЛА) — это отличный пример сценария использования интернета вещей, который прекрасно иллюстрирует необходимость в узкоспециальном решении для хранения данных, повышающем ценность использования БПЛА. Такие дроны часто эксплуатируются в суровых природных условиях под воздействием экстремальных температур и плохой погоды. Поэтому решения для хранения данных, примененные в таких устройствах, должны обладать повышенной прочностью и надежностью. В качестве примеров можно привести высоконадежные промышленные e.MMC-накопители повышенной прочности и встроенные универсальные флеш-накопители UFS.  Кроме того, нередки случаи использования поисково-спасательных БПЛА в качестве одного из компонентов более разветвленной сети систем и устройств, что дает дронам возможность оптимизировать траектории полета и выполнять общие задания в автоматическом режиме. Это означает, что информационная архитектура должна быть масштабируемой и поддерживать одновременное использование нескольких технологий с высочайшей эффективностью, производительностью и надежностью.Умные города — Для того, чтобы умные города могли правильно функционировать, следует обеспечить хранение как архивных данных, так и данных в реальном времени. При анализе данных в реальном времени IoT-технологии опираются на хранилища, расположенные на периферии сети и в конечных точках. Так, например, для правильной работы умных систем общественного транспорта нужны оперативные данные о дорожном движении, ведь без них невозможно быстро и точно подстроиться под пиковые моменты спроса, возникающие, в частности, в часы пик. В этом плане такой сценарий использования чем-то напоминает умные автомобили, потому что важнейшей характеристикой СХД также будет минимальное время отклика.В качестве обратного примера: при хранении архивной информации скорость передачи данных в реальном времени не так важна, поскольку на передний план выходит гарантия их целостности в долгосрочной перспективе. В этом случае следует обратить внимание на облачные решения. Умные инструменты картографирования углерода служат еще одним примером использования IoT, выстроенным вокруг данных об углеродных выбросах за прошедшие периоды и сделанных на их основе прогнозах о динамике изменений и приоритетных направлениях, служащих для принятия мер по снижению таких выбросов.От общей к специализированной архитектуреВыбор способа хранения данных и наиболее подходящих методов извлечения из них ценой аналитической информации будет зависеть от технологии и сети передачи данных. СХД с поддержкой NVMe, например, идеально подходят для сценариев использования, в которых на передний план на протяжении всего цикла обработки данных выходят высокая производительность и низкая задержка. Поэтому для оптимального использования ценных IoT-данных требуется специализированное хранилище, и это следует учитывать уже на этапе проектирования всего комплекса информационной инфраструктуры.Впрочем, для управления корпоративными IoT-данными многие компании до сих пор используют серийную или неспециализированную архитектуру, хотя она не полностью закрывает постоянно меняющиеся потребности IoT-приложений и рабочих нагрузок как у частных, так и у корпоративных клиентов.Для поисково-спасательных БПЛА, например, приоритетами являются надежность и прочность, в то время как в цифровой медицине решения для хранения данных должны гарантировать целостность и безопасность критичной информации о пациентах в долгосрочной перспективе.Именно поэтому представляется необходимым перейти от типовых хранилищ к специализированным СХД и предлагать индивидуальные решения для конкретных проблем и сценариев использования. Для любой информационной инфраструктуры приоритетом является повышение ценности данных. В случае, если IoT-данные предполагается использовать в реальном времени, стратегия организации СХД должна быть выстроена специально под IoT и учитывать следующие соображения:Доступность: как обеспечивается поддержка, сетевое соединение и обслуживание?Устойчивость к износу: преобладающая операция – ЗАПИСЬ или ЧТЕНИЕ?Требования к хранилищу: какие данные и в каком объеме необходимо обрабатывать, анализировать и сохранять в конечных точках, на периферии сети и в облаке?Окружающие условия: высота над уровнем моря, температура, влажность и вибрации — в каких условиях предстоит принимать и хранить данные?Специализация как ключ к оптимизацииЗалогом оптимального использования всего потенциала меняющегося ландшафта IoT-данных является выбор специализированных и дающих уникальные бизнес-преимущества решений для хранения данных. В условиях, когда требования к разным IoT-приложениям кардинальным образом отличаются, не представляется возможным как прежде полагаться на какой-то один стандарт, на «универсальные» решения для хранения данных.Внедрение инновационных и специализированных решений для хранения данных поможет компаниям и организациям не заблудиться в быстро меняющемся IoT-пространстве и избежать необоснованных потерь ценных данных в процессе их обработки.  Линь ЧечунДиректор по маркетингу технической продукции Western Digital",
        "user": "\n      ChromaStone1789\n    ",
        "time": "сегодня в 18:32",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": "Big Data *Data Engineering *",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    },
    {
        "url": "https://habr.com/ru/company/otus/blog/589321/",
        "title": "Понимание утечек памяти в Java",
        "tag": "javaутечки памятиmemory leaksнагрузочное тестированиеload qa",
        "body": "1. ВведениеОдним из основных преимуществ Java является автоматизированное управление памятью с помощью встроенного сборщика мусора (или сокращенно GC). GC неявно заботится о выделении и освобождении памяти и, таким образом, способен решать большинство проблем, связанных с ее утечкой.Хотя GC эффективно обрабатывает значительную часть памяти, он не гарантирует надежного решения проблемы с ее утечкой. GC достаточно умен, но не безупречен. Утечки памяти все еще могут закрасться даже в приложения, созданные добросовестным разработчиком.По-прежнему возможны ситуации, когда приложение создает значительное количество лишних объектов, расходуя ресурсы памяти, что иногда приводит к его полному отказу.Утечки памяти — это настоящая проблема в Java. В этом руководстве мы рассмотрим, каковы потенциальные причины утечек, как распознавать их в рантайме и как справиться с ними в нашем приложении.2. Что такое утечка памятиУтечка памяти — это ситуация, когда в куче присутствуют объекты, которые больше не используются, но сборщик мусора не может удалить их из памяти и, таким образом, они сохраняются там без необходимости.Утечка памяти плоха тем, что она блокирует ресурсы памяти и со временем снижает производительность системы. Если с ней не бороться, приложение в конечном итоге исчерпает свои ресурсы и завершится с фатальной ошибкой java.lang.OutOfMemoryError.Существует два различных типа объектов, которые находятся в Heap-памяти (куче) — со ссылками и без них. Объекты со ссылками — это те, на которые имеются активные ссылки внутри приложения, в то время как на другие нет таких ссылок.Сборщик мусора периодически удаляет объекты без ссылок, но он никогда не собирает объекты, на которые все еще ссылаются. В таких случаях могут возникать утечки памяти:Признаки утечки памятиСерьезное снижение производительности при длительной непрерывной работе приложенияОшибка кучи OutOfMemoryError в приложенииСпонтанные и странные сбои приложенияВ приложении время от времени заканчиваются объекты подключенияДавайте подробнее рассмотрим несколько таких сценариев и как с ними бороться.3. Типы утечек памяти в JavaВ любом приложении утечка памяти может произойти по множеству причин. В этом разделе мы обсудим наиболее распространенные из них.3.1. Утечка памяти через статические поляПервый сценарий, который может привести к потенциальной утечке памяти, — это интенсивное использование статических переменных.В Java статические поля имеют срок жизни, который обычно соответствует полному жизненному циклу запущенного приложения (за исключением случаев, когда ClassLoader получает право на сборку мусора).Давайте создадим простую Java-программу, которая заполняет статический список:public class StaticTest {\n    public static List<Double> list = new ArrayList<>();\n\n    public void populateList() {\n        for (int i = 0; i < 10000000; i++) {\n            list.add(Math.random());\n        }\n        Log.info(\"Debug Point 2\");\n    }\n\n    public static void main(String[] args) {\n        Log.info(\"Debug Point 1\");\n        new StaticTest().populateList();\n        Log.info(\"Debug Point 3\");\n    }\n}Теперь, если мы проанализируем кучу во время выполнения этой программы, то увидим, что она увеличилась между точками отладки 1 и 2.Но когда мы оставляем метод populateList() в точке отладки 3, куча еще не убрана сборщиком, как это видно в ответе VisualVM:Однако в приведенной выше программе, в строке номер 2, если мы просто отбросим ключевое слово static, то это приведет к резкому изменению использования памяти, как показывает отклик:Первая часть до точки отладки почти не отличается от того, что мы получили в случае static. Но на этот раз после выхода из метода populateList() вся память списка очищается, поскольку у нас нет на него ссылок.Следовательно, нам нужно очень внимательно следить за использованием статических переменных. Если коллекции или большие объекты объявлены как статические, то они остаются в памяти на протяжении всего времени работы приложения, тем самым блокируя жизненно важную память, которую можно было бы использовать в другом месте.Как предотвратить это?Минимизируйте использование статических переменныхПри использовании синглтонов полагайтесь на имплементацию, которая лениво, а не жадно загружает объект.3.2. Через незакрытые ресурсыВсякий раз, когда мы создаем новое соединение или открываем поток, JVM выделяет память для этих ресурсов. В качестве примера можно привести соединения с базой данных, входные потоки и объекты сессий.Забыв закрыть эти ресурсы, можно заблокировать память, что сделает их недоступными для GC. Это может произойти даже в случае исключения, которое не позволяет программному процессу достичь оператора, выполняющего код для закрытия этих ресурсов.В любом случае, открытые соединения, оставшиеся от ресурсов, потребляют память, и если с ними не разобраться, они могут ухудшить производительность и даже привести к ошибке OutOfMemoryError.Как предотвратить это?Всегда используйте блок finally для закрытия ресурсовКод (даже в блоке finally), закрывающий ресурсы, сам не должен содержать исключений.При использовании Java 7+ можно использовать блок try-with-resources.3.3. Неправильная имплементация equals() и hashCode()При определении новых классов очень распространенной ошибкой является отсутствие надлежащих переопределенных методов для equals() и hashCode().HashSet и HashMap используют эти методы во многих операциях, и если они переопределены неправильно, то могут стать источником потенциальных проблем с утечкой памяти.Давайте рассмотрим как пример тривиальный класс Person и используем его в качестве ключа в HashMappublic class Person {\n    public String name;\n    \n    public Person(String name) {\n        this.name = name;\n    }\n}Теперь мы вставим дубликаты объектов Person в Map, использующую этот ключ.Помните, что Map не может содержать дубликаты ключей:@Test\npublic void givenMap_whenEqualsAndHashCodeNotOverridden_thenMemoryLeak() {\n    Map<Person, Integer> map = new HashMap<>();\n    for(int i=0; i<100; i++) {\n        map.put(new Person(\"jon\"), 1);\n    }\n    Assert.assertFalse(map.size() == 1);\n}Здесь мы используем Person в качестве ключа. В связи с тем, что Map не допускает дублирования ключей, то мы вставили в качестве ключа дубликаты объектов Person, что не должно увеличивать память.Но поскольку мы не определили правильный метод equals(), дубликаты объектов накапливаются и увеличивают память, поэтому в памяти мы видим больше одного объекта. Куча в VisualVM в этом случае выглядит следующим образом:Однако, если бы мы правильно переопределили методы equals() и hashCode(), то в этой Map существовал бы только один объект Person.Давайте рассмотрим правильную имплементацию equals() и hashCode() для нашего класса Person:public class Person {\n    public String name;\n    \n    public Person(String name) {\n        this.name = name;\n    }\n    \n    @Override\n    public boolean equals(Object o) {\n        if (o == this) return true;\n        if (!(o instanceof Person)) {\n            return false;\n        }\n        Person person = (Person) o;\n        return person.name.equals(name);\n    }\n    \n    @Override\n    public int hashCode() {\n        int result = 17;\n        result = 31 * result + name.hashCode();\n        return result;\n    }\n}В этом случае будут верны следующие утверждения:@Test\npublic void givenMap_whenEqualsAndHashCodeNotOverridden_thenMemoryLeak() {\n    Map<Person, Integer> map = new HashMap<>();\n    for(int i=0; i<2; i++) {\n        map.put(new Person(\"jon\"), 1);\n    }\n    Assert.assertTrue(map.size() == 1);\n}После правильного переопределения equals() и hashCode() куча для той же программы выглядит следующим образом:Другой пример — использование инструмента ORM, такого как Hibernate, который применяет методы equals() и hashCode() для анализа объектов и сохраняет их в кэше.Вероятность утечки памяти довольно высока, если эти методы не переопределены, поскольку Hibernate не сможет сравнивать объекты и заполнит свой кэш их копиями.Как предотвратить это?Как правило, на практике,  при определении новых сущностей всегда переопределяйте методы equals() и hashCode().Недостаточно их просто переопределить, это необходимо сделать оптимальным образом. Для получения дополнительной информации ознакомьтесь с нашими учебными пособиями Generate equals() and hashCode() with Eclipse и Guide to hashCode() in Java.3.4. Внутренние классы, которые ссылаются на внешние Это происходит в случае нестатических внутренних классов (анонимных классов). Для инициализации они всегда требуют экземпляр внешнего класса.Каждый нестатический внутренний класс по умолчанию имеет неявную ссылку на содержащий его класс. Если мы используем объект этого внутреннего класса в приложении, то даже после того, как объект нашего содержащего внешнего класса покинет область видимости, он не будет убран в мусор.Рассмотрим класс, который содержит ссылки на множество громоздких объектов и имеет нестатический внутренний класс. При создании объекта только внутреннего класса, модель памяти выглядит следующим образом:Однако, если мы просто объявим внутренний класс как статический, то память уже будет выглядеть так:Как предотвратить это?Если внутреннему классу не нужен доступ к членам внешнего класса, подумайте о том, чтобы превратить его в статический.3.5. Через методы finalize()Использование финализаторов - еще один источник потенциальных проблем с утечкой памяти. Когда метод finalize() класса переопределяется, то объекты этого класса не сразу убирают в мусор. Вместо этого GC ставит их в очередь на финализацию, которая происходит позже.Кроме того, если код метода finalize(), не является оптимальным, а также очередь финализации не успевает за сборщиком мусора Java, то рано или поздно приложение столкнется с ошибкой OutOfMemoryError.Для демонстрации возьмем класс, в котором мы переопределили метод finalize(), и его выполнение занимает немного времени. Когда большое количество объектов данного класса собирается в мусор, то в VisualVM это выглядит так:Однако если мы просто удалим переопределенный метод finalize(), то та же программа даст следующий ответ:Как предотвратить это?Мы всегда должны избегать финализаторовБолее подробно о finalize() читайте в разделе 3 (Как избежать использования финализаторов) нашего руководства по методу finalize в Java.3.6. Интернированные строкиПул строк Java претерпел значительные изменения в Java 7, когда он был перенесен из PermGen в HeapSpace. Однако для приложений, работающих на версии 6 и ниже, мы должны быть более внимательны при работе с большими строками.Если мы считываем огромный объект-массив String и вызываем для него intern(), то он попадает в пул строк, который находится в PermGen (постоянной памяти) и будет оставаться там до тех пор, пока работает наше приложение. Это блокирует память и создает большую ее утечку в нашем приложении.PermGen для этого случая в JVM 1.6 выглядит в VisualVM следующим образом :В отличие от этого, если мы просто читаем строку из файла и не интернируем ее, PermGen выглядит так:Как предотвратить это?Самый простой способ решить эту проблему — обновить Java до последней версии, так как начиная с Java версии 7 пул строк перемещен в HeapSpace.При работе с большими строками увеличьте размер пространства PermGen, чтобы избежать возможных ошибок OutOfMemoryErrors:-XX:MaxPermSize=512m3.7. Использование ThreadLocalsThreadLocal (подробно рассматривается в учебнике \"Введение в ThreadLocal в Java\") - это конструкция, которая дает нам возможность изолировать состояние для конкретного потока, тем самым позволяя достичь его безопасности.В этой конструкции каждый поток хранит неявную ссылку на копию переменной ThreadLocal и будет поддерживать только свою собственную независимую копию, вместо совместного использования ресурса несколькими потоками, в течение всего времени, пока поток активен.Несмотря на все преимущества, переменные ThreadLocal являются спорными, поскольку они могут приводить к утечкам памяти при неправильном использовании. Joshua Bloch однажды прокомментировал применение локальных переменных потоков:Неаккуратное использование пулов потоков в сочетании с небрежным применением локальных переменных потоков может привести к непреднамеренному удержанию объектов, как было отмечено во многих местах. Но возлагать вину на локальные переменные неоправданно.Утечки памяти при использовании ThreadLocalsПредполагается, что ThreadLocals будут утилизироваться, как только удерживающий их поток перестанет существовать. Но проблема возникает, когда ThreadLocals применяются вместе с современными серверами приложений.Современные серверы приложений используют пул потоков для обработки запросов вместо создания новых (например, Executor в Apache Tomcat). Более того, они также используют отдельный загрузчик классов.Поскольку пулы потоков в серверах приложений работают на основе концепции повторного использования, они никогда не утилизируются - их используют повторно для обслуживания другого запроса.Теперь, если какой-либо класс создает переменную ThreadLocal, но явно не удаляет ее, то копия этого объекта останется в воркере Thread даже после остановки веб-приложения, тем самым препятствуя утилизации объекта.Как предотвратить это?Хорошей практикой является очистка ThreadLocals, когда они больше не используются — ThreadLocals предоставляет метод remove(), который удаляет значение текущего потока для этой переменной.Не используйте ThreadLocal.set(null) для очистки значения — он в действительности не очищает, а вместо этого ищет Map, связанную с текущим потоком, и устанавливает пару ключ-значение как текущий поток и null соответственноЕще лучше рассматривать ThreadLocal как ресурс, который должен быть закрыт в блоке finally, чтобы быть уверенным в его закрытии во всех случаях, даже при исключении:try {\n    threadLocal.set(System.nanoTime());\n    //... further processing\n}\nfinally {\n    threadLocal.remove();\n}4. Другие стратегии борьбы с утечками памятиХотя универсального решения при борьбе с утечками памяти не существует, есть некоторые способы, с помощью которых их можно минимизировать.4.1. Включить профилированиеПрофилировщики Java — это инструменты, которые отслеживают и диагностируют утечки памяти в приложении. Они анализируют, что происходит внутри нашего приложения — например, как выделяется память.Используя профилировщики, можно сравнить различные подходы и найти области, где оптимально используются наши ресурсы.В разделе 3 этого руководства мы использовали Java VisualVM. Пожалуйста, ознакомьтесь с нашим руководством по профилировщикам Java, чтобы узнать о различных типах профилировщиков, таких как Mission Control, JProfiler, YourKit, Java VisualVM и Netbeans Profiler.4.2. Подробная сборка мусораПри активации подробной сборки мусора мы отслеживаем детальную трассировку GC. Чтобы включить эту функцию, нам нужно добавить следующее в конфигурацию JVM:-verbose:gcДобавив этот параметр, мы сможем увидеть подробности того, что происходит внутри GC:4.3. Использование ссылочных объектов для предотвращения утечек памятиДля борьбы с утечками памяти можно также воспользоваться ссылочными объектами в Java, которые поставляются с пакетом java.lang.ref. С помощью пакета java.lang.ref вместо прямых ссылок на объекты мы используем специальные ссылки, которые позволяют легко собирать мусор.Очереди ссылок предназначены для того, чтобы мы знали о действиях, выполняемых сборщиком мусора. Для получения дополнительной информации прочитайте Baeldung-учебник \"Мягкие ссылки в Java\", а именно раздел 4.4.4. Предупреждения об утечке памяти в EclipseДля проектов на JDK 1.5 и выше Eclipse выдает предупреждения и ошибки всякий раз, когда сталкивается с очевидными случаями утечки памяти. Поэтому при разработке в Eclipse мы можем регулярно посещать вкладку \"Проблемы\" и быть более бдительными в отношении предупреждений об утечке памяти (если таковые имеются):4.5. БенчмаркингМы можем измерить и проанализировать производительность Java-кода, выполняя эталонные тесты. Таким образом, мы можем сравнить производительность альтернативных подходов к выполнению одной и той же задачи. Это поможет нам выбрать лучший из них и поможет сэкономить память.Для получения более подробной информации о бенчмаркинге, ознакомьтесь с нашим учебным пособием \"Микробенчмаркинг с Java\".4.6. Обзоры кодаНаконец, у нас всегда есть классический, старый добрый способ — сделать простой обзор кода.В некоторых случаях даже этот тривиальный на первый взгляд метод может помочь в устранении некоторых распространенных проблем утечки памяти.5. ЗаключениеГоворя простым языком, мы можем рассматривать утечку памяти как болезнь, которая снижает производительность нашего приложения, блокируя жизненно важные ресурсы памяти. И, как и все другие болезни, если ее не лечить, со временем она может привести к фатальным сбоям приложения.Решить проблему утечки памяти непросто, и ее обнаружение требует высокого мастерства и владения языком Java. При борьбе с утечками памяти не существует универсального решения, поскольку они могут возникать из-за множества разнообразных событий.Однако, если мы будем использовать лучшие практики и регулярно проводить тщательный анализ кода и профилирование, то сможем свести к минимуму риск утечки памяти в нашем приложении.Фрагменты кода, которые использовались для генерации ответов VisualVM, показанных в этом руководстве, доступны на GitHub.Материал подготовлен в рамках курса «Нагрузочное тестирование». Если вам интересно узнать подробнее о формате обучения и программе, познакомиться с преподавателем курса — приглашаем на день открытых дверей онлайн. Регистрация здесь.",
        "user": "\n      MaxRokatansky\n    ",
        "time": "сегодня в 18:28",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": "Блог компании OTUS Java *Тестирование веб-сервисов *",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    },
    {
        "url": "https://habr.com/ru/company/typeable/blog/589257/",
        "title": "Какой вклад внесло функциональное программирование в современные языки?",
        "tag": "haskelllispfunctional programmingfpprogrammingфпфункциональное программированиеяпязыки программированияпарадигмыпарадигмы программирования",
        "body": "Современные языки программирования обладают большим набором разнообразных средств и удобных фишек, что позволяет писать совершенно разный код на одном и том же языке для одной и той же задачи.\nПарадигма программирования — это в первую очередь стиль мышления: то, как программист думает о представлении данных и процессе их обработки. Другими словами, парадигма живёт в голове программиста, а не является свойством языка. Разные языки могут в той или иной степени поддерживать определённую парадигму. Если сейчас зайти на Википедию и начать читать про самые популярные ЯП, мы увидим, что многие из них заявлены как \"мультипарадигменные\": на них можно писать в разных стилях, но какие-то из них использовать будет удобнее.\n\nВ своей недавней статье мы рассказывали о практических применениях Лиспа и упомянули, что он сильно повлиял на развитие других языков программирования, но не стали вдаваться в детали. Пришло время более подробно раскрыть эту тему и разобраться, какой вклад функциональное программирование в целом (не только Лисп!) внесло в развитие других языков. Поскольку мы используем Haskell как основной язык разработки, и наша команда разработчиков состоит из ФП-энузиастов, мы не смогли пройти мимо такой темы.\nВ этом посте рассмотрим несколько механизмов, которые либо зародились в ФП-языках, либо нашли в них наибольшее применение и были ими популяризованы, и в итоге появились в языках, изначально не функциональных.\nФункции первого класса\nОтличительная особенность ФП-стиля в целом — это широкое применение функций, которые становятся одним из самый главных инструментов разработки. Давайте быстро пробежимся по основным определениям, которые описывают различия функций от процедур и других похожих конструкций из не-функциональных языков.\n\nФункция высшего порядка (higher-order function) — это такая функция которая либо принимает другую функцию в виде аргумента либо возвращает функцию в результате. Их ещё называют функционалами. Такое поведение можно реализовать даже в чистом С, используя указатели на функции:\nvoid update_user_balance(int user_id, double (*update_fn)(double)) {\n  // ...\n  user->balance = update_fn(user->balance);\n  // ...\n}\nФункция первого класса (first-class function) — те, которыми можно манипулировать как со всеми другими значениями: передавать как аргументы, возвращать в качестве результата, присваивать переменным и полям структур. \nБезымянная функция (lambda function) — это функция без названия 😉. Кроме отсутствия имени поддержка безымянных функций снимает другие ограничения языка на объявление функций (в некоторых языках, например, в стандарте C99, объявления функции могут встречаться только на верхнем уровне). Поддержка безымянных функций предполагает, что функция может быть объявлена в любом месте где валидны обычные выражения. Безымянные функции чаще всего используются в функционалах, их совместное использование очень удобно и позволяет сильно сократить код. \n// Пример использования безымянной функции для печати содержимого \n// std::vector\nint main() {\n  std::vector<int> v;\n  v.push_back(1);\n  v.push_back(2);\n  std::for_each(v.begin(), v.end(), [] (int x) { \n    std::cout << x << '\\n'; \n  });\n}\nЗамыканиe (closure) — функция может захватить некоторые переменные из контекста в котором она была объявлена, не позволяя сборщику мусора уничтожить данные которые могут быть использованы в этой функции до тех пор пока в приложении существует ссылка на саму функцию. Пример на TypeScript:\nfunction createClosures() {\n  // Переменная видна внутри функций ниже\n  let counter = 0;\n  // Значения полей inc и print являются замыканиями\n  return {\n    inc: () => { counter++; },\n    print: () => console.log('counter value: ' + counter),\n  };\n}\n\nconst c = createClosures();\nc.inc();\nc.inc();\nc.print(); // >>> \"counter value: 2\"\nАбстракция списков\nАбстракция списков (list comprehension) позволяет компактно записывать обработку или генерацию списков из уже существующих. Одним из первых языков, в котором использовался такой синтаксис, была Miranda, из которой его позаимствовал Haskell, а затем подобные конструкции стали появляться в \"менее функциональных\" языках, таких как Python, C#, Ruby.\n\nВ качестве примера рассмотрим код на Python и Haskell, который составляет словосочетания из прилагательных и существительных. Эти два фрагмента очень похожи и отличаются только синтаксическими мелочами, не правда ли?\n# Пример на python\nnouns = [\"waterfall\", \"moon\", \"silence\"]\nadjectives = [\"late\", \"divine\", \"blue\"]\nphrases = [a + \" \" + n for n in nouns for a in adjectives]\n# >>> ['late waterfall', 'divine waterfall', 'blue waterfall', 'late moon', 'divine moon', 'blue moon', 'late silence', 'divine silence', 'blue silence']\n-- То же самое на haskell\nnouns = [\"waterfall\", \"moon\", \"silence\"]\nadjectives = [\"late\", \"divine\", \"blue\"]\nphrases = [a ++ \" \" ++ n | n <- nouns, a <- adjectives]\n-- >>> ['late waterfall', 'divine waterfall', 'blue waterfall', 'late moon', 'divine moon', 'blue moon', 'late silence', 'divine silence', 'blue silence']\nАлгебраические типы данных\nТакже эти типы могут называться ADT-типами, типами-суммами, дискриминированными объединениями, дизъюнктивными объединениями, копроизведениями, а может ещё какими-то умными словами. Вы можете быть знакомы с идеей таких типов под разными названиями, но, если говорить коротко, то это составной тип, который содержит поле-дискриминант (можно назвать тегом) вместе с ассоциированными с ним данными. Ниже пример на Haskell такого типа-объединения, описывающего возможные действия пользователя в гипотетической реализации приложения TodoMVC. Часть действий несут в себе \"полезную нагрузку\" (строку или ID элемента).\ndata UserAction\n  = TextInput String\n  | EnterPressed\n  | EscPressed\n  | CheckTodoItem ItemId Bool\n  | EditTodoItem ItemId String\n  | DeleteTodoItem ItemId\n  | ApplyFilter TodoFilter\nНесмотря на простоту и полезность в моделировании сущностей предметной области, поддержку ADT редко можно встретить в полном объеме в популярных языках и в базах данных. Вот некоторые примеры которые реализуют похожие типы: Enum в Rust, Sealed Classes в Kotlin, std::variant в C++\nСопоставление с образцом\nCопоставление с образцом (Pattern Matching) — это синтаксическая конструкция, которая позволяет получить доступ к данным структуры, состоящей из одного или нескольких вариантов с разным набором полей (те самые ADT, алгебраическая сумма типов, enum, std::variant и т.д., про которые говорится в предыдущем пункте). Pattern Matching напоминает всем знакомый из императивных языков оператор switch-case, но его главное преимущество в том, что доступ к полям вариантов проверяется компилятором статически, используя информацию о типе выражения, в то время как switch-case не позволяет избежать ошибок с некорректным доступом к полям, отсутствующими case'ами или избыточными проверками.\nPattern Matching — ещё один прием который был полуляризован в функциональных языках, где показал свою практичность и в настоящее время в разных формах активно заимствуется и интегрируется в Python, Java, C#, Rust и в других популярных языках.\n-- Пример функции обновления состояния в гипотетическом TodoMVC\n-- написанном в стиле архитектуры Elm. Сопоставление с Образцом\n-- используется для анализа события сгенерированного пользователем.\n-- Событие имеет тип UserAction, который мы описали выше как пример АТД.\nupdateTodoList :: UserAction -> TodoState -> TodoState\nupdateTodoList action oldState = case action of\n  TextInput newTitle -> oldState {title = newTitle}\n  EnterPressed -> appendNewItem oldState\n  EscPressed -> stopEditing oldState\n  CheckTodoItem itemId itemChecked ->\n    updateItemById itemId (#checked .~ itemChecked)\n  EditTodoItem itemId itemTitle ->\n    updateItemById itemId (#title .~ itemTitle)\n  DeleteTodoItem itemId -> deleteTodoItembyId itemId oldState\n  ApplyFilter newFilter -> oldState {filter = newFilter}\nЛенивые вычисления\nВ большинстве ЯП вычисление значения происходит в момент его присвоения переменной, все аргументы вычисляются перед вызовом функции (строгие вычисления). Альтернативный подход — \"ленивый\", когда вычисление значения откладывается до его использования. Ленивые вычисления позволяют работать с бесконечными структурами данных, писать декларативный код с определениями, организованными в порядке, удобном для чтения, а не в порядке их вычисления. Если вы использыете DSL подход, ленивые вычисления помогают легко реализовать такие конструкции как if-then-else (будет вычисляться значение только в нужной ветке).\nИстория термина уходит корнями в лямбда-исчисление, одну из теоретических основ ФП, поэтому неудивительно, что в основном оно используется в ФП-языках. Например, в Haskell всё вычисляется лениво по-умолчанию.\nЭлементы \"ленивости\" можно найти и в других языках, даже в чистом Си логические операторы && и || ленивые: не вычислюят свой второй аргумент, если первый вычислился в 0 или 1 соответственно. В более высокоуровневых языках чаще используется термин \"отложенные вычисления\", которые реализованы с помощью функций-генераторов и ключевого слова yield. Такие генераторы есть, например, в Python или в Java\nКонтинуации\nКонтинуация (продолжение) представляет собой \"остаток вычислений\", т.е. для каждого подвыражения в программе описывается то, что осталось сделать с результатом этого выражения. Выражение получает континуацию в виде дополнительного аргумента, и, когда получен результат, текущая функция вызывает переданную континуацию с вычисленым значением вместо прямого возврата результата. Такой стиль передачи результата называется Continuation-passing style (CPS).\n// Прямой стиль передачи результата\nfunction getFoo(): Foo {..}\n\n// CPS-стиль\nfunction getFooCPS<A>(cont: (foo: Foo) => A): А {..}\nCPS стиль редко встречается непосредственно в исходном коде программ. Одна из главных областей его использования — в компиляторах, как промежуточный формат перед генерацией машиного кода. Перевод кода в CPS позволяет преобразовать рекурсивные вызовы функций к хвостовой рекурсии, которую легко оптимизировать так, чтобы при вычислениях не рос стек.\nКонтинуации сами по себе являются очень мощным инструментом, с помощью которого можно реализовать управляющие конструкции, такие как преждевременный выход из функции, явный вызов хвостовой рекурсии, императивные циклы и другие. Более подробно про использование континуаций можно посмотреть тут на примере языка Scheme.\nFutures and promises\nFutures, Promises, Deferred, а далее просто промисы являются конструкцией, которая содержит вычисление асинхронного значения. Они возникли в фунциональном программировании как инструмент для упрощения паралельных вычислений и выполнения запросов в распределенных системах.\nconst getJson = url => fetch(url).then(resp => resp.json());\n\n// Отправка 2-х последовательных запросов\nconst getUserInfo = getJson('/current-user-id').then(\n  userId => getJson(`/user-info/${userId}`).then(\n    userInfo => console.log(`Hi ${userInfo.firstName}, your id is ${userId}`)\n  )\n);\nПромисы были популяризованы во многом благодаря их адаптации и широкому применению в браузере. Исполнение JavaScript в браузере ограничено только одним потоком выполнения и ожидание ответа HTTP-запросов в блокирующем стиле, как принято в большинстве платформ, приводило бы к зависанию страницы и раздражению пользователей. По этой причине для обработки ответов HTTP-запросов в браузере используются коллбек-функции. В то же время комбинировать такие запросы не очень удобно, и для описания кода, ставшего нечитаемым из-за большого количества коллбеков, возник термин \"Ад обратных вызовов\" (Callback hell). Промисы позволили частично решить проблему с отправкой параллельных запросов и последовательной цепочкой запросов:\n// Отправка 3-х параллельных запросов\nconst fetchInParralel = Promise.all([\n  getJson('/current-user-info'),\n  getJson('/shopping-cart'),\n  getJson('/recently-viewed-items'),\n  ]).then(([userInfo, cart, viewedItems]) => {\n    // Отобразить страницу используя полученную с сервера информацию\n    // ...\n  })\n\nВо многих популярных языках (например C#, Java, JavaScript) промисы стали основным инструментом для асинхронного программирования.\nМонадический интерфейс\nНазвания многих конструкций и приемов программирования в Haskell были заимствованы из теории категорий и других областей математики. Один из таких терминов — \"Монада\" стал предметом многих мемов и шуток про функциональное программирование. В сети существуют множество статей с объяснением, что такое \"Монада\" в функциональных языках и как их использовать.\n\nЕсли же попытаться дать определение в общепонятных терминах, \"Монада\" — это просто интерфейс с двумя методами, которые позволяют связывать вычисления в цепочку как это делается на примере цепочки промисов. Промисы сами тоже являются примером реализации монадическиго интерфейса. В разных языках монадические вычисления могут иметь разные названия, например, bind, chain или pipe.\n// Пример монады для генерации псевдослучайных значений, параметр А —\n// тип генерируемого значения\nclass Random<A> {\n  // Создание Random из произвольного значение\n  static of<A>(value: A): Random<A> {...}\n  // Метод для реализации цепочки вызовов\n  chain<A, B>(this: Random<A>, then: (a: A) => Random<B>): Random<B> {...}\n}\n\ndeclare function randomNumber(min: number, max: number): Random<number>;\ndeclare const randomString: Random<string>;\n\n// Пример использования монадной цепочки\nconst randomUser: Random<User> = randomString.chain(\n  userName => randomNumber(12, 90).chain(\n    userAge => Random.of({ name: userName, age: userAge })\n  )\n);\n\nОдно из применение монад в чистых функциональных языках таких как Haskell — это инкапсуляция побочных эффектов. Т.к. с помощью вызовов обычных функций в таких языках нельзя обратиться к базе данных, прочитать файл или даже напечатать строку в стандартный вывод, для выполнения этих действий используются монады. В то же время эффектами их применение не ограничивается, монадный интерфейс универсален, позволяет писать обобщенный, лаконичный и высокоуровневый код, поэтому монады используются в Haskell повсеместно. За пределами Haskell применение непосредственно монад не так распространено, но их влияние отслеживается в первую очередь в программирование с промисами, а также в конструкции async-await, про которую и поговорим далее. \nAsync\nЕсли вернуться к примерам кода с промисами, можно заметить, что, несмотря на преимущества промисов, цепочка вызовов выглядит немногим лучше использования коллбеков. Синтаксическая конструкция async-await позволяет пойти далее и улучшить код с цепочкой промисов, делая его похожим на код с блокирующими вызовами.\nconst getJson = url => fetch(url).then(resp => resp.json());\n\n// Отправка 2-х последовательных запросов\nasync function getUserInfo() {\n  const userId = await getJson('/current-user-id');\n  const userInfo = await getJson(`/user-info/${userId}`);\n  console.log(`Hi ${userInfo.firstName}, your id is ${userId}`);\n};\n\nВозникновение async-await можно отнести к исследовательским работам по конкуррентному программированию на Haskell и ML которые подтолкнули к появлению async workflows в F# (2007) и затем C# (2011). \nВыводы\nЯзыки программирования не стоят на месте и активно развиваются, обрастая новыми средствами, всё более продвинутыми и удобными. Как видим, в последнее время в популярных языках, таких как Python или С++, стало появляться всё больше фишек, пришедших из функционального программирования. Более молодые языки, например, Scala и Kotlin, изначально создавались с поддержкой функциональных средств.\nФункциональное программирование, оказывается, намного ближе, чем может показаться, даже если разработка ведётся на C++ или Java!\nВ комментариях будем рады услышать про ваш опыт использования этих или каких-то других функциональных фишек в повседневной разработке. \nВам может быть интересно:\n\nСильные стороны функционального программирования\nКак мы выбираем языки программирования в Typeable\nА вы знаете, где сейчас используется Лисп?\n\n7 полезных инструментов на Haskell\n",
        "user": "\n      apache2\n    ",
        "time": "сегодня в 18:27",
        "ratings": " 136.72 \n    Рейтинг\n  ",
        "hub": "Блог компании Typeable Программирование *Функциональное программирование *",
        "suit": "\n      typeable.io\n    ",
        "date": "1  января  2014"
    },
    {
        "url": "https://habr.com/ru/company/fgts/blog/585978/",
        "title": "Пять способов контролировать доступ к приложениям в Kubernetes",
        "tag": "Фактор групkubernetestraefikauthentificationk8scontainerscontainous",
        "body": "Разработчики изучают современные языки программирования и облачные технологии в стремлении повысить продуктивность и ускорить работу. Учитывая гибкость нативных облачных экосистем, возникает все больше вопросов, связанных с безопасностью вообще и управлением доступом в частности. Многие организации внедряют Kubernetes (K8s) как основной инструмент развертывания и администрирования контейнеризированных приложений, но задаются вопросом, как реализовать управление доступом, особенно аутентификацию, в контексте K8s. В этой статье мы ответим на этот вопрос, рассмотрев несколько тем:Распространенные методы аутентификации.Интеграция этих методов с Kubernetes.Распространенные подходы к аутентификацииLDAPМногие организации уже используют сервис каталогов, например Active Directory (AD), для хранения информации о пользователе и организации. Большинство этих систем поддерживают протокол LDAP. С помощью протокола LDAP пользователи в организации могут проходить аутентификацию на основе имеющейся информации, которой управляет ИТ-отдел. Более того, LDAP позволяет приложениям использовать для управления доступом дополнительную информацию, например, о группах и политиках. Это оптимальный вариант для бизнес-приложений и внутренних рабочих нагрузок.OAuth 2.0При использовании сторонних веб-приложений пользователям часто приходится проходить аутентификацию во множество разрозненных систем, для которых центральный сервис идентификации недоступен. Конечно, можно создавать уникальные аккаунты для каждого сервиса, но масштабирование при такой подходе невозможно. Протокол OAuth 2.0 призван решить эту задачу. Обычно он используется в реализациях методов  аутентификации (потоков), например в стандарте  OpenID Connect. Главное преимущество OAuth 2.0 — возможность утверждать уровень доступа для приложений. Это позволяет использовать существующий идентификатор (например, аккаунт Google), чтобы проходить аутентификацию и контролировать доступ к информации в сторонних приложениях.JSON Web Token (JWT)JSON Web Token все чаще используются для аутентификации, особенно для API. Эти токены состоят из объектов JSON в кодировке Base64URL: заголовок, полезная нагрузка и подпись. Криптографическая подпись рассчитывается с помощью общего секрета и пары открытого и закрытого ключей. С ее помощью можно проверить источник объекта. Например, при наличии общего секрета подпись JWT можно вычислить с помощью алгоритма HS256 (HMAC с SHA-256):signature = HS256(\n    Base64URLEncoding(header) + '.' + Base64URLEncoding(payload),\n    secret\n)Итоговый JWT состоит из трех компонентов:Base64URLEncoding(header) + '.' + Base64URLEncoding(payload) + '.' + signatureЭти токены не обеспечивают безопасность данных, поскольку не зашифрованы, но этот простой подход можно использовать в API для идентификации вызывающих объектов. JWT — это хороший вариант для интеграции провайдеров аутентификации с пользовательскими API и взаимодействием между сервисами. Часто он реализуется с помощью потоков OAuth 2.0.HMACВ примере выше используется HMAC с алгоритмом SHA-256 для вычисления подписи заголовка токена и полезной нагрузки. HMAC можно использовать для полезной нагрузки с самыми разными спецификациями. В JWT, например, это объект JSON, но тот же механизм можно использовать и с другими данными, чтобы выполнить аутентификацию подписанного сообщения.Двусторонний TLS или Mutual TLS (mTLS)Аутентификация TLS сегодня используется почти во всех веб-приложениях. Этот механизм использует сертификаты для аутентификации веб-сайта или веб-сервиса, чтобы клиент был уверен в надежности сервера. Двусторонний TLS обеспечивает аутентификацию обеих сторон: клиент проверяет сервер, а сервер проверяет клиента, чтобы применить политики управления доступом и авторизации. Двусторонний TLS обычно используется для взаимодействия между службами и компаниями, где существует ограниченное и известное число клиентов, у которых есть доступ к общим конечным точкам.Интеграция приложений с аутентификацией и KubernetesМы рассмотрели распространенные подходы к аутентификации, а теперь поговорим о том, как использовать их, если приложения развернуты в Kubernetes. K8s дает гибкие возможности для реализации решений, отвечающих индивидуальным потребностям организаций. Рассмотрим несколько методов, которые подойдут почти для любого сценария.Ingress-контроллерIngress-контроллеры — это самый популярный механизм для связи пользователей с приложениями в Kubernetes. При выборе контроллера нужно учесть много факторов, поскольку у каждого контроллера свои технологии и методы реализации. Среди критериев выбора, например, поддержка подходов к аутентификации, о которых мы говорили.Traefik  содержит промежуточный слой ForwardAuth, с помощью которого можно делегировать аутентификацию внешнему сервису. Если использовать эту возможности в стратегии K8s Ingress, все сервисы для входящего потока трафика получают преимущества управления аутентификацией без лишних сложностей на уровне отдельного сервиса. TraefikEE упрощает управление поставщиками аутентификации, интегрируя поддержку LDAP, OAuth 2.0, JWT и HMAC в виде единого решения.Серверы аутентификацииВозможности аутентификации с помощью Ingress-контроллера можно дополнить выделенными серверами аутентификации. Authelia — это пример сервера аутентификации и авторизации с открытым кодом, который работает с K8s и легко интегрируется с технологиями Ingress, например Traefik. Помимо описанных ранее механизмов, эти сервисы предоставляют расширенные возможности аутентификации, например двухфакторную аутентификацию (2FA)  и единый вход (SSO).ЗаключениеНаладить управление доступом к приложениям с помощью аутентификации важно в любой инфраструктуре, а в Kubernetes особенно. Тщательно выбирая архитектурные шаблоны и технологии, можно реализовать оптимальные подходы к аутентификации без лишних усилий.Traefik — это отличный вариант для Kubernetes. Traefik с открытым исходным кодом и поддержкой большого и активного сообщества доступен бесплатно и совместим с большинством новейших улучшений в Kubernetes Ingress. Краткое описание решения на русском языкеЗапросить Демо или задать вопрос вы можете через нашу форму.P.S. 18 ноября компания Traefik проводит вебинар, посвященный сложностям, связанным с управлением приложениями в разных типах контейнеров, а также рассказывают, как можно упростить эту задачу.Регистрация доступна по ссылке.",
        "user": "\n      fgts_ru\n    ",
        "time": "сегодня в 17:34",
        "ratings": " 47.62 \n    Рейтинг\n  ",
        "hub": "Блог компании Фактор груп Информационная безопасность *Open source *DevOps *Kubernetes *",
        "suit": "\n      www.fgts.ru\n    ",
        "date": "29  июня  2009"
    },
    {
        "url": "https://habr.com/ru/company/otus/blog/589297/",
        "title": "«Выстрелить и забыть» в Cats Effect",
        "tag": "scalaпаттернCats Effectвыстрелить и забытьпарсер-комбинаторы",
        "body": "Последнее время меня часто спрашивают о паттерне \"fire-and-forget\": как его применить в Cats Effect и какие потенциальные проблемы могут возникнуть. Поэтому я решил написать небольшой пост. Надеюсь, вам понравится!Подробнее о Cats Effect и конкурентности читайте в моей книге Essential Effects.Что значит «выстрелить и забыть»?Чтобы понять принцип «выстрелить и забыть» (fire-and-forget), давайте сначала посмотрим на простой синхронный вызов метода или функции. При синхронной операции мы вызываем функцию и ожидаем, когда она вернет значение. Вычисления продолжаем только после получения результата.def doSomething(i: Int): String = {\n  println(s\"[${Thread.currentThread.getName}] doSomething($i)\")\n  s\"$i is a very nice number\"\n}\n\ndef doSomethingElse(): Int = {\n  println(s\"[${Thread.currentThread.getName}] doSomethingElse()\")\n  12\n}\n\nval doBoth = {\n  val result = doSomething(12)\n  println(s\"[${Thread.currentThread.getName}] doSomething(12) produced: $result\")\n  doSomethingElse\n}Обратите внимание: все println выполняются в одном потоке.Теперь давайте разбираться с «выстрелить и забыть». Что значит «выстрелить»? Выстрелить — это запустить вычисление без ожидания результата, т.е. асинхронно. (Название «выстрелить и забыть» пришло от военных — запуск ракеты.)Какая должна быть сигнатура типа для такой асинхронной операции? Она требует входные данные, и сразу же возвращает управление в точку вызова. Но здесь мы не получаем «фактический» результат вычисления — только сообщаем: «Вычисления запущены, вот вам некое значение, которое позволит получить конечный результат, когда он будет вычислен». Смоделируем это в виде трейта с методом fire, который возвращает «конечный результат» EventualResult (в виде функции):trait Asynchronous {\n  type EventualResult[A] = () => A\n\n  def fire[A](run: => A): EventualResult[A]\n}Этот интерфейс можно реализовать, используя тип Future:object Asynchronous {\n  import scala.concurrent._\n  import scala.concurrent.duration._\n\n  val global = \n    new Asynchronous {\n      implicit val ec = scala.concurrent.ExecutionContext.global\n\n      def fire[A](run: => A): EventualResult[A] = {\n        val res = Promise[A].completeWith(Future(run)) // start the computation\n        () => Await.result(res.future, Duration.Inf)   // wait on demand\n      }\n    }\n}Затем преобразуем наш синхронный код для использования интерфейса Asynchronous:val doBothAsync = {\n  val await = Asynchronous.global.fire(doSomething(12))\n  val result = await()\n  println(s\"[${Thread.currentThread.getName}] doSomething(12) produced: $result\")\n  doSomethingElse\n}Обратите внимание, что println в doSomething и doSomethingElse выполняются в разных потоках.Теперь  нам нужно \"забыть\" (forget). То есть после того, как мы начали вычисление (fire), мы ничего не делаем для обработки конечного результата.  Мы \"забываем\" (forget) его:trait Asynchronous {\n type EventualResult[A] = () => A\n def fire[A](run: => A): EventualResult[A]\n def fireAndForget[A](run: => A): Unit =\n   fire(run) // игнорируем результат\n}object Asynchronous {\n import scala.concurrent.\n import scala.concurrent.duration.\n val global =\n   new Asynchronous {\n     implicit val ec = scala.concurrent.ExecutionContext.global\n     def fire[A](run: => A): EventualResult[A] = {\n       val res = PromiseA.completeWith(Future(run)) // старт вычисления\n       () => Await.result(res.future, Duration.Inf) // ожидание\n     }\n   }\n}Метод fireAndForget вызывает метод fire, запускающий асинхронное вычисление, но игнорирующий возвращаемый EventualResult. То есть мы вообще не ждем никакого результата.Давайте обновим наш пример:val doBothFireAndForget = {\n  Asynchronous.global.fireAndForget(doSomething(12))\n  doSomethingElse\n}Обратите внимание, что здесь такое же асинхронное выполнение, как и в примере с \"только fire\", но мы не выводим никаких промежуточных результатов, потому что решили их не ждать.Используем Cats EffectДля реализации паттерна \"выстрелить и забыть\" в Cats Effect необходимо выполнить следующее:Определить синхронное вычисление как эффект.Запустить выполнение эффекта асинхронно (\"выстрелить\").Игнорировать возвращаемое значение (\"забыть\").Давайте сделаем это:import cats.effect._\nimport cats.implicits._\n\nclass Something extends IOApp {\n\n  def run(args: List[String]): IO[ExitCode] =\n    doBoth.as(ExitCode.Success)\n\n  def doSomething(i: Int): IO[String] = // <1>\n    for {\n      _ <- IO(println(s\"[${Thread.currentThread.getName}] doSomething($i)\"))\n    } yield s\"$i is a very nice number\"\n\n  def doSomethingElse(): IO[Int] =\n    for {\n      _ <- IO(println(s\"[${Thread.currentThread.getName}] doSomethingElse()\"))\n    } yield 12\n\n  val doBoth: IO[Int] =\n    for {\n      _ <- doSomething(12).start // <2> <3>\n      i <- doSomethingElse\n    } yield i\n}\n\nnew Something().main(Array.empty)Мы изменили тип возвращаемого значения со String на IO[String], чтобы определить (синхронный) эффект.И запустили эффект IO асинхронно с помощью start. Он возвращает тип IO[Fiber[IO, String]] (Cats Effect 3 — IO[FiberIO[String]]), который позволяет управлять асинхронным выполнением и получением конечного результата.Мы также игнорируем Fiber, созданный start, \"анонимизируя\" его имя как .Безопасно ли «забыть»?Когда мы \"стреляли и забывали\" с помощью Cats Effect, мы намеренно не отслеживали асинхронную операцию. Но безопасно ли это?Чтобы ответить на этот вопрос, давайте вспомним, какие операции есть у файбера: можно дождаться его завершения (с помощью join) или отменить (с помощью cancel). Поскольку мы ожидаем асинхронного выполнения, то join нам не нужен, и мы поговорим о cancel. Потребуется ли нам когда-нибудь отменять вычисление, которое мы \"запустили и забыли\"? Несомненно, нам это пригодится! Задача может быть очень долгоживущей. Например, цикл обработки событий. Здесь вы можете подумать: \"О, этот эффект выполняется вечно, его не нужно отменять\", поэтому просто \"выстрелим и забудем\". Но отмена может потребоваться! Асинхронный эффект существует в некотором контексте и важно убедиться, что он отменяется при отмене его \"родительского\" контекста:val businessLogic =\n  for {\n    _ <- longLivedEffect.start\n    _ <- doSomeOtherStuff\n  } yield ()В этом примере после завершения businessLogic, возможно, стоит отменить и longLivedEffect, но подход \"выстрелить и забыть\" исключает такую возможность. Нам нужно как-то связать время жизни этого долгоживущего эффекта с окружающей бизнес-логикой. Один из вариантов — использовать guarantee:val businessLogic =\n  for {\n    fiber <- longLivedEffect.start\n    _ <- doSomeOtherStuff.guarantee(fiber.cancel) // <1>\n  } yield ()Отмена файбера при успешном завершении дополнительной бизнес-логики, а также при ошибке или отмене.Но использование guarantee не масштабируется на несколько эффектов. Например, мы хотим, чтобы файбер отменялся, если отменяется какой-либо эффект внутри businessLogic. В такой ситуации мы можем использовать Resource. Resource — структура, которая управляет аллокацией и деаллокацией ресурсов:val longLived: Resource[IO, IO[Unit]] =\n  longLivedEffect.background // <1>\n\nval businessLogic =\n  longLived.use { join => // <2>\n    doSomeOtherStuff\n  }Вместо start мы используем метод background, возвращающий Resource, который управляет асинхронным файбером. Управляемое состояние само по себе является действием IO[Unit], которое позволяет вам при необходимости выполнить join к нижележащему файберу.Используя Resource через use, мы получаем доступ к управляемому состоянию, где можем выполнять другие эффекты. Он определяет статическую область, в которой доступно состояние, при этом за пределами этой области гарантируется корректное получение и освобождение состояния. В случае ресурса, возвращаемого background, эффект получения состояния — это \"запуск файбера\", а эффект освобождения — \"отмена файбера\".РезюмеПаттерн «выстрелить и забыть» (fire-and-forget) состоит из трех частей:Определение синхронного эффекта.Асинхронный запуск эффекта.Контроль асинхронного выполнения эффекта: ожидание результата или отмена.Вы можете смело запускать и забывать короткоживущие асинхронные эффекты! Но помните, что выполнение всегда происходит в каком-то контексте, и у вас есть такие методы, как background, для контроля асинхронного жизненного цикла.Материал подготовлен в рамках курса «Scala-разработчик». Всех желающих приглашаем на demo-занятие «Scala и парсер-комбинаторы». На занятии мы: — Познакомимся с парсер-комбинаторами на Scala;— Будем парсить описание REST API написанное с помощью markdown.>> РЕГИСТРАЦИЯ",
        "user": "\n      MaxRokatansky\n    ",
        "time": "сегодня в 17:29",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": "Блог компании OTUS Программирование *Scala *",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    },
    {
        "url": "https://habr.com/ru/post/589295/",
        "title": "Разработка мобильных игр на Unity. URP, 2D Animation и другие новомодные вещи на примере игры",
        "tag": "unitycsharpgamedevexamplelogicmobile",
        "body": "Всем привет! Это снова Илья и сегодня мы поговорим о технической реализации мобильной игры в современных реалиях. Статья не претендует на уникальность, однако в ней вы можете найти для себя что-то полезное. А чтобы рассмотреть разработку на реальном проекте - мы возьмем реализацию нашей игры, которая на днях выходит в Soft-Launch.Дисклеймер! Код в этой статье не проходил рефакторинг и носит лишь ознакомительный характер, чтобы поделиться идеями. И вообще, в целом, это smellscode.Итак, запасаемся кофе, открываем Unity и погнали!Базовая настройка проекта. URP и все-все-все.Начнем с того, что мы работаем с URP (Universal Render Pipeline). Почему так? Потому что он проще в настройке и обладает более гибким контролем, чем стандартный рендер. Ну и добиться хорошей производительности на тапках исходя из этого - намного проще.Стоит указать, что ниже пойдет речь о 2D игре. Для 3D игр подходы будут несколько отличаться, как и настройки.Мы реализовали два уровня графики. Low Level - для деревянных смартфонов и High Level - для флагманов. Уровни графики подключаются при помощи Project Settings.В нашем проекте стоят следующие настройки (для Quality уровней):Настройки графики для пресета Low в Project Settings:На что здесь следует обратить внимание:Texture Quality - качество текстур. Для High - мы берем полный размер текстур, для Low - Четверть. Можно еще внести Middle пресет с дополнительным уровнем.Resolution Scaling везде стоит 1 - мы берем это значение из URP Asset.Все что связано с реалтаймом - отключаем.Теперь перейдем к настройкам самих URP Asset. На что следует обратить внимание:Для разных уровней качества можно установить Render Scale - тем самым снижая разрешение для отрисовки. Также незабываем про Dynamic / Static батчинг.Adaptive PerformanceОтличная штука для автоматической подгонки производительности мобильных игр (в частности для Samsung-устройств):Другие полезные настройки:Отключите 3D освещение, лайтмапы, тени и все что с этим связано.Используйте для сборки IL2CPP - ускорьте работу вашего кода.Используйте Color Space - Linear.По-возможности подключите multithreaded rendering.Игровой фреймворкЕдем дальше. URP и другие настройки проекта сделали. Теперь настало время поговорить о нашем ядре проекта. Что оно включает в себя?Само ядро фреймворка включает в себя:Игровые менеджеры для управления состояниями игры, аудио, переводов, работы с сетью, аналитикой, рекламными интеграциями и прочим.Базовые классы для интерфейсов (компоненты, базовые классы View).Классы для работы с контентом, сетью, шифрованием и др.Базовые классы для работы с логикой игры.Базовые классы для персонажей и пр.Утилитарные классы (Coroutine Provider, Unix Timestamp, Timed Event и пр.)Зачем нужны менеджеры?Они нужны нам для того, чтобы из контроллеров управлять состояниями и глобальными функциями (к примеру, аналитикой).Хотя мы и используем внедрение зависимостей, менеджеры состояний реализованы в качестве синглтонов (атата по рукам, но нам норм) и могут быть (и по их назначению должны быть) инициализированы единожды. А дальше мы просто можем использовать их:AnalyticsManager.Instance().SendEvent(\"moreGamesRequested\");А уже сам менеджер распределяет, в какие системы аналитики, как и зачем мы отправляем эвент.Базовые классы.Здесь все просто. Они включают в себя базовую логику для наследования. К примеру, класс BaseView и его интерфейс:namespace GameFramework.UI.Base\n{\n    using System;\n    \n    public interface IBaseView\n    {\n        public void ShowView(ViewAnimationOptions animationOptions = null, Action onComplete = null);\n\n        public void HideView(ViewAnimationOptions animationOptions = null, Action onComplete = null);\n\n        public void UpdateView();\n    }\n}namespace GameFramework.UI.Base\n{\n    using System;\n    using UnityEngine;\n    using UnityEngine.Events;\n    using DG.Tweening;\n    \n    internal class BaseView : MonoBehaviour, IBaseView\n    {\n        // Private Params\n        [Header(\"View Container\")]\n        [SerializeField] private Canvas viewCanvas;\n        [SerializeField] private CanvasGroup viewTransform;\n\n        private void Awake()\n        {\n            // View Canvas Detecting\n            if (viewCanvas == null)\n            {\n                viewCanvas = GetComponent<Canvas>();\n                if (viewCanvas == null) throw new Exception(\"Failed to initialize view. View canvas is not defined.\");\n            }\n            \n            // View Transform Detecting\n            if (viewTransform == null)\n            {\n                viewTransform = GetComponent<CanvasGroup>();\n                if (viewTransform == null) throw new Exception(\"Failed to initialize view. View transform is not defined.\");\n            }\n            \n            // On Before Initialized\n            OnViewInitialized();\n        }\n\n        public virtual void OnViewInitialized() {\n        }\n\n        private void OnDestroy()\n        {\n            viewTransform?.DOKill();\n            OnViewDestroyed();\n        }\n\n        public virtual void OnViewDestroyed() {\n        }\n\n        public virtual void UpdateView() {\n        }\n\n        public bool IsViewShown()\n        {\n            return viewCanvas.enabled;\n        }\n        \n        public void ShowView(ViewAnimationOptions animationOptions = null, Action onComplete = null)\n        {\n            viewCanvas.enabled = true;\n            if (animationOptions == null) animationOptions = new ViewAnimationOptions();\n            \n            if (animationOptions.isAnimated)\n            {\n                viewTransform.DOFade(1f, animationOptions.animationDuration).From(0f)\n                    .SetDelay(animationOptions.animationDelay).OnComplete(() =>\n                        {\n                            if (onComplete != null) onComplete();\n                            OnViewShown();\n                        });\n            }\n            else\n            {\n                if (onComplete != null) onComplete();\n                OnViewShown();\n            }\n        }\n\n        public void HideView(ViewAnimationOptions animationOptions = null, Action onComplete = null)\n        {\n            if (animationOptions == null) animationOptions = new ViewAnimationOptions();\n            \n            if (animationOptions.isAnimated)\n            {\n                viewTransform.DOFade(0f, animationOptions.animationDuration).From(1f)\n                    .SetDelay(animationOptions.animationDelay).OnComplete(() =>\n                    {\n                        viewCanvas.enabled = false;\n                        if (onComplete != null) onComplete();\n                        OnViewHidden();\n                    });\n            }\n            else\n            {\n                viewCanvas.enabled = false;\n                if (onComplete != null) onComplete();\n                OnViewHidden();\n            }\n        }\n\n        public virtual void OnViewShown(){\n        }\n\n        public virtual void OnViewHidden(){\n        }\n    }\n}А дальше мы можем использовать его, к примеру таким образом:namespace Game.UI.InGame\n{\n    using System;\n    using System.Collections;\n    using System.Collections.Generic;\n    using UnityEngine;\n    using UnityEngine.Events;\n    using UnityEngine.UI;\n    using GameFramework.UI.Base;\n    using GameFramework.Components;\n    using GameFramework.UI.Components;\n    using GameFramework.Managers;\n    using GameFramework.Models;\n    using GameFramework.Models.Ads;\n    using Game.Models;\n    using GameFramework.Utils;\n    \n    internal class ToDoListView : BaseView\n    {\n        // View Context\n        public struct Context\n        {\n            public Action onToDoListClosed;\n        }\n        private Context _ctx;\n        \n        // View Params\n        [Header(\"View References\")] \n        [SerializeField] private Button closeButton;\n        [SerializeField] private AudioClip clickButtonSFX;\n        \n        // Private Params\n        private AudioSource _windowAudioSource;\n\n        public ToDoListView SetContext(Context ctx)\n        {\n            _ctx = ctx;\n            \n            // Initialize Audio SOurce\n            if (_windowAudioSource == null)\n            {\n                _windowAudioSource = transform.gameObject.AddComponent<AudioSource>();\n                transform.gameObject.AddComponent<AudioSettingsApplier>().currentAudioType = GameFramework.Models.AudioType.Sounds;\n            }\n            \n            // Add Handlers\n            closeButton.onClick.AddListener(() =>\n            {\n                _ctx.onToDoListClosed.Invoke();\n                PlayClickSoundSFX();\n            });\n            \n            return this;\n        }\n        \n        public override void OnViewDestroyed()\n        {\n            closeButton.onClick.RemoveAllListeners();\n        }\n        \n        public override void UpdateView()\n        {\n        }\n        \n        private void PlayClickSoundSFX()\n        {\n            if (_windowAudioSource != null && clickButtonSFX!=null)\n            {\n                _windowAudioSource.playOnAwake = false;\n                _windowAudioSource.clip = clickButtonSFX;\n                _windowAudioSource.loop = false;\n                _windowAudioSource.Play();\n            }\n        }\n    }\n}Классы для работы с контентом, сетью, шифрованиемНу здесь все просто и очевидно. Вообще, у нас реализовано несколько классов:1) Классы шифрования (Base64, MD5, AES и пр.)2) FileReader - считывающий, записывающий файл, с учетом кодировки, шифрования и других параметров. Также он умеет сразу сериализовать / десериализовать объект в нужном формате и с нужным шифрованием.3) Network-классы, которые позволяют удобно работать с HTTP-запросами, работать с бандлами / адрессаблс и др.Классы для шифрования нужны, чтобы работать с сохранениями и передачей данных на сервер в безопасном формате (относительно безопасном, но от школьников уже спасет).Утилитарные классыЗдесь у нас хранятся полезные штуки, вроде Unix Time конвертера, а также костыли (вроде Coroutine Provider-а).Unix Time Converter:namespace GameFramework.Utils\n{\n    using UnityEngine;\n    using System.Collections;\n    using System;\n \n    public static class UnixTime  {\n        public static int Current()\n        {\n            DateTime epochStart = new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc);\n            int currentEpochTime = (int)(DateTime.UtcNow - epochStart).TotalSeconds;\n            return currentEpochTime;\n        }\n \n        public static int SecondsElapsed(int t1)\n        {\n            int difference = Current() - t1;\n            return Mathf.Abs(difference);\n        }\n\n        public static int SecondsElapsed(int t1, int t2)\n        {\n            int difference = t1 - t2;\n            return Mathf.Abs(difference);\n        }\n    }\n}Костыль Coroutine-Provider:namespace GameFramework.Utils\n{\n    using System.Collections;\n    using System.Collections.Generic;\n    using UnityEngine;\n    \n    public class CoroutineProvider : MonoBehaviour\n    {\n        static CoroutineProvider _singleton;\n        static Dictionary<string,IEnumerator> _routines = new Dictionary<string,IEnumerator>(100);\n\n        [RuntimeInitializeOnLoadMethod( RuntimeInitializeLoadType.BeforeSceneLoad )]\n        static void InitializeType ()\n        {\n            _singleton = new GameObject($\"#{nameof(CoroutineProvider)}\").AddComponent<CoroutineProvider>();\n            DontDestroyOnLoad( _singleton );\n        }\n\n        public static Coroutine Start ( IEnumerator routine ) => _singleton.StartCoroutine( routine );\n        public static Coroutine Start ( IEnumerator routine , string id )\n        {\n            var coroutine = _singleton.StartCoroutine( routine );\n            if( !_routines.ContainsKey(id) ) _routines.Add( id , routine );\n            else\n            {\n                _singleton.StopCoroutine( _routines[id] );\n                _routines[id] = routine;\n            }\n            return coroutine;\n        }\n        public static void Stop ( IEnumerator routine ) => _singleton.StopCoroutine( routine );\n        public static void Stop ( string id )\n        {\n            if( _routines.TryGetValue(id,out var routine) )\n            {\n                _singleton.StopCoroutine( routine );\n                _routines.Remove( id );\n            }\n            else Debug.LogWarning($\"coroutine '{id}' not found\");\n        }\n        public static void StopAll () => _singleton.StopAllCoroutines();\n    }\n}Логика сценНаша игра - это по своей сути интерактивная история с различными мини-играми (поиск предметов, простенькие бои, крафтинг, найди пару, а также большое количество головоломок).Каждая сцена - содержит в себе основной Installer, который помимо различных View, подключает логические блоки - своеобразные куски механик:Эти куски механик последовательно выполняются, отдавая события OnInitialize, OnProgress, OnComplete. Когда последний блок сыграет свой OnComplete - он завершит работу сцены (закончит уровень).Зачем это сделано?Мы можем собирать каждую сцену из отдельных механик, как конструктор. Это может быть диалог -> поиск предметов -> катсцена -> поиск предметов -> диалог, или любой другой порядок.Мы можем сохранять прогресс внутри сцены, привязываясь к определенному блоку.Блоки механик удобнее изменять, нежели огромный инсталлер с кучей разных контроллеров.Работа с контентомПри работе с контентом, мы стараемся делать упор на оптимизацию. В игре содержится много UI, скелетные 2D анимации, липсинк и прочее. Вообще, контента достаточно много, не смотря на простоту игры.Анимации в игреСамый удобный для нас вариант - оказался из коробки. Мы используем систему для работы с костной анимацией от самой Unity:Да, можно взять Spine, но у нас нет настолько большого количества анимаций, поэтому вариант от Unity - весьма оптимален.Упаковка и сжатиеВсе, что можно и нужно запихнуть в атласы - мы запихиваем в атласы и сжимаем. Это могут быть элементы UI, иконки и многое другое. Для упаковки атласов - используем стандартный Unity пакер из Package Manager (V1):ЛокализацияВся локализация базируется на JSON. Мы планируем отказаться от этого в ближайшее время, но пока что на время Soft-Launch этого хватает:Работа с UIПри работе с UI мы разбиваем каждый View под отдельный Canvas. 99% всех анимаций работает на проверенном временем DOTween и отлично оптимизирован.View инициализируются и обновляются по запросу через эвенты, которые внедряются в Level Installer, либо в отдельных блоках логики.Что мы используем еще?Salsa - для липсинка;2D Lighting - для освещения. В большинстве сцен используется освещение по маске спрайта;DOTween - для анимаций;ИтогоРабота с механиками получается достаточно гибкой за счет блоков логики. Мы изначально думали взять связку Zenject + UniRX, но решили отказаться от нагромождения большой системы. Да, мы сделали проще, но нам и не нужно всех возможностей этих огромных библиотек.Полезные ссылки:http://dotween.demigiant.com/https://assetstore.unity.com/packages/tools/animation/salsa-lipsync-suite-148442https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@7.1/manual/Lights-2D-intro.htmlhttps://docs.unity3d.com/Packages/com.unity.2d.animation@1.0/manual/index.html",
        "user": "\n      poznohub\n    ",
        "time": "сегодня в 17:27",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": ".NET *Unity *",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    },
    {
        "url": "https://habr.com/ru/company/piter/blog/587208/",
        "title": "Книга «Вселенная видеоигр. Документальный блокбастер от разработчика The Division, Far Cry 3 и Assassin's...»",
        "tag": "видеоигры",
        "body": " Привет, Хаброжители! Дэвид Полфельдт, управляющий директор Ubisoft Massive, ключевого подразделения одной из крупнейших и самых влиятельных компаний в игровом бизнесе, участвовал в создании Assassin's Creed, Far Cry и Tom Clancy's The Division. Во «Вселенной видеоигр» он вспоминает свой творческий путь от крохотного темного подвала в Стокгольме до офиса в Голливуде, попутно размышляя о том, как развивалась и менялась индустрия, как и когда игры стали искусством, об их постоянно расширяющемся художественном и сюжетном потенциале.\n\nКак я утонул на Корсике\r\nБольшая игра трипл-эй, такая как Assassin’s Creed, Far Cry или The Division, разрабатывается в пестром калейдоскопе смелых предположений и неудачных прототипов до тех пор, пока истинно по-дарвиновски не обретет наилучшую из возможных форму. Сама игра похожа на еще не рожденного ребенка, пребывающего в мире грез и жаждущего начать жить. Во время процессов зарождения и роста команда и издатели определяют ключевые моменты процесса, называемые вехами, каждый из которых имеет свое собственное значение и цель.\n\r\n«Зеленый свет», например, означает, что издатель впечатлен вашей презентацией и готов потратить на вашу идею деньги (ура!). «Первая играбельная версия» — это еще одна веха, название которой говорит само за себя. (Отпразднуйте это! Скоро будет труднее.)\n\r\nУ следующих вех криптоназвания, которые сами по себе ни о чем не говорят, например альфа и бета. Для игр трипл-эй это уже исключительно сложные уровни, для достижения которых требуется несколько сотен талантливых разработчиков. Если говорить простым языком, альфа — это тот момент, когда все, что в конечном итоге ожидается в финальной игре, в том или ином виде включено или должно быть включено в программное обеспечение. На стадии альфа-игры отдельные функции могут быть в ужасной форме, но они должны быть там. Это все равно что прийти на вечеринку без одежды, небритым и в плохой физической форме, но при этом никто из присутствующих и глазом не моргнет. Все, что от вас требуется, — это появиться таким, какой вы есть. Прихорашиваться не обязательно.\n\r\nКак только альфа одобрена, команда бросает все силы на то, чтобы превратить все незаконченные системы во что-то функциональное, и наполняет игру содержанием и функциями, которые предполагались с самого начала (например, возможность охотиться на всех животных, а не только на бородавочника). Когда ценой нечеловеческих усилий это наконец удается, разработчики достигают стадии бета, а это означает, что никакая функция или контент уже не могут быть добавлены. Следующие недели и месяцы уходят на исправление ошибок, на чисто математическую доработку кода, а также на выборочную полировку фирменных черт игры до совершенства.\n\r\nИли, ну… мы утверждаем, что так оно и есть. Это теоретические альфа и бета. Вполне предсказуемо, что в реальной жизни никто не обладает способностью или дисциплиной следовать всем этим здоровым принципам, а это значит, что ворота ада открываются уже на альфе, а где-то между бета-версией и (теоретически) финальной вехой, называемой «золотой кандидат в мастера», все становится только хуже. Если «кранч» волшебным образом не случился раньше, то он гарантированно случится после альфы. (Кранч — этим словечком разработчики игр называют периоды чрезмерной сверхурочной работы. Кранч подразумевает, что на вас физически и эмоционально давят моральный гнет, недостаток сна, плохая еда, безумно удлиненный рабочий день и ваш полный отказ от какой бы то ни было личной жизни в пользу Великого Запуска Игры.)\n\r\nВ конце 2014 года, после успешного выступления на E3 в 2013 году, The Division с задержкой приближалась к стадии альфа. И вновь мы испытывали, казалось бы, бесконечное терпение наших дорогих друзей в штаб-квартире Ubisoft. Возможно, игра и была в приемлемой форме. Но даже если так, мы не могли этого утверждать наверняка. Как всегда бывает в играх с открытым миром, все вставало на свои места чуть ли не в последний момент. Линейные и сессионные игры намного проще контролировать во время разработки, потому что границы фиксированы, а количество доступных игроку вариантов ограничено. Но в играх с открытым миром одним из основных принципов дизайна является максимизация вариантов выбора для игрока. Это приводит к необходимости сосредоточиться на системах и функциях, которые могут быть взаимосвязаны и взаимодействовать различными способами в любой точке огромного мира.\n\r\nРазработчики игр открытого мира стараются дать игроку как можно больше свободы. Если все идет как надо, то получается фантастическая игра с щедрым набором возможностей. Проблема в том, что в течение длительных периодов разработки множество базовых систем не связаны друг с другом и выглядят разобщенными. Играть в такую игру до того, как она станет единым целым, — это все равно что съесть по отдельности все ингредиенты, которые составляют обед из трех блюд, а затем попытаться угадать, что же собирался приготовить шеф-повар.\n\r\nНикто из нас не мог судить о скорости нашей работы по имеющимся фрагментам, и казалось, что мы в тупике. Ради этого я поставил на карту судьбу студии? Такие мысли крутились в моей голове, когда однажды я, охваченный страхом и печалью, пытался играть в последнюю версию игры. Версия была ужасной. Играть в нее было невозможно. То же самое думали и в Париже, о чем мне не преминули сообщить. У нас была плохая репутация в штаб-квартире, и многие никак не могли забыть, что я вел себя как примадонна.\n\r\nПоддавшись панике, я приостановил работу над проектом «Орегон» — к большому разочарованию команды — и бросил все силы студии на работу над предстоящей вехой The Division. Лично мне это казалось логичным, ведь на карту была поставлена наша репутация. От успеха Snowdrop и амбициозной игры Clancy зависело многое. Но это создало некоторые трения на местах. Те, кто и раньше работал над The Division, привыкли сами принимать решения, а теперь им приходилось приспосабливаться к работе с целой группой самоуверенных коллег, которым отдали кусок империи. Но процесс был болезненным и для тех, кто оставил работу над «Орегоном»: ведь они любили свой проект и связанную с ним свободу, которую заработали тяжким трудом над Far Cry 3. Теперь они были вынуждены отказаться от любимого дела и выкладываться по полной, работая над проектом, от которого долгое время были оторваны. В какой-то степени это означало, что я нарушил свое обещание. По моему мнению, это была необходимая жертва для достижения наших долгосрочных целей, но использование подобного аргумента для менеджера — это всегда хождение по тонкому льду. Мне было одиноко, но, с другой стороны, не я сражался на передовой. Я чувствовал себя традиционным боссом, который сидит за столом и принимает рассудительные решения, не до конца понимая, чего будут стоить его тонкие стратегии людям.\n\r\nИ честно говоря, игра выглядела дерьмово. Это было ужасно, и никто не мог сказать, когда же все встанет на свои места. Никто из нас никогда не создавал игру такого уровня сложности. Эрик съездил в Париж на встречу и, вернувшись, рассказал, что ему задавали много вопросов обо мне, о Massive, о наших способностях и самой игре. Слова, оброненные кем-то в Париже, ранили как кинжал: Дэвид умеет продавать, но не создавать. Какое изящное немногословное оскорбление.\n\r\nВскоре после Рождества 2014 года я приехал в Париж. Штаб-квартира Ubisoft расположена в Монтрей, в восточной части Парижа. Это недалеко от кладбища Пер-Лашез, где похоронен легендарный вокалист группы The Doors Джим Моррисон. У Ubisoft была большая скидка в отеле «Мама Шелтер» на улице Рю-де-Баньоле, так что за завтраком я повстречал множество коллег со всего мира. Это было похоже на летний лагерь для тайного общества и укрепляло ощущение семьи, которое Ив культивировал в компании. Интерьер отеля был разработан Филиппом Старком, что придало этому месту странную атмосферу в духе ретро 80-х годов. Отель выглядел как крошечный ночной клуб с черными стенами, неоновыми лампами, простыми металлическими деталями и сценой, готовой к выступлению Pet Shop Boys. В фойе располагался магазин, торгующий масками для реслинга, сувенирами Филиппа Старка, дорогими мотоциклетными шлемами и книгами о дизайне. Интересно, подумал я, бывают ли у них покупатели.\n\r\nОтель находился достаточно близко от офиса, так что можно было дойти прогулочным шагом по тихим закоулкам и через шоссе, отделяющее настоящий Париж от Монтрей. Это различие важно для французов, но для меня не было никакой разницы между частями большого города. Насколько я мог судить, все это был просто Париж. Меня ждало несколько несложных встреч: дискуссия с центральной IT-службой о закупках, обсуждение пары кадровых вопросов и встреча, посвященная Just Dance Now, второстепенному проекту Massive. \n\r\nВо время моей прогулки в офис мне на почту пришло электронное письмо от моего босса, Кристин, с просьбой как можно скорее зайти к ней сегодня же. Этой встречи не было в планах. Мой разум тут же пришел в боевую готовность. За эти годы я понял, что у нее действительно трудная работа: быть скептиком среди романтиков. Ей всегда приходилось задавать непростые вопросы и участвовать в обсуждении неприятных тем. Сообщение от нее с просьбой о немедленной встрече, скорее всего, не означало ничего хорошего.\n\r\nВнезапно мне стало холодно под слабыми зимними лучами январского солнца. Мои мысли перенесли меня в другое время и в другое место, где несколько лет назад я медленно тонул в черной воде.\n\r\nКак-то раз я сдавал экзамен на лицензию дайвера в мутных водах Стокгольма, где абсолютно не на что смотреть. Там можно только научиться хорошо нырять. По ощущениям это было похоже на погружение в коричневый соус. Ничего общего с тем, что вы увидите, если погуглите слово «дайвинг».\n\r\nВ то время я встречался с девушкой, которая увлекалась подводным миром и только что вернулась с работы инструктором по дайвингу на Красном море. Ее бывший парень раньше служил в израильской армии и в моем воображении выглядел как крепко сложенный Брэд Питт на лендровере и, конечно же, с продвинутой дайверской лицензией в кармане. Как я мог за ним угнаться? Тем не менее я решил попробовать и в качестве сюрприза забронировал для нас двоих путешествие на Корсику, где, как я обещал, мы вместе погрузимся в лазурно-голубые воды Средиземного моря. Курс дайвинга в Стокгольме был подготовкой к тому, чтобы стать Лучшим Бойфрендом в истории человечества.\n\r\nЧерез несколько недель я получил лицензию, немного удивившись, насколько все оказалось просто и обыденно. Разве дайвинг не должен быть опасным и очень серьезным? Требующим дотошной точности во всем? Нет, очевидно, нет. Все это было больше похоже на то, как я представлял себе серфинг: клевые ребята сидят на пляжах и каменистых берегах в крутом профессиональном снаряжении, с банкой пива в руке и обмениваются историями о былых победах, укрепляя узы своего отважного братства.\n\r\nКогда мы, в конце концов, сели в лодку, направляющуюся к прекрасному французскому острову, я был уверен, что нырять гораздо легче, чем мне казалось раньше. Мы нашли хороший пляж где-то на юге, и там же был дайверский клуб, прекрасно соответствующий моим идиллическим ожиданиям. Клуб расположился в гараже недалеко от воды, и все, кто там работал, выглядели как накачанные загорелые моряки. Très круто.\n\r\nУверенный в себе инструктор по дайвингу посмотрел мой сертификат и крякнул.\r\n— Хм. Я вижу, вы новичок в этом деле? — спросил он с сильным французским акцентом.\r\n— Да, — сказал я, чувствуя, что обманул его ожидания. — Я только что получил сертификат.\r\n— Ладно, ладно, alors… — сказал он и, немного поколебавшись, продолжил: — В таком случае мы погрузимся только на восемьдесят метров, oui?\r\n— ВОСЕМЬДЕСЯТ? — повторил я, выпучив глаза. — Восемьдесят?!\r\nНасколько мне было известно, так глубоко могли погружаться только профессиональные дайверы, дышащие странными смесями вроде «Нитрокс» или «Тримикс».\r\n— Oui, — сказал он. — Хотя нет, подожди-ка. Zut alors! НЕТ! Я хотел сказать, что мы погрузимся максимум на восемнадцать метров. Восемнадцать, ясно? Не восемьдесят.\r\nВот это уже звучало логично. Восемнадцать — это, предположительно, безопасная и комфортная для новичков глубина, которая подошла бы и для меня. Сдавайся, Брэд Питт!\n\r\nНемного проплыв на лодке, мы прыгнули в воду и начали погружение недалеко от корсиканских скал. Пузырьки наших выдохов поднимались вверх. Я был окружен волшебным пейзажем подводного мира, наполненного маленькими чудесами и захватывающими формами жизни. Это не было похоже на стокгольмские погружения. Шесть метров. Десять. Четырнадцать, шестнадцать. И наконец, восемнадцать! Это было невероятно захватывающе, как какое-то неожиданное возвращение домой. Я жестами показал своему партнеру по дайвингу и по совместительству своей девушке, что я в порядке и счастлив. Было сложно рассмотреть ее лицо за всем этим оборудованием и регулятором дыхания, но я представил, как она улыбнулась мне с любовью.\n\r\nЯ снова посмотрел на свой измеритель глубины: двадцать. Двадцать четыре. Двадцать шесть… Что? ЧТО? Мы так не договаривались! Я никогда раньше не погружался глубже двенадцати метров, и мне стало страшно, но все остальные в группе, казалось, не замечали моего ужаса. Как будто каждый находился в своем собственном пузыре сознания. Двадцать восемь. Тридцать. Тридцать два. Я чувствовал, как колотится мое сердце, и протянул было руку к своему партнеру, чтобы убедиться, что все в порядке, но она была слишком далеко от меня, изящная как дельфин.\r\nА потом…\r\nЯ столкнулся со смертью.\r\nВ буквальном смысле.\n\r\nЯ пытался сделать еще один вдох, но почувствовал лишь твердое сопротивление клапанов регулятора. Полная остановка. Вентиль был закрыт. Я знал, что этому может быть несколько причин, но причина была неважна, потому что, какой бы она ни оказалась, результат был один: у меня не было кислорода! Я смотрел вверх на толщу воды. Тридцать два метра до поверхности. Это действительно было похоже на собор света, так мне и обещали, но, к сожалению, выплыть сквозь такую толщу на поверхность, не вдохнув хотя бы раз или два, просто невозможно. Я помнил из своего курса обучения, что человек может выжить без воздуха в лучшем случае несколько минут, а потом случится неизбежное. Утопление казалось ужасным, но внезапно очень реальным. И то, что Корсика — весьма живописное для смерти место, ничуть меня не утешало.\n\r\nМой мозг как бы наблюдал за мной с безопасного расстояния, и мысли в голове рождались примерно следующие: Хм. Интересно. Я словно бы наблюдал за ходом эксперимента, отстранившись до такой степени, что думал о себе в третьем лице. Как любопытно! Что он будет делать? Словно бы смотришь фильм, когда финал близок и храброго героя ждет неминуемая трагическая смерть. Я мог представить свою девушку в слезах на пляже, кричащую в гневе, обращаясь к богам и заходящему солнцу. Зачем? Зачем ты украл у меня мою любовь?\n\r\nНесколько секунд я просто висел в невесомости, окруженный подводной красотой. Вместо приятного звука поднимающихся на поверхность пузырьков воздуха я слышал только жуткую тишину. Я ждал собственной реакции. Буду ли я паниковать? Буду ли плакать? Или же моим последним действием среди живых станет нелепое размахивание конечностями в приступе позорного отчаяния?\n\r\nНо…\r\nНичего.\n\r\nЯ вообще ничего не чувствовал. Мой мозг начал холодно и рационально анализировать происходящее. Что я знал о нехватке воздуха? Какова была правильная процедура? Какие выходы из положения были еще доступны для меня?\n\n«Что ж, — подумал я, — вот, значит, как я реагирую в условиях экстремального давления. Неплохо».\r\nЯ наблюдал, как мое тело без всяких эмоций предпринимает правильные действия.\r\nДоплыть до партнера (даже если для этого быстрого движения потребуется потратить тот драгоценный воздух, который еще есть в легких).\r\nСделать знак правой рукой.\r\nНачать совместное дыхание через баллон партнера.\r\nКаждый делает по два вдоха.\r\nПоменяться.\r\nНемедленно прервать погружение.\r\nВернуться на поверхность.\r\nЖить.\n\r\nМой визит в штаб-квартиру Ubisoft в начале 2015 года оказался слишком похож на это воспоминание.\n\r\nКогда я уже входил в здание офиса, мне пришло еще одно сообщение: «Встреча пройдет в кабинете Ива».\n\n«О боже, — подумал я. — Погружаюсь. Восемнадцать. Двадцать два. Двадцать шесть».\r\nВ лифте я наткнулся на Николя Шенера, который контролировал все производственные процессы в Ubisoft. Я по привычке попытался завести светскую беседу.\n\r\n— Привет, как дела? — спросил я. — Как прошло Рождество?\n\r\nОбычно Николя был дружелюбным и разговорчивым, но сейчас он никак не отреагировал и молча смотрел на двери лифта. Он явно хотел оказаться подальше отсюда как можно скорее, и я понял, что мое общество стало нежелательным. Я был опасным знакомством, тем, кого нужно избегать, если тебе небезразлична твоя собственная карьера. Я понял, что погрузился на еще большую глубину. Тридцать. Тридцать два.\n\r\nЯ вошел в кабинет генерального директора, и то, что я увидел, можно было описать только как интервенцию. По другую сторону большого стола сидело более ста лет опыта: Ив, Серж, Кристин и мой приятель из лифта Николя. Они смотрели на меня в полном молчании. Было ясно, что кислород у меня закончился и дышать нечем. Оставалось только наблюдать, что произойдет дальше. Как и на Корсике, я увидел себя словно со стороны и начал рационально обдумывать происходящее, подыскивая оставшиеся варианты решения. «Если бы они собирались уволить меня, то не стали бы тратить на это столько времени», — подумал я и почувствовал слабую надежду.\n\r\nИв заговорил о своих опасениях. Он был зол, раздражен, раздосадован и разочарован. Таким я его еще не видел. Как только он закончил говорить, Серж перехватил эстафету и произнес еще одну обжигающую лекцию, пронявшую меня до глубины души. Раз мы потеряли его доверие, значит, это действительно было дно. Словно импровизирующий джазовый оркестр, каждый из четверых по очереди сыграл свое соло. Хотя мелодия была у всех одна: «Вот и все. Это твой последний шанс. Мы чрезвычайно обеспокоены, злы и разочарованы. Ты понимаешь, что поставлено на карту?»\n\r\nКонечно, я это понимал, но было не время умничать и пререкаться. Честно говоря, даже для диалога время было неподходящим, потому что все, что я мог сказать, прозвучало бы как плохое оправдание или попытка труса переложить вину на других. Моя карьера шла ко дну. Мой мозг лихорадочно работал. Я с трудом понимал все, что говорилось. Забудь о стыде и страхе. Сосредоточься на решении проблемы, на проекте. Поднимайся на поверхность.\n\r\nЯ взял блокнот и начал записывать все, что мне говорили. Во время вспышек гнева и особенно эмоциональных яростных выпадов я просто делал вид, что записываю, давая понять, что их слова услышаны и восприняты всерьез. Мысленно я уже тянул руки с жестом о помощи к моим доверенным партнерам по дайвингу Петтеру и Звуковику. Это был единственный способ спасти The Division.\n\r\nНесколько часов спустя я наконец ушел из офиса с головокружением и сединой в волосах. Впервые в моей карьере меня самым настоящим образом отчитали. Но главная проблема заключалась в том, что в чем-то они были правы. Проект пребывал в сложном состоянии, и нужно было что-то изменить, чтобы выполнить грандиозное обещание, данное нами на E3 в 2013 году.\n\r\nЯ решил сыграть последним припрятанным козырем, поставив на него будущее Massive. Вернувшись домой, первым делом я собирался сменить продюсера и креативного директора The Division. Я хотел поручить эти роли старым надежным товарищам, единственным людям, которые, я был уверен, смогут спасти ситуацию: Петтеру и Звуковику.\n\r\nБолее подробно с книгой можно ознакомиться на сайте издательства\r\n» Оглавление\r\n» Отрывок\n\r\nДля Хаброжителей скидка 25% по купону — Вселенная\n\r\nПо факту оплаты бумажной версии книги на e-mail высылается электронная книга.",
        "user": "\n      ph_piter\n    ",
        "time": "сегодня в 17:18",
        "ratings": " 139.77 \n    Рейтинг\n  ",
        "hub": "Блог компании Издательский дом «Питер» Профессиональная литература ",
        "suit": "\n      piter.com\n    ",
        "date": "5  сентября  1991"
    },
    {
        "url": "https://habr.com/ru/company/vk/blog/589289/",
        "title": "Tarantool на процессорах Apple M1: первые результаты",
        "tag": "tarantoolM1процессоры",
        "body": "\nSpiderman + Youtube speedpaint by RowenHebing\nПроцессоры M1 от Apple уже давно не новость. Многие знают об их быстродействии и о том, что приложения для MacOS нужно адаптировать под новую архитектуру. В команде разработки платформы Tarantool мы тоже недавно поставили перед собой такую задачу.\nЯ, Алексей Корякин, технический директор Tarantool, входящего в экосистему VK. Расскажу, зачем нам вообще это было нужно (ведь macOS не устанавливают на продакшен-серверы), как мы решали задачу, и покажу результаты бенчмарков.\nКак появилась задача и как ее решали\nTarantool — высокопроизводительная платформа in-memory-вычислений, которая состоит из базы данных и application-сервера. Зачастую разработчики устанавливают Tarantool на свои рабочие машины и пишут код там. Для многих это удобней, чем работать на отдельном сервере, особенно если у них один рабочий компьютер.\nНекоторые разработчики из нашей команды тоже устанавливают себе Tarantool локально. Так же делает и наш product-менеджер, который в начале года купил новый MacBook Air с процессором M1. И вот однажды он пришел к технической команде с вопросом: «А почему Tarantool не работает нативно на процессоре M1? Я недавно купил новый MacBook Air, а Tarantool запускается только через Rosetta. Нативная поддержка процессоров Apple M1 могла бы стать отличным плюсом Tarantool для нашего комьюнити, чтобы коллеги, переехавшие на перспективные маки, могли эффективно разрабатывать системы под Tarantool».\nТехническая команда подумала и решила:\n\nTarantool известен тем, что он очень быстрый. M1 известен тем же. Нам стало интересно, а насколько быстрее может стать Tarantool, если посадить его на M1?\nApple активно обновляет всю линейку компьютеров Mac, переводя их на M1 (а теперь и на M1 Max), а разработчики и другие ИТ-специалисты по всему миру активно пересаживаются на новую платформу. Существующий x86_64-софт запускается через прослойку эмуляции Rosetta, которая не позволяет полноценно использовать всю мощь софта (включая Tarantool) на новых процессорах Apple. Надо это исправлять.\n\nИменно так у нас появилась новая приоритетная задача — поддержка процессора M1🙃 \nПримерно в то же время мы работали над поддержкой ARM64 для Linux. Так как M1 — это тоже, по сути, ARM64 со своей спецификой, мы решили, что сможем просто реализовать поддержку M1. Это оказалось близко к правде: большую часть задач по поддержке M1 мы закрыли поддержкой ARM64 для Linux. Основные проблемы были связаны со спецификой инструкций RISC архитектуры ARM (например, директ-передача управления от одного участка машинного кода другому возможна только в рамках 2 Мб смещений). Отличительной особенностью поддержки M1 стало Apple ABI, отличающееся от ARM Linux ABI. Пришлось тюнить код Tarantool специально под новые процессоры, подглядывая в открытую документацию Apple.\nВ целом поддержка ARM64 и M1 в частности — относительно простая инженерная задача. Были небольшие особенности, но они решались просто, без чтения тонны спецификаций и работы по ночам. На все у нас ушло примерно 4 месяца, с мая по август.\nБенчмарк производительности\nM1 известен своим быстродействием. Даже код, запускаемый через Rosetta, работает быстро. Мы тоже не обошли вопрос производительности стороной, решили проверить, насколько же быстрее стал наш Tarantool.\nМы сравнивали именно macOS на доступных commodity-устройствах с разной начинкой. У нас не было задачи сравнить разные ОС или гоняться с серверными процессорами вроде Xeon. Мы хотели узнать, насколько быстрее станет Tarantool для разработчика, который пересядет на новый макбук, а также предварительно оценить перспективы новой Mac-платформы. Для теста использовались несколько компьютеров, которые сейчас можно встретить у разработчиков или купить в re:Store:\n\nMacBook Pro 16,2 (2020),\nMac mini 8,1 (2018),\nMacBook Air 10,1 (2020), Apple M1.\n\nМы написали простой бенчмарк, у которого есть три составляющие:\n\nЭто код на Lua, а значит, задействуется application-сервер.\nЭтот код пишет в базу данных, а значит, задействуется транзакционный движок БД.\nЭтот код работает на М1 и не падает :)\n\nБенчмарк работает в одном системном треде, который запускает 50 файберов, каждый из которых вставляет по 100 операций за одну транзакцию. Такой сценарий выдает больше нагрузки на CPU, чем на память или диск. А раз мы тестируем процессор, это именно то, что нам нужно. Если бы транзакция состояла из одного или 3–4 апдейтов, тогда нагрузка сместилась бы скорее на RAM или диск, а не на CPU.\nМы проводили несколько тестов, вставляя от 1 до 20 миллионов записей. Причем каждый тест мы проводили по 15 раз и потом рассчитали медианное значение. Вот полные результаты, там можно детально посмотреть на все запуски. А тут для наглядности мы покажем медианные значения в виде графиков.\n\nWall Clock Benchmark, медианное значение 15 запусков. Чем меньше значение, тем быстрее отработал код\nВидим, что Tarantool на ноутбуке с процессором M1 показывает в 2 раза лучшие результаты, чем на ноутбуке того же года с другим процессором.\nТакже мы протестировали M1 при работе через транслятор Rosetta. По уровню производительности он оказался примерно равным Mac mini 2018 года, но заметно быстрее MacBook Pro 2020 года.\n\nWall Clock Benchmark. Те же устройства плюс запуск через Rosetta\nМы понимаем, что это не означает ускорение всех приложений в два раза. У всех специфичный код, разные задачи и условия. Но в любом случае можно ожидать, что ваша локальная инсталляция Tarantool будет работать быстрее.\nЧто в итоге?\nНачиная с версии 2.10.0-beta, Tarantool можно нативно запускать на процессорах M1. Пока что это preliminary-поддержка: что-то может сломаться или работать нестабильно. Сейчас мы закрыли почти все известные нам баги, осталось только несколько некритичных. Например, есть некоторые проблемы с JIT-компилятором. Но это не помешало product-менеджеру команды установить Tarantool на свой новый MacBook Air и каждый день с ним работать.\nДальше мы будем закрывать известные нам баги, а также те, которые будут присылать разработчики. Если у вас есть новый мак на процессоре M1, предлагаем попробовать новую версию Tarantool. Если у вас что-то не заработает — пишите баг-репорт, мы поможем.\nПопробовать кластер Tarantool можно на try.tarantool.io, а получить помощь — в Telegram-чате.",
        "user": "\n      techbeyond\n    ",
        "time": "сегодня в 17:07",
        "ratings": " 285.63 \n    Рейтинг\n  ",
        "hub": "Блог компании VK Высокая производительность *Тестирование IT-систем *Tarantool *",
        "suit": "\n      vk.com\n    ",
        "date": "15  октября  1998"
    },
    {
        "url": "https://habr.com/ru/company/kaspersky/blog/589263/",
        "title": "Security Week 46: новая уязвимость в Exchange Server",
        "tag": "exchange",
        "body": "На прошлой неделе, 9 ноября, компания Microsoft выпустила очередной набор патчей для собственных продуктов. Он закрывает 55 уязвимостей, из них 6 критических. Особое внимание уделено новой уязвимости в почтовом сервере Microsoft Exchange. Уязвимость CVE-2021-42321 (описание на сайте производителя, статья в BleepingComputer) оценивается в 8,8 балла по шкале CVSSv3 и позволяет выполнять произвольный код на сервере. Уязвимость работает после авторизации на сервере и уже используется в таргетированных атаках. \n\n\r\nУязвимости подвержены серверы Microsoft Exchange версий 2013, 2016 и 2019, работающие на стороне заказчика. Облачный вариант сервиса Exchange Online вне опасности. Проблема была обнаружена в процессе валидации команд в Exchange Powershell, средстве автоматизации работы с сервером. Еще одна серьезная уязвимость закрыта на прошлой неделе в Microsoft Excel. CVE-2021-42292 позволяет обходить встроенные средства защиты и выполнять произвольный код при открытии документа.\n \r\nПомимо этого, на прошлой неделе был опубликован Proof of Concept эксплойта для уязвимости CVE-2021-34484. Данная уязвимость изначально была закрыта в пользовательских и серверных версиях Windows еще в августе этого года. Тогда считалось, что это неопасная уязвимость, позволяющая удалять произвольные папки на жестком диске. Для ее эксплуатации требовался локальный доступ к компьютеру, который в принципе и так позволяет удалять данные. Исследователь Абдельхамид Насери (Abdelhamid Naceri) нашел возможность использовать баг для повышения привилегий в системе, а заодно обнаружил и способ обхода ранее выпущенного патча.\n\nЧто еще произошло\n\r\n12 ноября с почтового сервера ФБР было разослано фейковое сообщение о «кибератаке». Журналист Брайан Кребс (Brian Krebs) приводит версию самого взломщика: по его словам, он эксплуатировал уязвимость в системе регистрации новых пользователей для одного из сервисов ФБР. Через веб-интерфейс он смог инициировать отправку сообщений с почтового сервера, подставляя собственные данные в поле «тема» и в тело письма.\n\r\nЭксперты «Лаборатории Касперского» анализируют последствия посещения веб-сайтов, обещающих бесплатный доступ к популярным стриминговым платформам. Чаще всего рекламируется доступ к Netflix, хотя самое популярное телешоу у мошенников от другого поставщика — «Мандалорец». Результатом посещения такого ресурса может стать как потеря средств в результате кражи платежных данных, так и установка трояна на компьютер. \n\r\nСвежее вредоносное ПО для атаки на роутеры BotenaGo написано на языке Golang и включает более 30 эксплойтов для популярных устройств.\n\r\nЭксперт по парольным утечкам Трой Хант (Troy Hunt) приводит примеры Beg Bounty — попыток получить деньги от владельца ресурса за обнаружение тривиальных «уязвимостей» вроде отсутствия записи DMARC. Он называет это откровенно вредным явлением, так как подобные запросы отвлекают от реально серьезных проблем. И обращает внимание на необходимость публикации контактов для передачи информации о (настоящих) уязвимостях.",
        "user": "\n      Kaspersky_Lab\n    ",
        "time": "сегодня в 17:03",
        "ratings": " 269.84 \n    Рейтинг\n  ",
        "hub": "Блог компании «Лаборатория Касперского» Информационная безопасность *",
        "suit": "\n      www.kaspersky.ru\n    ",
        "date": "21  июля  1997"
    },
    {
        "url": "https://habr.com/ru/post/589285/",
        "title": "Создал дом из Симпсонов в 3D",
        "tag": "blender3d-моделированиеsimpsonsмультфильм",
        "body": "Всем привет это Денис Вебер.Сегодня я расскажу как создавал в 3D один из самых знаменитых домов во всём мире - дом семьи Симпсонов. Как обычно, специально для тех, кому больше нравится формат видео, а не текст, ссылку на видео и финальный результат я оставлю в конце статьи, Симпсоны я начал смотреть ещё в детстве и конечно не понимал и половины шуток в сериале. Но сейчас, периодически пересматриваю мои любимые серии с седьмого по двадцатый сезон и с каждым разом они нравятся мне всё больше. Думаю не стоит рассказывать о том, насколько успешными являются Симпсоны и как много народной любви они получают до сих пор.С момента создания канала PolyRoad, я нахожу идеи для своих новых проектов во всём. Так было с созданием города из игры Герои и моделированием уровней из Battletoads. И мне захотелось создавать один из самых главных символов сериала - дом Симпсонов. В нём они проводят чуть ли не большую часть времени всего сериала.В этот раз я создам настоящий дом Симпсонов, с подробным планом комнат, гаражом, задним двором и лужайкой. В сериале на вид он мультяшный, каким и должен быть, но я решил добавить чуть больше реализма, с помощью текстур и освещения. И в итоге, мы с вами увидим как дом симпсонов мог бы выглядеть в реальной жизни.В интернете я нашёл сразу несколько планов дома, но этот мне показался наиболее подходящим. Самым сложным во всём проекте было то, что на разных кадрах из мультфильма дом выглядит по-разному. Деревья то появляются, то исчезают. На схеме дома окон меньше, чем на виде снаружи, а внутри расположение отличается и от схемы. В общем, мне нужно было выбрать что-то среднее между теми вариантами, которые я нашёл.Я мысленно заглянул в будущее и подумал, что мне в любом случае придётся менять расположение стен, окон и других элементов дома, но, а пока я накидал план стен, перегородок, проёмов для дверей и окон для первого этажа и принялся за второй. На втором этаже планировка слегка отличается от первого, но несущие стены на этой схеме почему-то местами не совпадают, поэтому я подогнал их для каждого этажа. В доме есть ещё и подвал, но так как его не видно снаружи, я пока не стал его создавать.Такие схемы и планы зданий на самом деле сильно облегчают жизнь моделлерам. Если бы не они, пришлось бы собирать дом только по кадрам из мультфильма и получилось бы что-то наподобие дома Фландерса который построили в одной из серий.Сейчас здание похоже на долгострой в одном из спальных районов, но совсем скоро оно примет более приятный вид. После того как я разобрался с окнами и дверями, добавил крыши для разных частей дома.В левой части красуется высокая печная труба через которую Санта каждый год попадает к Симпсонам в камин. Сама по себе труба очень узкая, но мультфильмы часто пренебрегают такими мелочами как реализм.Уже сейчас можно прогуляться по комнатам дома с ремонтом от застройщика. Одна из важных частей всей сцены - забор. Он огораживает участок Симпсонов, а у меня в проекте еще будет использоваться как ограничение вообще всей сцены.Чем дальше я продвигался в создании дома, тем больше референсов мне требовалось и я сделал больше ста скриншотов из разных серий симпсонов, чтобы точно ничего не пропустить. Справа от дома я добавил стену, которая отделяет участок Симпсонов от участка соседей. Кстати, никогда не задавался вопросом кто, кроме Фландерсов, живёт по соседству с главными героями.По моим скудным познаниями в архитектуре американских пригородов и игре Cities XL, я знаю, что здания в них стоят вдоль однотипных дорог и тротуаров которые делят большие районы на районы поменьше. Я добавил выезд из гаража и выставил плиты с бордюрами, для которых потом создал материал наподобие бетона. Можно было добавить немного вариативности в виде сколов и трещин, но плиты в сцене далеко не самый главный объект.Большие окна на первом этаже чем-то напомнили мне лоджии в российских домах времён СССР. Бывает начинаешь моделировать какую-нибудь делать, а спустя тридцать минут проклятий, понимаешь, что всё можно было создать с помощью других инструментов и гораздо проще.Для окон сверху я выдавил контур, добавил перекладины и объём с помощью модификатора Solidify.На скринах из мультфильма стёлка в доме имеют голубой оттенок и полностью непрозрачны, чтобы нельзя было заглянуть внутрь. Конечно иногда, они становятся прозраными, если это требуется по сюжету. С давних пор я перестал искать логику в мультфильмах. Для стёкол я настроил шейдер стекла, который по умолчанию есть в блендере. Хоть сейчас стёкла и выглядят непрозрачными, на финальном рендере всё будет более приближено к реальности.Кстати, в своём следующем видео я буду создавать текстуры с помощью программы Substance Painter. Если вы никогда не слышали о ней - это программа, по сути стандарт геймдев индустрии. С помощью неё огромное количество игровых студий создают текстуры для своих хитов.А тем временем я добавил оконные рамы и стёкла для всех окон дома. Так как окна имеют почти одинаковую форму и одинаковые материалы, достаточно было скопировать уже созданные модельки и подогнать их по размерам. В своих видео я постоянно сравниваю 3д объекты с объектами из реальной жизни. Например, здесь, я смоделировал ворота для гаража, которые потом, слегка доработав, можно будет открывать и закрывать, как самые настоящие.Двери тоже можно сделать буквально в пару кликов. Я создал контур, выдавил из него геометрию, а потом придал объёма с помощью того же модификатора Solidify. Пока я использую базовые материалы, но совсем скоро добавлю им больше реалистичности.Ещё несколько штрихов в виде дверных ручек, замочных скважин и двери почти готовы. Осталась только одна деталь. У неё даже есть название - дверной молоток. Мелочи всегда сильно влияют на общее впечатление о сцене.На заднем дворе Симпсонов больше всего объектов. Тут и дом на дереве и качели, гриль для мяса и много чего ещё. В разных сериях сериала, на заднем дворе предметы, и даже деревья то пропадают, то появляются и с завидным постоянством меняют своё расположение, поэтому я решил выбрать несколько разных объектов, которые бы смотрелись гармонично и были в тему.Я начал с оранжевой плитки, которую создал из обычной плоскости, добавив углубления. Почти во всех кадрах из сериала, около двери во двор стоит гриль на колёсиках. Такой гриль в нашей стране превратится в одноразовый мангал, который все используют годами.Гриль состоит из сферы, которую можно разделить, если нужно будет создавать анимацию, но, а пока я просто создал похожую на оригинал модельку, добавил ей ручки, ножки, решётку и колёса. Как я не устаю говорить, вся суть в мелочах. Поэтому я конечно не забыл про антенну на крыше.На заднем дворе, где-то между кустами закопан небольшой мусорный бак. Он играет не последнюю роль в сериале и в одной из серий, Гомер использовал его в качестве средства для лечения больных спин жителей Спрингфилда. Чтобы более точно передать вид американского бака с заднего двора, я скачал такой референс и смоделировал бак по нему.Создаём цилиндр, выдавливаем его верхнюю часть вовнутрь, а по бокам добавляем геометрию для углублений, накрываем бак крышкой, созданной с помощью петли самого бака. Готово. Теперь вы знаете как создать мусорный бак в блендере. И не забудьте про ручку.На лужайке перед домом красуется почтовый ящик с модным красным флажком. Я смоделировал ящик из цилиндра и куба. Проще простого.Неважно, что этот кран пропадает и появляется когда ему нужно, главное, что для него тоже нашлось своё место. Если я не уверен в каком-то референсе, всегда ищу другие. В большинстве случаев зрители не заметят никаких различий.Если смотреть на дом с такого ракурса, на крыше не видно отдельных кусков черепицы, которые появляются, когда камера подлетает ближе. Я решил создать черепицу, которая будет больше приближена к реальности. Для этого я добавил первый кусок, размножил его с помощью массива и подогнал получившееся полотно под размер крыши. Потом отзеркалил на противоположную сторону и добавил модификатор Displace. Черепица получила небольшие неровности, чтобы не выглядеть просто гладкими плитками. Перед тем как закончить с крышей, я настроил шейдеры так, чтобы блендер красил кусочки черепицы в случайные оттенки коричневого цвета. Согласитесь, так всё выглядит намного интереснее.Раз уж я взялся за черепицу, почему бы не добавить вариативности и забору. Может показаться, что черепица и забор совершенно разные модельки, но для забора я добавил точно такой же модификатор и так же как для черепицы настроил текстуру.В некоторых местах дома выступают кирпичи. Это что-то наподобие потёртостей или отвалившейся штукатурки. Можно было сделать их с помощью текстур, но я решил не заморачиваться, выдавил геометрию в нужных местах и покрасил кирпичи другим цветом.Для текстуры дома я собрал такие ноды. На первый взгляд кажется, что это очень заморочисто, но на самом деле здесь просто смешивается несколько типов процедурных текстур и шумов. Зато после настройки цвета результат получился таким, как я хотел.Я добавил текстуру травы и начал настраивать материал бетона для плит. В отличие от PBR текстур, о которых я говорил в одном из видео, процедурная генерация не использует текстуры, а создаёт материал как бы \"на лету\".Для дерева я выставил такие настройки материала. И снова добавил разнообразия в цветах с помощью нода Object Info.Для оконных рам, дверных косяков и других деревянных частей я добавил похожий материал дерева. На самом деле можно вечно настраивать, изменять, добавлять какие-то дополнительные ноды для текстур, но жаль, что большинство таких изменений просто будет незаметно обычному зрителю. Главное, чтобы в итоге картинка выглядела так, как нравится дизайнеру.Пришло время домика на дереве. Этот домик тоже участвовал во многих сериях Симпсонов. По сути домик на дереве - это обычное дерево, ветки которого растут так, что на них можно разместить деревянную коробку. Поэтому половина дела - смоделировать дерево. Ствол дерева можно создать из обычного куба, выдавливая геометрию в нужные стороны и ветки точно также.Сам домик наспех сколочен из неровных деревянных досок, в которых вырезаны дырки для окон и дверей. По своей сути это обычный сарай. Я добавил лестницу, созданную из тех же досок, что и сам дом и какое-то время потратил на то, чтобы придать веткам нужную форму.Листву я создал точно таким же образом, как в проекте с городом из Герои 3.Ещё одно дерево стоит на противоположной стороне двора. Для него я сделал всё точно также, только немного изменил расположение веток.Если в обычной жизни вырвать часть веток из дерева и посадить их как куст не получится, в 3д возможно всё. Кусты в моём проекте - это просто куски дерева. И совсем необязательно придумывать что-то новое.К сцене прибавляется всё больше объектов и рассказывать о каждом из них нет смысла. Потому что моделируются они просто, а какой-то большой роли не играют.На заднем дворе я решил поставить полноценные качели. Я собрал их из кривых и добавил текстуру красного крашеного металла. Нужно сказать, что не в каждом дворе в России есть даже такие качели.Если для вас прошло несколько минут, для меня почти двадцать часов. Удивительно, но я практически доделал сцену. Можно присмотреться к кадрам из мультфильма и увидеть, что газон показан чёрточками. В разных местах самого газона и по краям рядом с объектами. Я решил, что лучше всего трава будет смотреться в местах, где её нельзя срезать, а именно у корней деревьев, рядом со стенами и заборами, поэтому добавил частицы с травой как раз в эти места.На заднем дворе в некоторых сериях можно увидеть будку для собаки Симпсонов - Маленького Помощника Санты. Интересно, что по сути весь сериал начинается именно с Маленького Помощника Санты. В самой первой серии Гомер и Барт забирают его к себе домой с собачьих бегов.  Для будки я использовал всё те же доски, и перекрасил их в розовый цвет. Десять минут и конура готова.Можно было найти ещё с десяток предметов, которые бы хорошо смотрелись на сцене, но я решил немного притормозить и привязал колесо к дереву, которое часто мелькает в сериале, а лежанку с маленькой лестницей поставил рядом с домом. Теперь для комфортного проживания есть все необходимые условия.Осталось добавить совсем небольшие детали. Например, коврик на входе и свернутую газету, которую не успели убрать с лужайки. Я добавил кусты по периметру участка и конечно же самый главный атрибут - занавески. Пусть они и не сильно заметны снаружи, но точно создают дополнительный уют.Пару часов я потратил на добавление облаков, и деревьев вокруг участка, чтобы дом посреди поля не смотрелся одиноко, а прямо сейчас я готов вам показать то, что получилось.В очередной раз скажу, что получил большое количество опыта. Несмотря на то, что некоторые вещи становится делать чуть легче, чем раньше, я всё равно продолжаю сталкиваться с тем, что для меня непонятно. Всё это значит только одно: нужно повышать сложность проектов ещё больше.Напишите в комментариях, нравится ли вам тема Симпсонов. Если эта тема будет интересна, я пойду дальше и полностью повторю интерьер дома. Судя по кадрам из мультфильма, там есть над чем поработать.Видео с канала PolyRoad:",
        "user": "\n      dendead\n    ",
        "time": "сегодня в 17:03",
        "ratings": " 269.84 \n    Рейтинг\n  ",
        "hub": "Работа с 3D-графикой *DIY или Сделай сам ",
        "suit": "\n      www.kaspersky.ru\n    ",
        "date": "21  июля  1997"
    },
    {
        "url": "https://habr.com/ru/company/otus/blog/589281/",
        "title": "Моделирование атрибуции в маркетинге",
        "tag": "dwhмаркетингМоделирование атрибуцииатрибуция в маркетингеАтрибуционная модель данныхolapclickhousedata engineer",
        "body": "Атрибуция в маркетинге долгое время считалась одной из самых неприятных аналитических задач. Но объединив сырые данные (raw data), SQL и dbt (Data Build Tool) ранее такая сложная задача может стать невероятно простой.Атрибуция в маркетинге — это по сути ваше понимание механизмов и тактик маркетинга, которые способствуют привлечению новых клиентов в ваш бизнес. Рано или поздно каждому аналитику предстоит углубиться в дремучий лес маркетинговой атрибуции, ведь это критически важная информация, которая необходима каждой маркетинговому отделу в мире. Если вам будет интересно познать степень опечаленности ваших коллег при упоминании маркетинговой атрибуции, просто просмотрите цепочку комментариев под этим твитом (лично я остановился после 50-ого):В чем проблема с маркетинговой атрибуцией для SaaS?Смог ли кто-нибудь выработать к ней подход или это гиблое дело?Что на счет разных моделей: first-touch, last-touch, linear, decay, the works...И прошу, только не заливайте мне о Google Analytics.Что вы там найдете:Продукты, которые стоят тысячи долларов в месяц, чаще всего построены по “принципу черного ящика” без какого либо намека на прозрачность.“Простые”, “готовые” решения, которые стоят недорого, предлагают анализ только ограниченного объема данных.Маркетологов, которые полностью опустили руки (просто действуют наобум) и измотаны обещаниями вендоров.По мнению маркетологов, атрибуция — это задача обработки данных: “Просто получите эти данные, и у вас сложится полная картина того, что работает!” — но на самом деле, это задача моделирования данных. Логика за вашей моделью атрибуции, а именно то, что данные говорят о вашем бизнесе, также важна, как и сами данные. И эта логика будет меняться в зависимости от вашего бизнеса. Вот почему так много продуктов для атрибуции не работают на практике.Так что же вам действительно необходимо для построения модели атрибуции?Сырые данные в вашем хранилище (warehouse), которые отражают все многообразие взаимодействия клиентов с вашим брендом. Для электронной коммерции — это будут посещения их веб-сайтов. Для клиентов B2B - это могут быть разговоры с отделами продаж.2. SQLСобственно на этом все! Добавив SQL поверх сырых данных, как результат, вы получите следующее:Самую дешевую модель атрибуции. В этом руководстве предполагается, что вы работаете с современным стеком данных, и уже обладаете всей необходимой инфраструктурой:Данные о событиях вы собираете, например, с помощью инструментов Snowplow или Segment (хотя Segment может быть дороговат);Данные с рекламных платформ извлекаете с помощью Stitch или Fivetran;Загружаете данные в современное облачное хранилище данных, такое как Snowflake, BigQuery или Redshift;Используете dbt, чтобы ваши аналитики могли моделировать данные в SQL.2. Самую гибкую модель атрибуции. Вы владеете бизнес-логикой и можете расширять ее по своему усмотрению и легко менять, подстраивая под изменения в бизнесе3. Самую прозрачную модель атрибуции. Вам не нужно полагаться на логику вендоров. Если ваш отдел продаж считает, что ваша атрибуция неверна, покажите им dbt-доки, пройдитесь с ними по логике вашей модели и внесите изменения с помощью одной строки SQL.Атрибуция в маркетинге подразумевает большие наборы данных, и это действительно интересная инженерно-аналитическая задача. Давайте же рассмотрим, как построить лучшую модель атрибуции, которую когда-либо видел ваш бизнес.Атрибуционная модель данных На самом деле невозможно точно сказать, почему тот или иной человек становится вашим клиентом. Лучшее, что мы можем сделать, как аналитики, чтобы подобраться к сути — это сделать довольно хорошее предположение. Для этого мы собираемся использовать подход, называемый позиционной атрибуцией (positional attribution). По сути, это означает, что мы будем взвешивать важность различных касаний (touch — взаимодействие клиента с брендом) в зависимости от их положения (порядка, в котором они происходят на протяжении жизненного цикла клиента).Для этого мы собираемся построить таблицу, содержащую каждое “касание” (touch), которое предшествовало тому, как конкретный человек стал клиентом, и канал, который привел к этому касанию.Затем мы оцениваем относительный вес каждого касания, которое привело к конверсии. Эта оценка выполняется путем присвоения касаниям «баллов»: каждая конверсия приносит ровно один балл, и этот балл делится между касаниями клиента. Существует четыре основных способа распределить этот балл:First touch: приписать заслугу за всю конверсию первому касанию.Last touch: приписать всю конверсию последнему касанию.Forty-twenty-forty: приписать 40% (то есть 0,4 балла) к первому касанию, 40% к последнему касанию, и оставшиеся 20% разделить между всеми касаниями поровну.Linear: разделить балл поровну между всеми касаниями.Нет однозначного и быстрого ответа на вопрос, какой метод атрибуции вам следует использовать — обсудите со своим отделом маркетинга, что больше всего подходит для вашей компании.В общем, вот примерно то, с чем мы будем работать (на примере электронной коммерции)CUSTOMER_IDSESSION_IDSTARTED_ATUTM_SOURCEUTM_MEDIUMUTM_CAMPAIGNCONVERTED_ATFIRST_TOUCH_POINTSLAST_TOUCH_POINTSFORTY_TWENTY_FORTY_POINTSLINEAR_ATTRIBUTION_POINTS74542020-02-03 12:29:32facebook_adspaid_social10percentpromocode2020-02-04 10:05:31100.40.25745162020-02-03 18:50:24adwordspaid_searchbranded_search2020-02-04 10:05:31000.10.25745232020-02-04 04:50:24direct2020-02-04 10:05:31000.10.25745322020-02-04 10:05:21direct2020-02-04 10:05:31010.40.25295612020-02-01 12:55:16facebook_adspaid_social10percentpromocode2020-02-10 01:54:54100.40.332956682020-02-10 00:51:56facebook_adspaid_socialfreeshipping2020-02-10 01:54:54000.20.332956692020-02-10 01:53:55bingsearch2020-02-10 01:54:54010.40.33Здесь мы видим, что у клиента 745 было четыре взаимодействия до конверсии: он сначала перешел на ваш сайт по рекламе в Facebook, затем он попал туда через Adwords-рекламу, и, наконец, посетили ваш сайт, введя URL-адрес прямо в браузер (дважды!) перед покупкой.Если вы сразу готовы погрузиться в dbt-проект, чтобы увидеть, как это все работает, вы можете ознакомиться с нашим примером проекта атрибуции здесь. Если нет, то далее для вас последует подробный разбор.Как построить модель атрибуции1. Соберите необходимые источники данныхСессии:Необходимая dbt-техника: пакетыНам нужна таблица, которая отражает каждый раз, когда клиент взаимодействует с нашим брендом. Для компаний электронной коммерции самое близкое, чем мы можем воспользоваться, — это сессии (sessions, сеансы). (Если же вы работаете в B2B организации, вам скорее всего больше подойдут таблицы взаимодействий между вашим отделом продаж и потенциальным клиентом из вашей CRM).Сессии — это дискретные периоды активности клиентов на сайте. Отраслевые стандарты определяет сессию как серию действий, за которыми следует 30-минутное окно без какой-либо активности.Вот пример:SESSION_IDCUSTOMER_IDSTARTED_ATENDED_ATUTM_SOURCEUTM_MEDIUMUTM_CAMPAIGN129562020-02-01 12:55:162020-02-01 12:55:47facebook_adspaid_social10percentpromocode242020-02-02 13:06:472020-02-02 13:18:56facebook_adspaid_socialfreeshipping311702020-02-03 12:15:002020-02-03 12:15:19facebook_adspaid_social10percentpromocodeСтоит отметить, что сессии также содержат реферальную информацию, которая помогает нам понять откуда пришел клиент. Эти UTM-теги часто устанавливаются отделом маркетинга, поэтому всегда стоит сначала проверить, есть ли у них какая-либо таблица, определяющая теги, которая использует ваша компания. Точная иерархия для source, medium и campaign часто варьируется от бизнеса к бизнесу. Вот что говорится по этой теме в доках Google Analytics:utm_source: рекламодатель, сайт, публикация и т. д., который направляет трафик на ваш ресурс, например: google, newsletter (рассылка по электронной почте), billboard (баннер).utm_medium: рекламное или маркетинговое средство, например: cpc, баннер, рассылка по электронной почте.utm_campaign: имя кампании, слоган, промокод и т. д. применимые к продукту.Для создания таблицы сессий, на вашем сайте должна быть реализована ​​система отслеживания событий, которая генерирует запись по факту каждого просмотра страницы. Если вы используете бесплатную версию Google Analytics, к сожалению, невозможно получить такой уровень детализации в ваших данных. Вместо того, чтобы тратить более 100 тысяч долларов в год на Google Analytics 360 для получения доступа к исходным данным, мы рекомендуем вам использовать опенсорсную альтернативу — Snowplow. Если ваша команда уже использует Segment или Heap, то это тоже хорошие альтернативы Google Analytics.После того, как данные о просмотрах страниц уже есть в вашем хранилище, вам нужно будет сделать две вещи:Разбивка на сессии (sessionization): агрегировать эти просмотры страниц в сессии, добавив логику, которая определяет промежутки продолжительностью 30 минут или более.2. Связывание пользователей (user stitching): если пользователь впервые посещает ваш сайт без какой-либо идентифицирующей информации (обычно это “customer_id” или “email”), а затем по прошествии какого-то времени конвертируется, его предыдущие (анонимные) сессии нужно обновить, закрепив их за этим пользователем. Ваша система веб-отслеживания должна уметь связывать эти сессии вместе.Такого рода моделирование — достаточно сложная задача, особенно для компаний с тысячами просмотров страниц в день (Спасибо, Господи, за инкрементальные модели). К счастью, кое-кто очень умный написал пакеты, которые сделают за вас всю тяжелую работу, независимо от того, отслеживаются ли просмотры ваших страниц с помощью Snowplow, Segment или Heap. Пользуйтесь этими благами, установив соответствующий пакет для преобразования данных.Конверсии:Вам также нужно знать, когда клиент совершил конверсию. Обычно для компании электронной коммерции это факт совершения первого заказа этим клиентом.CUSTOMER_IDCONVERTED_AT11702020-02-03 14:20:0820142020-02-04 4:30:2122652020-02-04 9:43:35Вам может потребоваться преобразовать данные для получения этой формы -  для этого используйте dbt.2. Найдите все сессии предшествующие конверсииМы хотим ограничить наш анализ только сессиями, которые произошли до конверсии. Для этого нужно объединить два источника данных:select\n    *\nfrom sessions\n\nleft join conversion using (customer_id)\n\nwhere sessions.started_at <= customer_conversions.converted_at\n\tand sessions.started_at >= dateadd(days, -30, customer_conversions.converted_at)Мы часто ограничиваем сессии, которые засчитываются для атрибуции, только сессиями в течение 30 дней, предшествующих конверсии (что часто называют “окном атрибуции”). Это имеет особый смысл, когда у вас есть четкое представление о пути конверсии, который находится в определенном временном диапазоне; если вы будет засчитывать древние касания, это может привести к получению странных или бесполезных данных.3. Рассчитайте общее количество сессий и их индексыНеобходимая SQL-техника: оконные функции.Когда мы ограничились только сессиями, предшествующими конверсии, нам нужно узнать следующую информацию:Сколько сессий было у этого клиента до конверсии? (total_sessions)Каков порядковый номер каждой сессии в рамках каждой конкретной группы сессий? (session_index)Эти поля формируют основу для расчета количества баллов атрибуции, присваиваемых каждой сессии.select\n    *,\n\n    count(*) over (\n        partition by customer_id\n    ) as total_sessions,\n\n    row_number() over (\n        partition by customer_id\n        order by sessions.started_at\n    ) as session_number\n\nfrom sessions\n\nleft join customer_conversions using (customer_id)\n\nwhere sessions.started_at <= customer_conversions.converted_at\n    and sessions.started_at >= dateadd(days, -30, customer_conversions.converted_at)3. Распределите баллыТеперь, когда у нас есть поля session_index и total_session, мы находимся в пределах всего нескольких case-операторов от наших заветных баллов атрибуции:select\n    *,\n    case\n        when total_sessions = 1 then 1.0\n        when total_sessions = 2 then 0.5\n        when session_number = 1 then 0.4\n        when session_number = total_sessions then 0.4\n        else 0.2 / (total_sessions - 2)\n    end as forty_twenty_forty_points,\n\n    case\n        when session_number = 1 then 1.0\n        else 0.0\n    end as first_touch_points,\n\n    case\n        when session_number = total_sessions then 1.0\n        else 0.0\n    end as last_touch_points,\n\n    1.0 / total_sessions as linear_points\n\nfrom sessions_before_conversionНаконец-то! Вы построили свою модель атрибуции! По этим баллам вы можете определить, какой канал и кампания привели к наибольшему количеству конверсий с течением времени. Выполнение такого рода  агрегирования мы уже оставим нашим бизнес-аналитикам:-- in your BI tool:\nselect\n    date_trunc(week, converted_at) as date_week,\n    utm_campaign,\n    sum(first_touch_points) as attribution_points\nfrom attribution\ngroup by 1, 2В зависимости от вашего инструмента бизнес-аналитики вы также можете подумать над созданием интерфейсов, позволяющих заинтересованным сторонам (стейкхолдерам) переключаться между методологиями атрибуции и уровнем агрегации (отчеты по дням, неделям и месяцам, а также группировка по кампаниям, источникам или средствам).Но погодите-ка, почему мы отталкиваемся здесь от даты конверсии, а не от даты сессии? Когда происходят сессии, мы не знаем, приведут ли они к конверсиям (и когда, если приведут). В результате, если мы будем группировать все по дате сессии, наши коэффициенты конверсии продолжат расти от недели к неделе.Для маркетингового отдела это может затруднить принятие решений, поскольку цифры всегда меняются. Поэтому вместо того, чтобы отвечать на вопрос “какое количество сессий Facebook привело к конверсиям на этой неделе?”, мы выбираем ответить на вопрос “сколько конверсий на этой неделе было результатом сессий Facebook?” Теперь, когда ваш отдел маркетинга знает, какие каналы ведут к наибольшему количеству конверсий они могут спросить: “Какой канал приводит к наиболее ценным конверсиям?”4. [Бонус] Добавьте показатели доходаЕсли у вас есть долларовая стоимость конверсии, вам следует присоединить ее к своей модели!Просто умножьте свои баллы на ценность конверсии:select\n\t... ,\n    revenue * first_touch_points as first_touch_revenue,\n    revenue * last_touch_points as last_touch_revenue,\n    revenue * forty_twenty_forty_points as forty_twenty_forty_revenue,\n    revenue * linear_points as linear_revenue\nfrom sessions_before_conversionТеперь, когда ваш отдел маркетинга знает, какие каналы ведут к наиболее ценным конверсиям, они могут спросить “какой канал обеспечивает максимальную окупаемость наших затрат? \"5. [Бонус] Добавьте данные по расходам на рекламуДля этого в вашем хранилище должны быть максимально прозрачные данные о расходах на рекламу - одна запись на кампанию, на день, с атрибутами кампании (канал, источник) и потраченной суммой.Для этого вам необходимо поместить в свое хранилище данные с каждой платформы, на которую вы тратите деньги (Adwords, Facebook, Instagram, Bing и т. д.). Мы используем Stitch и Fivetran для доступа к API всех рекламных платформ и загрузки этих данных в наше хранилище. Поскольку эти источники данных загружаются в формате источника (т.е. столбцы и таблицы именуются API платформы, а не нами), вам необходимо преобразовать их, чтобы получить согласованную структуру, а затем объединить их все вместе.DATE_DAYUTM_SOURCEUTM_MEDIUMUTM_CAMPAIGNSPEND2020-02-01facebook_adspaid_social10percentpromocode132020-02-02facebook_adspaid_social10percentpromocode152020-02-03facebook_adspaid_social10percentpromocode132020-02-04facebook_adspaid_social10percentpromocode102020-02-05facebook_adspaid_social10percentpromocode132020-02-06facebook_adspaid_social10percentpromocode132020-02-07facebook_adspaid_social10percentpromocode122020-02-08facebook_adspaid_social10percentpromocode11(Профессиональный совет: поищите на dbt package hub пакеты под конкретные рекламные платформы, которые сделают эту тяжелую работу за вас!)Важно отметить, что эти данные должны иметь те же utm параметры, что и данные вашей сессии. Таким образом, мы сможем объединить два набора данных для расчета:Стоимости конверсии: количество рекламных долларов, потраченных на привлечение клиента.Рентабельность затрат на рекламу: полученный доход/ рекламные расходыwith ad_spend as (\n\n    select * from {{ ref('ad_spend') }}\n\n),\n\nattribution as (\n\n    select * from {{ ref('attribution_touches') }}\n\n),\n\n-- aggregate first as this is easier to debug / often leads to fewer fanouts\nad_spend_aggregated as (\n\n    select\n        date_trunc('month', date_day) as date_month,\n        utm_source,\n\n        sum(spend) as total_spend\n\n    from ad_spend\n\n    group by 1, 2\n\n),\n\nattribution_aggregated as (\n\n    select\n        date_trunc('month', converted_at) as date_month,\n        utm_source,\n\n        sum(linear_points) as attribution_points,\n        sum(linear_revenue) as attribution_revenue\n\n    from attribution\n\n    group by 1, 2\n\n),\n\njoined as (\n\n    select\n        *,\n        1.0 * nullif(total_spend, 0) / attribution_points as cost_per_acquisition,\n        1.0 * attribution_revenue / nullif(total_spend, 0) as return_on_advertising_spend\n\n    from attribution_aggregated\n\n    full outer join ad_spend_aggregated\n    using (date_month, utm_source)\n\n)\n\nselect * from joined\norder by date_month, utm_sourceЭто даст нам представление о данных такого рода:DATE_MONTHUTM_MEDIUMCONVERSIONSREVENUETOTAL_SPENDCOST PER ACQUISTIONRETURN ON AD SPEND2020-02-01adwords1.37$19.11$47.00$34.33$0.412020-02-01bing0.48$6.432020-02-01direct10.98$141.432020-02-01facebook_ads5.14$66.43$312.00$60.67$0.212020-02-01google3.04$41.61В этом запросе следует обратить внимание на несколько вещей:Мы агрегировали в CTE, а затем объединили два CTE вместе - я всякий раз начинаю нервничать, когда join и агрегация происходят в одном и том же запросе (слишком большой риск разветвления!). Разделение логики сохранит ее чистоту, и ваш оптимизатор запросов будет вам благодарен.В дикой природе редко можно встретить full outer join! У нас он тут, чтобы гарантировать, чтоКонверсии, не связанные с рекламными расходами, отображаются в нашем результирующем наборе.Расходы на рекламу, не приводящие к конверсиям, по-прежнему будут отображаться в нашем результирующем наборе.Часто utm-атрибуты в ваших данных о расходах на рекламу не идеально соответствуют utm-параметрам ваших сессий, поэтому вам нужно будет выполнить корректировку исходных данных, чтобы этот join заработал.Мы делали join по дате конверсии и дате расходов на рекламу. Как объяснялось выше, это предотвращает изменение цифр после отчетного периода. Именно так большинство маркетинговых отделов решают эту проблему.6. Предоставьте к ней доступ!Теперь, когда у вас есть данные и несколько рабочих запросов, закомитте их в свой dbt-проект, смастерите несколько дашбордов и передайте их в руки заинтересованных сторон.Но прежде чем поделиться дашбордами с более широким кругом лиц, протестируйте их со своими стейкхолдерами, чтобы найти возможные проблемы и понять, насколько они важны, и стоит ли их как-нибудь менять. Но не дожидайтесь, пока ваша работа станет идеальной, прежде чем поделиться ею!В зависимости от вашего инструмента бизнес-аналитики вы также можете подумать на параметрами дашборда, чтобы иметь возможность:переключаться между методологиями атрибуции;переключаться между уровнями агрегации (источник utm/средство/кампания).После того, как ваши основные стейкхолдеры протестировали вашу модель и оценили ее, мы рекомендуем вам посовещаться с ними, чтобы выбрать один конкретный метод атрибуции. Все основные дашборды должны использовать только этот одобренный метод - по сути каждый метод атрибуции неправильный, но очень важно, чтобы все использовали одну и ту же версию “неправильного”.Понимание ограничений этой модели1. Позиционная атрибуция не является идеальным представлением человеческого процесса принятия решенийЧеловек очень сложно устроен. Есть целые области, посвященные пониманию процесса принятия решений. Таким образом, вероятно, невозможно точно ответить на вопрос; “Что привело к конверсии?” с любым инструментом, не говоря уже о каком-то SQL! Преимущество реализации этого в SQL заключается в том, что мы можем хотя бы двигаться в правильном направлении, сохраняя полный контроль над моделью.При этом стоит признать, что эту модель лучше всего себя показывает при просмотре данных в совокупности. Мы рекомендуем:Ограничится неделями - даже группировка по дням может стать слишком детализированной.Избегайте распределения долларовых расходов по клиентам: часто возникает желание начать оперировать выражениями типа “покупатель 123 обошелся мне в 12 долларов”. Но поскольку атрибуция - это несовершенная наука, мы не рекомендуем вам это делать.2. Просмотры рекламы в этой модели не учитываютсяВ этом анализе есть одно упущение: просмотры рекламы (например, когда клиент видит одну из ваших реклам в Instagram, но в результате не переходит на ваш сайт). Для большинства компаний невозможно получить записи об этих просмотрах в хранилище данных, поэтому для этого вам придется полагаться на показатели, сообщаемые платформами. Всегда относитесь к ним с недоверием: в интересах рекламной платформы раздувать эти цифры настолько, насколько это возможно.3. Веб-отслеживание ненадежноеУ некоторых ваших клиентов веб-отслеживание может быть заблокировано, и вы никогда не узнаете, откуда пришел этот клиент.Двое ваших клиентов могут пользоваться одним компьютером, и все их сессии могут быть связаны с недавней покупкой, совершенной кем-то одним из них. Или (что более вероятно) один пользователь может иметь сразу две учетные записи с разных устройств.Дело в том, что веб-отслеживание ненадежно. Для некоторой части вашей клиентской базы цифры всегда будут немного неточными.Делайте ее под себяХотя основные идеи моделирования атрибуции одинаковы для всех компаний, нюансы вашего бизнеса могут изменить некоторые детали того, как вы бы ее реализовали.Кастомизация под B2BЕсли вы работаете в сфере B2B, использовать взаимодействия с отделом продаж вместо сессий в качестве представления “касаний” может оказаться более результативным. Это все зачастую отражено в инструменте CRM - если вы используете Salesforce, просмотрите таблицы “events” и “tasks”. Многие команды будут регистрировать стихийные звонки в tasks, а запланированные встречи в events - вместе с отделом продаж постарайтесь выяснить, как эти данные принято регистрировать в вашей компании.Вам также необходимо учитывать, что именно определяет “конверсию”, особенно когда конверсии происходят на уровне аккаунта, но взаимодействие может происходить на уровне клиента.Индивидуальные методологии начисления баллов Некоторые маркетинговые отделы любят внедрять свои собственные методы начисления баллов. Например, одна команда, с которой мы работали, немного модифицировала логику, отдав приоритет платным каналам:если цикл взаимодействия с клиентом (customer’s journey) не оплачивается (является органикой), просто применяем к нему стандартные 40-20-40;если оплачивался, тогда, как только происходит платная сессия, все последующие неоплачиваемые каналы больше не учитываются.В чем логика? Если вы узнали о компании через adwords или facebook (и т. д.), а затем перешли на их сайт через обычный поиск или прямо на сайт, то на самом деле всю работу уже проделала первоначальная реклама. Соответственно так мы и распределяем заслуги.Атрибуция на уровне заказаЕсли вы являетесь компанией электронной коммерции, которая тратит значительную часть своих маркетинговых долларов на ретаргетинг существующих клиентов (более 1 миллиона долларов в год), возможно, имеет смысл инвестировать в создание атрибуции на уровне заказов. Вместо того, чтобы отмечать первый заказ как дату конверсии, у каждого клиента может быть несколько конверсий, и каждая конверсия имеет независимую атрибуцию. Это менее распространенный подход, чем атрибуция на уровне клиента, но мы работали с несколькими компаниями, которые сочли такое представление своих данных куда более полезным. Если вы планируете это сделать, сначала обязательно обсудите это со своим маркетинговым отделом, поскольку важно, чтобы это моделирование максимально соответствовало их представлениям о паттернах покупок клиентов.Настройте свой SQL, чтобы найти все сессии предшествующие первому заказу, а также сессии, которые были в промежутке между каждым заказом — вам нужно будет использовать оконную функцию в таблице orders, чтобы найти дату предыдущего заказа. Убедитесь, что вы используете order_id вместо вашего customer_id в операторе partition оконной функции.select\n    *,\n\n    count(*) over (\n        partition by order_id\n    ) as total_sessions,\n\n    row_number() over (\n        partition by order_id\n        order by sessions.started_at\n    ) as session_number\n\nfrom sessions\n\nleft join orders\n    using (customer_id)\n\nwhere sessions.started_at <= orders.created_at\n    and sessions.started_at >= dateadd(days, -30, orders.created_at)\n    -- ensure sessions aren't counted twice\n    -- use a coalesce to ensure the first order isn't excluded by a NULL join \n    and coalesce(sessions.started_at > orders.previous_created_at, true) Использование других источниковСессии — не единственный способ взаимодействия пользователей с вашим брендом. Команды обычно еще добавляют данные: реферальных опросов: «Как вы узнали о нас?» использования промокодовOOH(Out of home)-кампаний, например, телевизионная реклама и рекламные билборды.Для источников, о которых у вас есть записи о взаимодействии (например, реферальные опросы и промокоды), вам необходимо преобразовать ее в тот же формат, что и сессии и объединить их с вашими данными. Вам также нужно будет закодировать бизнес-логику того, какой вес придавать этим сессиям, например, будет ли ответ на опрос весомее, чем сессии?Для источников, о которых у вас нет записей о взаимодействии (в частности, для OOH-кампаний), вам нужно будет сделать некоторые обоснованные предположения, основанные на относительном росте трафика (это часто называют моделированием всплесков, но это уже выходит за рамки этой статьи!).Заключение:Если вы достигнете той точки, когда вы сможете одновременно отчитываться о расходах на рекламу и конверсиях таким образом, чтобы это было полезно для вашего маркетингового отдела, дела у вас уже идут лучше, чем у большинства компаний.Затем вы достигните решающего момента, где вам придется ответить для себя на вопросы: Стоит ли мне уделять больше времени этой модели? Должен ли я использовать марковскую модель, смешивать прогнозируемую пожизненную ценность клиента (CLV) и учитывать другие расходы, такие как зарплаты маркетологов?Инвестирую ли я время в качество исходных данных? Часто самая сложная часть этого анализа — объединить расходы на рекламу и касания, поскольку UTM-кампании часто обслуживаются достаточно плохо. Могу ли я поработать с отделом маркетинга, чтобы создать более совершенные UTM-схемы?Или мне не стоит заостряться на этом и сконцентрировать свои силы на другой задаче? Для многих компаний ресурсы данных весьма ограничены, и инвестирование большего количества времени в эту модель происходит за счет других проектов.В общем, я лично склоняюсь в сторону вариантов 2 и 3 — слишком часто мы видим, как много времени и усилий уходит на работу над атрибуцией, только лишь бы никогда больше не использовать дашборды. Даже если ваша модель используется, всегда есть риск, что вы сделаете модель более сложной, не сделав ее более полезной.Вместо этого лучше сделайте шаг назад, поймите, что вы проделали отличную работу, и двигайтесь дальше.Спасибо Эрин Огилви, Тристану Хэнди и Джанессе Ланц за их неоценимый вклад в это руководство.Материал подготовлен в рамках курсов \"Data Warehouse Analyst\" и \"Data Engineer\". Всех желающих приглашаем на ближайшие demo-занятия:1. «Полуструктурированные данные в Аналитических Хранилищах: Nested JSON + Arrays». На этом открытом уроке разберем:— Источники полуструктурированных данных: Events, Webhooks, Logs— Подходы: JSON functions, special data types, External tables (Lakehouse)— Оптимизация производительности>> РЕГИСТРАЦИЯ2. Интенсив «Автоматизация наполнения аналитического DWH данными из открытых источников».OLAP-хранилища — это отраслевой стандарт для организации DWH и настройки регулярного получения аналитических инсайтов через Data Science-инструменты или более традиционные платформы отчетности.На этом занятии мы подробно разберем, что такое OLAP-хранилища и self-service BI и углубимся в практический пример использования одной из хорошо зарекомендовавших себя технологий ClickHouse.>> РЕГИСТРАЦИЯ ",
        "user": "\n      MaxRokatansky\n    ",
        "time": "сегодня в 16:56",
        "ratings": " 323.82 \n    Рейтинг\n  ",
        "hub": "Блог компании OTUS ",
        "suit": "\n      otus.ru\n    ",
        "date": "1  апреля  2017"
    }
]